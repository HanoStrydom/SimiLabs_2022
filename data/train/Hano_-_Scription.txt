


 

Facial expression recognition as a tool to determine if students understand a topic


Hano Lombard Strydom
orcid.org 0000-0002-2362-6327


Dissertation submitted in partial fulfilment of the requirements for the degree Honours in Computer Science and Information Technology at the North-West University


Supervisor:	Mr. Henri van Rensburg


Date of submission: 	30 October 2022
Student Number
31597793 
ABSTRACT 
Current CFU techniques for determining comprehension are effective in smaller classrooms but not in large lecture halls. Lecturers and other professionals need a tool in large classrooms that can help them get a better understanding of whether the students understand the topic that is being discussed. The aim is to create a system that will be helpful to lecturers, by providing the ability to “read the room” when it comes to teaching in large classrooms. This was accomplished by researching and using a pre-trained facial expression recognition model to identify the students’ facial expressions, and then use that expression to classify whether the student is confused, happy or neutral by comparing the underlying facial characteristics of confusion with Ekman’s six basic emotions. The artefact was developed with the use of technologies such as a webcam and a standard computer and libraries such as OpenCV, face_recognition and matplotlib. The final build provides lecturers with a general sentiment of a classroom, giving them a better understanding of whether students in large classrooms understand the work that is being discussed. 
Huidige CFU-tegnieke vir die bepaling van begrip is effektief in kleiner klaskamers, maar nie in groot lesingsale nie. Dosente en ander professionele persone benodig 'n hulpmiddel in groot klaskamers wat hulle kan help om 'n beter begrip te kry of die studente die onderwerp wat bespreek word, verstaan. Die doel is om 'n stelsel te skep wat vir dosente nuttig sal wees, deur die vermoë te bied om "die lokaal te lees" wanneer dit by onderrig in groot klaskamers kom. Dit is bewerkstellig deur 'n vooraf-opgeleide gesigsuitdrukkingherkenningsmodel te ondersoek en te gebruik om die studente se gesigsuitdrukkings te identifiseer, en dan daardie uitdrukking te gebruik om te klassifiseer of die student verward, gelukkig of neutraal is. Dit word gedoen deur die onderliggende gesigskenmerke van verwarring met Ekman se basiese emosies ses te vergelyk. Die artefak is ontwikkel met die gebruik van tegnologieë soos 'n webkamera en 'n standaard rekenaar en biblioteke soos OpenCV, face_recognition en matplotlib. Die finale artefak verskaf die algemene sentiment van ‘n klaskamer vir ‘n dosent, wat die dosent dan ‘n beter begrip gee of student in groot klaskamers die werk verstaan wat bespreek word. 
Keywords:
Facial Recognition, Emotion Detection, Checks for understanding, Facial Expression Recognition  

TABLE OF CONTENTS
ABSTRACT	2
CHAPTER 1 – INTRODUCTION	8
1.1	Introduction	8
1.2	Background to study	8
1.3	Problem statement	9
1.4	Where would such a proposed system be advantageous?	9
1.5	Research aims and objectives	9
1.5.1	Research aim	10
1.5.2	Research objectives	10
1.5.3	Study design	10
1.5.3.1	Research Paradigm	10
1.5.3.2	Research Methodology	11
1.6	Research methodology	12
1.6.1	Process of obtaining informed consent	12
1.6.2	Data collection	12
1.6.2.1	Data collection tool	12
1.7	Rigour, validity, and reliability	13
1.8	Legal and Ethical considerations	13
1.8.1	Permission and informed consent	14
1.8.2	Anonymity	14
1.8.3	Confidentiality	14
1.9	Approach to project management and project plan	14
1.9.1	Scope..	14
1.9.2	Limitations	15
1.9.3	Risks……	15
1.9.4	Project plan	15
1.10	Provisional chapter division	17
1.11	Summary	18
CHAPTER 2 – LITERATURE REVIEW	19
2.1	Introduction	19
2.2	Methods to determine if students understand their work	19
2.2.1	Introduction	19
2.2.1.1	Assessments	19
2.2.1.2	CFU techniques	22
2.2.1.3	Student Feedback	23
2.2.2	Summary	24
2.3	Facial expressions and connected emotions	25
2.3.1	Introduction	25
2.3.2	Classroom facial expressions and it’s connected emotion	25
2.3.3	Importance of these expressions	26
2.3.4	Technologies and algorithms for recognizing facial expressions	26
2.3.5	Summary	28
2.4	Facial detection and recognition	29
2.4.1	Introduction	29
2.4.2	How facial detection and recognition work	29
2.4.3	Applications of facial detection and recognition	31
2.4.4	Advantages and Disadvantages of facial detection and recognition	32
2.4.5	Summary	34
CHAPTER 3 – ARTEFACT PLANNING AND DEVELOPMENT	35
3.1	Introduction	35
3.2	Description of artefact	35
3.3	The life cycle followed and its different phases	36
3.4	Description of the development of the artefact	38
3.5	Summary	46
CHAPTER 4 – RESULTS	47
4.1	Evaluation	47
4.2	Limitations	48
4.3	Recommendations	48
4.4	Conclusion	49
CHAPTER 5 – REFLECTION	50
5.1	What did I learn while completing this project	50
5.2	Did I achieve the objectives that were set for the project?	50
5.3	How successful was I in managing the project and meeting target dates?	51
5.4	Conclusion	51
BIBLIOGRAPHY	52

LIST OF TABLES
Table 1 - Definitions	7
Table 2 - Abbreviations	7
Table 3 - Due Dates	15
Table 4 - Gantt chart tasks	16
Table 5 - CFU Disadvantages	23
Table 6 - SDLC Phases	36
Table 7 - Close-ended Survey Questions	58
Table 8 - Survey Question	58

LIST OF FIGURES
Figure 1 - Gantt Chart	16
Figure 2 - Basic Agent Working (Russel & Norvig, 2021)	27
Figure 3 -Software Development Life Cycle	37
Figure 4 - Iterative Life Cycle (Okesola et al., 2020)	37
Figure 5 - Facial Detection	40
Figure 6 - Loading model and weights	40
Figure 7 - Angry Expression	41
Figure 8 - Neural Expression	41
Figure 9 - Confused Counter UI	42
Figure 10 - Console Confused Percentage	42
Figure 11 – Privacy	43
Figure 12 - Time Period GUI	44
Figure 13 - The console after a specified time period	44
Figure 14 - CSV File	45
Figure 15 - Graphical Results	46

Definitions

Table 1 - Definitions
Read the room	In the context of this research, read the room refers to the ability of a lecturer or professional to become aware of the attitude and emotions of the students or employees.


Abbreviations

Table 2 - Abbreviations
CFU	Checks for understanding
CSV	Comma-separated value
DSR	Design science research
E.g.,	Example
FER	Face Expression Recognition
FCP	Facial Characteristic Points
GUI	Graphical User Interface
HOG	Histogram of oriented gradients
SIFT	Scale-invariant feature transform
SDLC	System Development Life Cycle
URL	Uniform Resource Locators

 
CHAPTER 1 – INTRODUCTION
1.1	Introduction
This project is about finding a solution to the problem that lecturers, teachers and professionals are unable to keep an eye on every student or employee in large classrooms or online environments, allowing them to gauge attention and understanding based on facial expressions. In this project, the aim will be to determine if facial recognition technology can be used to solve this problem. This chapter will provide the study background, problem statement, possible uses, paradigmatic perspective, aims, objectives, research methodology, validity, data collection and its processing, project plan and approach, ethical and legal implications, provisional chapter division, summary and lastly the references.
1.2	Background to study
There are numerous techniques for lecturers and teachers to determine whether students in a classroom comprehend the current topic (Fisher & Frey, 2014). These techniques are called “Checks for understanding” or in short “CFU” (Quinzio-Zafran & Wilkins, 2020). The CFU is designed for school classrooms; therefore, not all of these checks can be used in big lecture halls, corporate meetings, or online learning environments.
For physical classes, one CFU technique is to use oral language (Quinzio-Zafran & Wilkins, 2020), like asking questions (Finley, 2014). Because of time constraints, lecturers are unable to enquire every student in big classrooms about their understanding of the topic (Kopf et al., 2005). Alternatively, lecturers rely on students’ facial expressions to determine the student’s understanding of the topic (Sathik & Jonathan, 2013). However, checking everyone's facial expressions can be challenging in large classrooms or online learning environments (Kopf et al., 2005).
COVID-19 forced education to switch to an online delivery mode, creating new challenges for lecturers and teachers alike (Paudel, 2021). The primary obstacle for students was acquiring the necessary technology to participate in these online classes (Alshamrani, 2019). For lecturers, a proven disadvantage is that the social interaction between students and lecturers, and the opportunity to observe students’ behaviour was significantly decreased (Lamanauskas & Makarskaite-Petkeviciene, 2021). Lecturers must look at the textbook, their notes, or slides on the screen, and is not always able to keep an eye on all the students attending the class, therefore unable to observe the students’ facial expression and determine if the students understand the topic.
Considering the concerns discussed above, it appears that lecturers, teachers, and professionals need to be able to “read the room” without physically having to pay attention to each individual student. 
This study will attempt to fill the gap by providing a way for lecturers to determine how many students understand the current topic at hand, without physically paying attention to the individual students.
1.3	Problem statement
Lecturers, teachers, and professionals require a method to use in large lecture halls and online environments for determining whether students or employees comprehend current topics. Previous methods include the CFU techniques, but these options don’t allow the lecturer to read the whole room. 
As mentioned previously, conventional CFU techniques are insufficient for large classrooms or online learning environments.
Would a system that enables lecturers to “read the room” without having to physically look at each student, be advantageous for a lecturer? 
1.4	Where would such a proposed system be advantageous?
This system can be used in a variety of environments. It is suitable for use in large lecture halls, classrooms, online environments, and corporate meetings. Anywhere a lecturer, teacher, or corporate staff member is required to determine whether students or employees understand the topic at hand.
1.5	Research aims and objectives
A well-defined research objective is critical for establishing the project's purpose. Additionally, there should be a clear list of objectives that must be met for the research objective to be accomplished. Achieving the aim and completing the set objectives will allow the lecturers to read the room and get a better understanding of what the students are feeling during classes.
 
1.5.1	Research aim
The aim is to create a system that will be helpful to lecturers, by providing the ability to “read the room” when it comes to teaching, using facial expression recognition.
1.5.2	Research objectives
To accomplish the above-mentioned aim, there are certain objectives that the researcher will need to reach
•	Research current methods to determine if students understand their work
•	Analyse facial expressions and how those emotions are displayed
•	Exploring facial detection techniques and how it works
•	Survey lecturers and students to get a better understanding of what would be beneficial to them
•	Comparison between different types of facial detection and expression programs 
•	Developing an artefact using the above-mentioned programs 
•	Evaluating if the proposed artefact will be able to identify common emotions linked to understanding 
1.5.3	Study design
1.5.3.1	Research Paradigm
A paradigm is described as a collection of shared beliefs and consensus among scientists about the proper way to approach and solve problems (Kuhn, 1970). Choosing the appropriate paradigm is critical when conducting research because it defines how the researcher thinks and interprets the data gathered (Kivunja & Kuyini, 2017). It also helps us shape the following fundamental elements: ontology, epistemology, axiology, and methodology (Park et al., 2020).
The research paradigm that will be used during this study is Positivism. Positivism states that the world is independent, meaning that people cannot change nature’s laws.  Researchers will gain knowledge through observation and experimentation, without changing the nature of the environment.  (Rehman & Alharthi, 2016). The experiments of this study will be done objectively from a distance, not bothering the students while they attend class. Surveys and questionnaires are a collection method for this paradigm and can be used to determine whether the experiments achieved accurate results. This paradigm makes use of quantitative data (Sousa, 2010), therefore a methodology will be used that supports this paradigm. Numerous methodologies can be used, but the one that will be used in this study is “Design Science Research”.
1.5.3.2	Research Methodology
As mentioned above, the research methodology that will be used in this project is “Design Science Research”, hereinafter referred to as DSR. DSRs’ main goal is to create an artefact that will solve a specific problem and then evaluate it afterwards. The reason why this methodology is suitable for this project is that DSR helps us achieve two goals in this project: Researching the problem and creating a solution to the problem (Dresch et al., 2015). 
Using DSR, quantitative data will be gathered from the artefact and the participants, and this data will be analysed and compared to determine whether or not the artefact yields positive results, thereby providing an answer to the problem identified. 
To achieve the goal of creating an artefact that will solve the problem identified in the problem statement, the DSR steps need to be taken (Peffers et al., 2007):
1.	Identify the problem - Chapter 1
This section contains the problem identified in the problem statement, as well as the justification for its resolution.

2.	List the objectives that need to be met in order to solve the problem - Chapters 1 and 2
Eight objectives must be met, as stated in the "Aims and Objectives" section of Chapter 1. To accomplish these goals, research must be conducted. This is part of Chapter 2 - Literature.

3.	Design and develop the artefact - Chapter 3
Using the research from Chapter 2, an artefact will be designed and developed to address the problem identified, using the Iterative system development life cycle, that will be discussed in section 3.3. 

4.	Implement the artefact - Chapter 3
This step is typically where the artefact is put into practice. However, for this project, the artefact will not be used in classrooms or online environments, but rather will be subjected to tests and experiments with a select group of participants to identify whether this solution will work in practice.



5.	Evaluate the artefact - Chapters 4 and 5
The artefact will be tested on participants, with the results being noted. The participants will then complete a survey, the results of which will be compared to the artefact’s result.

6.	Conclusion - Chapters 4 and 5
Following the completion of the tests in step 6, the results will be compared to determine whether or not the artefact successfully resolved the identified problem of assisting lecturers in lecture halls to determine whether students understand the current topic at hand.
Additionally, advice and errors will be listed to ensure that other researchers do not make the same errors.
1.6	Research methodology
As mentioned previously in the section on study design, throughout this project, "Design Science Research" will be used to collect data from the artefact and the participants. These data are critical to the methodology and research project in order to determine if it is a viable solution to the identified problem.
1.6.1	Process of obtaining informed consent
Consent must be obtained from participants prior to data collection. If the document is in physical form, the participant must read and sign it. If the form is online, the participant must first select a radio button before the survey or questionnaire can be accessed. 
1.6.2	Data collection
During this project, the positivism paradigm and DSR methodology are used. This means that quantitative data will be collected. Quantitative data is numerical values that can be used to answer questions (Apuke, 2017). Numerical values can be used to group and compare, making it easy to analyse the data.
1.6.2.1	Data collection tool
There are numerous quantitative data collection tools available, including surveys, questionnaires, observations, and experiments (Sukamolson, 2007). All of the tools listed above will be used to collect quantitative data for this project. Surveys will be used to elicit responses from participants' emotions. These data will be compared to the artefact's results to determine whether they were accurate. Observations will be used to gather researcher input to compare to the results of the artefacts. Experiments with the artefact will be conducted to gather data and improve the artefact.
1.7	Rigour, validity, and reliability
When doing research, it is important that the research is not lacking rigour, meaning it must be thorough. To ensure this, the researcher must follow the mentors’ advice, not be biased, and be careful when conducting experiments and recording the findings of the experiment and surveys. 
Validity is very important because it determines whether the study is logically and factually sound. Reliability refers to how consistent the results are being measured (Heale & Twycross, 2015). Reliable data does not ensure validity, but usually, validity ensures reliability. To ensure reliability, the surveys, questionnaires, and experiments will be done in the same environments, respectively. To ensure validity, the measuring tool must be able to measure what it says it can (Sürücü & Maslakçi, 2020).
To ensure reliability, all surveys and questionnaires are the same and will be answered in the same environment. The experiments and surveys should also give the specific results that this study aims to achieve.
1.8	Legal and Ethical considerations
When it comes to facial recognition, there are a plethora of ethical considerations. The primary consideration is the storage of facial recognition data. According to the "Protection of Personal Information Act No.4 of 2013," or more commonly known as the POPIA, students must be informed that facial recognition technology will be used (Government, 2019) and that their facial features will be used in an algorithm to determine their current emotion. Additionally, it will be stated that no personal information gathered from the recordings will be stored in a database.
Another consideration is the use of information that will be gathered through questionnaires and surveys. The collected data will not be shared or sold to third parties and will be used solely for research purposes. A data ethics form will be completed, ensuring that all necessary steps are taken with the data. This also contributes to the project's validity and reliability.
 
1.8.1	Permission and informed consent
All subjects will be informed in advance of any recording or use of facial recognition technologies, and the aforementioned considerations will be made clear to them.
1.8.2	Anonymity
Because no personal data such as names, surnames, or student numbers will be collected, subjects will remain anonymous. The facial recognition will occur in real-time, which means that no recordings will be stored in a database. Only generalised data from users’ facial features will be used in the algorithms, further ensuring the subjects' anonymity.
1.8.3	Confidentiality
Since no personal data about subjects will be stored in a database, all personal information about them will be kept strictly confidential. Non-personal information will be used solely for research purposes and will not be shared or sold to third parties.
1.9	Approach to project management and project plan
When conducting research, it is very important to address the following: Scope, limitations, and risks. The scope is, in essence, the aim of the project. The limitations are the restrictions that can take place, and that must be accounted for. The risks are things that could possibly go wrong during the project. 
To make a success of this project, proper planning needs to be done, and all deadlines must be reached within time. A Gantt chart will be used to do the planning, along with the due dates provided. 
1.9.1	Scope
The scope of this project is to determine whether facial recognition can be used to assist lecturers, teachers, and professionals in their daily lives to determine if students or employees understand the topic at hand. The technology that will be used is a 1080p Hikvision web camera. The system will be written in python using Visual Studio Code and Visual Studio 2019.
 
1.9.2	Limitations
Limitations are any facts or events that the researcher cannot control. Sometimes these limitations are predictable, and sometimes these limitations are unpredictable (Price & Murnan, 2004). In this research project, there are two known limitations:
•	Time
The final documentation, artefact and poster should be finished by the 27th of October. 
•	Technology
Most facial recognition algorithms are written in Python, and therefore I have to code in Python to make use of facial recognition algorithms and libraries such as “OpenCV”.
1.9.3	Risks
A big risk that can occur is if students do not consent to be recorded on a camera for research purposes. Another risk could also be that the existing libraries and code for recognising facial expressions do not contain the main expression that will be focused on. 
1.9.4	Project plan
The following dates are important as they are the due dates of the different project parts. It is very important to stick with the planning that will be displayed in the Gantt Chart to ensure the deadlines are met in time. The figure below will provide a timeline that will be followed. 
The table below consists of two columns: Date and Description. The dates are the due dates provided to the researcher. The description table describes what should be submitted on the given date. It is very important that the Gantt chart is followed to ensure the due dates are met.
Table 3 - Due Dates
Date	Description
22 April 2022
Extended to 2 May 2022	Project planning and research proposal submission
30 June 2022
Extended to 7 July 2022
Extended to 27 July 2022	Literature Study
20 October 2022	Poster and Artefact demonstration
27 October 2022	Complete documentation submission

Table 4 - Gantt chart tasks
 
 
Figure 1 - Gantt Chart
 
1.10	Provisional chapter division
Below is an overview of the chapters that will be present in this research project.
Chapter 1 – Introduction: 
This section will focus on everything before the literature study. It covers the background of the project and what is the problem identified. There will also be looked at the aim of the project and the objectives that need to be reached, along with a project plan. The paradigm and methods will also be discussed.
Chapter 2 – Literature review:
The literature review will include research that has been conducted to aid in the completion of the objectives stated under "Aims and Objectives." Three topics will be discussed: current research methods for determining whether students understand a topic, facial expressions and their connected emotions, and finally, facial detection and recognition. 
Chapter 3 – Artefact planning and development:
This chapter will outline the steps necessary to complete and test the artefact on time. The research used in the literature review will determine what technology and algorithms will be used to develop this artefact. Additionally, documentation will be completed during the planning and development of the artefact, which will contribute to the Chapter 5 reflection. Subjects will also be tested, and a questionnaire will be completed to determine whether the artefact correctly predicted the emotion the subject was experiencing at the time.
Chapter 4 – Results
The results of this project will be discussed in this chapter, where the research question will be answered. There will also be an overview of Chapter 3 to determine whether the artefact was successful in detecting the subjects’ emotions.
Chapter 5 – Reflection
This chapter will focus on the researchers’ experience throughout the project, the lessons learned and what could have been done differently. The reflection is critical because it could inform other researchers of potential mistakes and how to avoid them.
1.11	Summary
In summary, this chapter concluded the following: The current methods teachers, lecturers and professionals use to check student or employee understanding are insufficient for large- and online environments. In the problem statement, there is concluded that there is a need for lecturers and professionals to be able to keep an eye on everyone, without physically looking at them. The proposed system is applicable in numerous environments, but the main focus will be on large lecture halls and online environments.
The project aims to equip lecturers with the ability to read the room. To achieve this aim, seven objectives have been identified that must be addressed and accomplished. To ensure that these objectives are met, a Gantt chart will be used, and the scope of the project will be revisited to ensure that the researcher remains focused on the problem. Throughout this project, the positivism paradigm will be followed, and the methodology that will be used is Design science research. By adhering to this paradigm and methodology, rigour, validity, and reliability are ensured. Seeing that facial features will be captured and data extracted from it, legal- and ethical considerations, risks and limitations should be kept in mind. Consent will be obtained, and participants will be kept anonymous and their data confidential. Lastly, the five provisional chapters were laid out, which will be discussed in further detail in the subsequent chapters.
 
CHAPTER 2 – LITERATURE REVIEW
2.1	Introduction
This Chapter will include research that will be addressing the aim of creating a system that will aid lecturers in classrooms.  It will explore the existing methods for determining comprehension, as well as their advantages and disadvantages. Following this, facial expressions seen in lecture rooms will be analysed and discussed. Lastly, facial detection techniques will be investigated, and its applications will be discussed.
Discussing the topics above will give the necessary information to develop an artefact in Chapter 3 and aid the researcher in answering the research question.
2.2	 Methods to determine if students understand their work
2.2.1	Introduction
There are numerous techniques for lecturers and teachers to determine whether students in a classroom comprehend the current topic (Fisher & Frey, 2014). It can be done via assessments, CFU techniques and student feedback. Each of these methods has its benefits and drawbacks that will be discussed. It is crucial to determine whether students comprehend the work being covered in class because the lecturer needs this information to determine which areas to spend more time on, and which parts of the subject the students have more difficulty understanding. The following three methods for measuring student comprehension will be discussed and compared: assessments, CFU techniques, and student feedback.
2.2.1.1	Assessments
Assessments are very important in any type of education (Taras, 2008). It enables instructors to determine how effectively students comprehend the subject and its topics. The lecturers can then structure their teaching methods and determine where further focus is required (Tosuncuoglu, 2018). There are two types of assessments that are most commonly used by lecturers to assess students (Dixson & Worrell, 2016):
•	Formative assessments
•	Summative assessments
Formative assessment is a way teachers and lecturers receive feedback, and check for understanding during the learning process (Andersson, 2008). In-class discussions, weekly quizzes and homework assignments can be considered as formative assessments. Formative assessment takes place during the semester and the lecturer should be able to use this assessment to identify, analyse and respond to the problems of students (Hunt & Pellegrino, 2002). 
The following advantages and disadvantages of formative assessments are described by Ahmad- and Jeelani Bhat (2019).
Advantages of Formative Assessments
1.	Provides information to lecturers
Formative assessment provides valuable information to lecturers during classes and assessment periods to determine if students understand topics. 
2.	Provides feedback to students 
Using these assessment methods, students can also become aware of the topics they do not understand before they write summative assessments. 
3.	 Adjusting teaching methods 
Lecturers can use this assessment method to help them adjust their teaching methods that enable them to help students understand certain topics. 

Disadvantages of Formative Assessments
1.	Time
Lecturers have a lot of material to cover in class and don't always have enough time to conduct formative assessments like class discussions and quizzes.
2.	Lack of participation
Students are very shy in larger classrooms and may not participate in classes. During quizzes that may not count towards their grade, students may not care about the assessments, and not take them seriously.
3.	Lack of training or experience
Without proper training or experience, lecturers may not know how to analyse these types of assessments to determine where a gap is in students’ understanding.
Summative assessments are usually formal assessments taken at a specific point in time (Garrison & Ehringhaus, 2007), like semester tests and exams during or at the end of a semester. The purpose of summative assessments is to see how good students understand the subject after a certain time, or the amount of work handled, and assign a grade to the student (Angelo & Cross, 1993). There are certain advantages described by Ahmad- and Jeelani Bhat (2019) that will be discussed below: 
Advantages of Summative Assessments 
1.	Test student knowledge after a time period 
Tests and exams can be used by lecturers to assess their students. This will provide quantitative data on how well students comprehend the subject and its many topics. Because these assessments take place periodically throughout the semester, lecturers can utilize the data to identify if students are improving.
2.	Marks and Grades 
These assessments are typically used to grade students and determine whether they have adequate knowledge to pass a module. In order to pass, this assessment method motivates students to study the subject and its topics. 
3.	Identify weak areas 
Lecturers utilize this data to determine where additional attention is needed, and which topics students are struggling with. 
Disadvantages of Summative Assessments:
1.	Guessing multiple choice questions (Qu & Zhang, 2013)
During summative assessments, multiple choice questions can be asked, and students can guess the correct answer, without understanding the work. This may give inaccurate information to the lecturer, leading them to believe that most students understand the work. 
2.	Cannot improve understanding after an examination (Ishaq et al., 2020)
Exams are at the end of a semester. The marks determine whether students pass. The drawback with this type of assessment is that if you see students struggling with certain topics, it is too late to help them because it is the end of the semester or year. 
3.	Not a true representation of understanding (Ishaq et al., 2020)
Tests and exams are very stressful, causing students to hit a ‘blank’, meaning they stress so much that they forget the work studied, but this can be a sign of the last disadvantage explained next. 
4.	Summative assessments test memory more than knowledge (Sporer, 2022)
During the exam and test periods, students are more likely to memorise the work, rather than understand it. Thus, summative assessment marks don’t always reflect students’ understanding, but rather their memory. 
2.2.1.2	CFU techniques
CFU is an abbreviation for “Checks for understanding”. The CFU techniques are designed for school classrooms; therefore, not all these checks can be used in big lecture halls, corporate meetings, or online learning environments. CFU techniques are useful in smaller classes, but in big classrooms, time constraints make it much more difficult (Kopf et al., 2005). According to Fisher and Frey (2014), there are numerous ways of checking understanding among students, a few of them are oral language, facial expressions, hand gestures and tests. A short description of these examples will follow: 
Oral Language
Oral language encompasses all verbal communication in the classroom. The most typical type involves lecturers asking students to demonstrate their comprehension of a topic (Dufour, 2021).
Facial Expressions
Teachers and lecturers rely heavily on facial expressions to determine whether or not students comprehend the subjects (Butt & Iqbal, 2011). Emotions are one of the hardest things to conceal among students. It is a technique for lecturers and teachers to determine whether or not students grasp the material without directly communicating with students or poor performance in tests, as explained in the section on assessments (Fisher & Frey, 2014). 
Hand gestures
Lecturers may request that students raise their hands if they comprehend or do not comprehend the material covered in class. The lecturers will then use this information to evaluate if they need to revisit the topics (McTighe, 2021). 
Tests
This may consist of a combination of formative and summative assessments, as described in the previous section. The downsides have been mentioned previously. 
Each of the aforementioned CFU techniques has its own advantages and disadvantages, specifically when it comes to large classrooms, but the most notable downsides are summarised in
Table 5 to see how they might, later on, be improved and even used to help lecturers. 
Table 5 - CFU Disadvantages
Disadvantages of certain CFU techniques
Oral language	Students are shy, and do not want to ask questions in large classrooms or see lecturers alone in their offices.
Facial Expressions	Lecturers cannot attentively look at more than 100 students to determine whether they understand a topic. 
Hand gestures	In large classrooms, lecturers may not be able to observe all the hands up in the air, or it may take a while to count.
Tests	As discussed in the Assessments section, there are numerous disadvantages to this technique.

2.2.1.3	Student Feedback
Student feedback is important to lecturers because it gives lecturers the chance to address problems and topics head-on. The student feedback technique relies on the cooperation of students. In order to establish whether the proposed system would be feasible, a voluntary survey was conducted among university students. The survey received 31 responses from various students, and the detailed results of this survey can be found in Appendix A. 

According to the survey results, 58% of students are shy and would not want to be personally addressed and 58.8% of students would not raise their hands in class. There are several ways students engage with lecturers, but the two most common are raising their hands in class and seeing the professors in their offices. These two techniques will be discussed below: 
Hand Raising
Hand-raising is a common behaviour by students in classes. It allows them to participate and draw the lecturer’s attention to ask for explanations (Böheim, 2020). Unfortunately, one drawback is that most students are shy about raising their hands in large classes (Kettner, 2015), and therefore a new way should be determined to gather student feedback. 


Visiting lecturers in the office
Visiting a lecturer can be daunting for students, and most students would prefer not to see lecturers in their offices during office hours. Lecturers are unable to help and assist students that do not seek any help. 
2.2.2	Summary
Lecturers wish to be able to assist students in thoroughly understanding and comprehending the work prior to exams and assessments, not only to increase students' grades but also to enhance their comprehension and prepare them for the workplace.
Assessments can be an effective method for gauging pupils' understanding; however, they are typically administered too late. As Ishaq et al. (2020)  noted, students tend to become anxious and forget their work, which can lead to poor performance. This may be due in part to the fact that students memorize rather than comprehend their work.
Because the majority of students do not feel comfortable engaging in in-class activities or visiting lecturers during office hours, student feedback is unreliable and difficult to use as an indicator of student understanding. 
Checks for understanding are an excellent approach for determining if students comprehend the material prior to tests and exams. The issue with CFU techniques is that they are extremely time-consuming and challenging to apply. The oral language and hand gesture CFU techniques are not reliable, seeing it requires student participation to be successful, and most of them are too shy to participate. Tests are too late, and lecturers are unable to see everyone’s facial expressions during a class. As indicated in the CFU techniques section, reading facial expressions is an effective CFU technique, however, it is very time-consuming and just not feasible in large classrooms. Assisting professors and aiding students will be revolutionized by the development of a system that will read and understand facial expressions, without lecturers spending time individually looking at students. 
This chapter brings us closer to the question of whether there is a way to improve the CFU techniques, allowing lecturers to get a better understanding of whether their students understand a topic. The next section will discuss the role of facial expressions in classrooms and their connected emotions.
 
2.3	Facial expressions and connected emotions
2.3.1	Introduction
In the previous section, it was established that the CFU techniques of utilising facial expressions in a classroom to properly identify whether pupils grasp the content, is a good approach to address the issues in large lecture halls. Now it has to be identified which facial expressions are encountered in classrooms, and what their associated emotions actually mean.
Different facial movements and muscle positions result in a specific expression (Butt & Iqbal, 2011). Facial expressions are a universal way to express emotion. Different things may infuriate various individuals, yet the facial muscles and movements are nearly identical among them (Ekman & Keltner, 1997). People struggle to conceal their emotions since emotions are spontaneous (Dimberg et al., 2000).  
This chapter will explore facial expressions in classrooms, their associated emotions, their importance, and the technologies used to detect them. 
2.3.2	Classroom facial expressions and it’s connected emotion
There are six types of emotions most commonly noticed on people: disgust, sadness, happiness, fear, anger, and surprise (Yang & Hirschberg, 2018). All these emotions can be encountered in classrooms. A smile may be an indication of happiness, and crossed eyebrows and a frown may be seen as anger or disgust. An open mouth and big eyes may be an indication of surprise. But one expression that is encountered a lot in classes is the “confused” facial expression. But what is considered as confused? 
Confusion is an emotion connected to knowledge, meaning it is an epistemic emotion. It causes cognitive disequilibrium, meaning learners face situations such as obstacles to goals, contradictions, incongruities, anomalies, and conflicts (D’Mello & Graesser, 2012). These obstacles can be anything from not understanding certain work or struggling to achieve good grades. Confusion can be picked up by looking at the facial expressions of students. A study was done and found that lowered eyebrows, frowns and tightened lids were associated with confused expressions (D’Mello & Graesser, 2014). Silvia (2013) stated that confusion makes people frustrated, which makes people scrunch, and lower their eyebrows and furrow which is caused by frowning. When looking at these expressions and actions, it can also be associated with other emotions, such as anger, sadness, fear and disgust. People that are angry will frown, and their brows will furrow (Reed et al., 2012), people that are sad or scared will also have their brows furrowed and have lowered eyebrows (Kohler et al., 2004) and finally, disgusted people will scrunch and their lids will tighten (Pochedly et al., 2012). All these expressions can also convey that students may experience confusion. 

2.3.3	Importance of these expressions
As mentioned previously, confusion causes cognitive disequilibrium, and students need to overcome this disequilibrium in order to move on (Arguel et al., 2019) and get a better understanding of the work. For a lecturer to improve classes or the way they present certain topics, they accurately need to understand the expressions of the students. Whether it is confusion, boredom, anger, happiness, or any expression the lecturer can identify within a classroom.   Lecturers will use this information to make the necessary changes during a lecture. 
Even though it can be an accurate way to determine the understanding of students, it is very difficult for lecturers to read all the faces of the students during a class, because there are too many students to keep an eye on at all times. 
Another big problem that can be encountered is alexithymia. Alexithymia is a disability, making it very difficult or even impossible for students to identify their own, or others’ emotions (Taylor, 2000). Alexithymia is not a rare disability. One in every ten people has Alexithymia (Cherney, 2021). Even in the rare case a lecturer doesn’t find it difficult to observe an entire classroom, the lecturer may have Alexithymia making it impossible to determine which facial expressions and emotions the students are currently expressing. 
These problems may be solved by using technologies with certain algorithms, allowing lecturers to get feedback on the facial expressions of students. 

2.3.4	Technologies and algorithms for recognizing facial expressions
To create a system that allows lecturers to read the room, another pair of “eyes” need to be available. There also needs to be a way for the system to communicate with the lecturer and let them know what it is currently perceiving. For the system to know what it is perceiving, an algorithm needs to be determined and implemented into the system. 
This means the system can be divided into 2 sections, technology, and workings, also known as the algorithms and coding.
Technologies and algorithms are very important throughout this project because the technologies, such as cameras, will be used as an extra pair of eyes that the lecturers can use to their advantage. It will be used to perceive the students’ facial expressions. The algorithms are the brain of the system and will take the percepts/observations from the camera, analyse the facial expression, and determine whether the student comprehends the work being discussed. 
This system can be seen as an agent. An agent can be seen as anything that perceives the environment through sensors and acts upon that environment through actuators (Russel & Norvig, 2021).    
The following Figure 2 demonstrates the working of an agent. The Agent is the system, and the environment is the classroom. The Sensor is the camera that perceives the environment. The actuator is the display that shows the result to the lecturer. The question mark is “brain” or the intelligence of the agent. In this case, it’s the algorithms and code making it possible for the agent to detect whether the student understands the topic being discussed. 
 
Figure 2 - Basic Agent Working (Russel & Norvig, 2021)

The technologies that can be used to develop the proposed system should therefore have appropriate sensors and actuators:
Sensors
•	Cameras
o	These cameras will be used as the “eyes” as mentioned above. It will be used to observe the large classrooms and give input to the system. 


Actuators 
•	Computer
o	In order for the program to run, a computer needs to be used. It needs to be a mid-range computer. Preferably with an Intel i5 CPU or higher. It also needs a USB port to connect a camera. 
•	Computer Screen
o	This is where the output will be displayed to the lecturer of how many students do not understand the topics being discussed.

Agent’s Intelligence refers to the “brain” of the system. This “brain” needs to be able to receive information from the technologies such as sensors and analyse it with the use of algorithms and programs.
•	Algorithms and Programs
o	There are numerous algorithms and code that can be used in order to assess facial expressions. The workings of these algorithms are discussed in “How facial detection and recognition work”. Numerous algorithms can be used and implemented, and a few algorithms will be researched and tested in Chapter 3 of this search project. 

2.3.5	Summary
Facial expressions are something everyone has. It is a trustworthy and valid method to determine whether a student understands a topic and the work that is being discussed. There are six common expressions, but the expression that is the most relevant to this study is confusion. This facial expression can be seen as lowered eyebrows and tightened eyelids. 
These emotions are very important to lecturers because it allows them to get a better understanding of whether or not the students are taking in the work, and how much they understand the topics being discussed. It was noted that it is difficult for lecturers to keep an eye on every student, or in some cases, it is impossible for them to determine which emotion is displayed by students and therefore need some help with another pair of “eyes”.
Technologies and programs or algorithms, also known as an agent, can be used to assist the lecturer. The programs and algorithms will use the input received from cameras and determine whether students understand their work. These results will be displayed on a screen assessable by the lecturer. This study will provide some background on how facial detection and recognition work.

2.4	Facial detection and recognition
2.4.1	Introduction
Facial recognition is used to identify and verify an individual’s identity and can be used in various scenarios such as security and entertainment (Mohammad, 2020). Facial recognition can be largely attributed to Woodrow Wilson Bledsoe. In the 1960s, Bledsoe created a database of thousands of images, in his efforts to create a face-identifying system (Libby & Ehrenfeld, 2021). 
Facial detection is a part of facial recognition which is a popular subject when it comes to biometrics (Khan et al., 2019). The difference between facial detection and facial recognition is that facial detection involves the detection of an image or video. This means the system will detect whether there is a  face on the image or video provided (Kumar et al., 2019). The system can be programmed to draw a square around the detected faces. The proposed system will use facial detection, and not facial recognition, seeing the aim of this project is not to determine whom the person perceived is, but rather to analyse the facial features using facial detection algorithms. This algorithm will determine which emotion the student is experiencing and use that information to get an understanding of whether the student comprehends the topics being discussed. 
During this section, it will be discussed how facial detection and recognition work, along with current facial detection and recognition applications, and the advantages and disadvantages thereof.
2.4.2	How facial detection and recognition work
Facial detection is the method of determining whether a human face is detected within an image (Nagpal et al., 2018). There are different methods that can be used to detect faces, such as template-matching methods, feature invariant methods, and appearances-based methods. (Dilawar & Siddiqui, 2016) 
Template-matching methods involve finding the correlation between the source image and a testing image of a face (Ping et al., 2003). If there is a high correlation, it means that a face was detected in the source image. 
Scale invariant feature transform, also known as SIFT, can be seen as a feature invariant method because it is a technique for detecting and extracting visual features (Gupta et al., 2021). It identifies key points called descriptors (Lindeberg, 2012) and provides them with quantitative information that may be utilized for object recognition. SIFT is not recommended for facial detection, because SIFT is good with general object detection, which is rigid with sharp edges, where faces are non-rigid and smooth (Geng & Jiang, 2009). 
Appearance-based methods like Eigenface. The Eigenfaces approach consists of extracting distinctive facial features and portraying the face as a linear combination of the so-called 'eigenfaces' derived from the feature extraction procedure. (üge Çarıkçı & Özen, 2012)
Another prominent facial detection method is the Histogram of oriented gradients, also known as HOG. HOG may be viewed as a more dense variant of SIFT (Pang et al., 2011). Gradient direction density statistics can characterize the shape and colour of a local image object, such as a human face. Since an image's gradient lies at the edge of a local object, its oriented gradient histogram indicates the edge direction density of detection targets (Li et al., 2016).
Face expression recognition, also known as “FER”, is the method of using either descriptors or Facial Characteristic Points to determine which facial expression is expressed. The efficiency of FER depends on how accurate the facial features can be extracted for the descriptor (Anil & Suresh, 2016). Appearance features and geometric features are the two methods used to extract facial features (Revina & Emmanuel, 2021). 
Appearance features
Generally, appearance-based methods extract the textural variations of face images using a variety of descriptors (Yu & Liu, 2015). The descriptors of different facial features will show a high variance, while descriptors of the same facial features will show little to no variance (Anil & Suresh, 2016). 

Geometric features
During this method, a facial image is segmented into three parts regions, the mouth, nose and eyes and eyebrows. Facial Characteristic Points, also known as “FCP”, are located for each of these segments (Youssif & Asker, 2011). Using these FCPs the algorithm can then determine the distances and movement between the facial features and then determine which facial expression is most likely expressed. 
In conclusion, two methods can be used to achieve FER, but for these methods to work, technologies and algorithms are needed as discussed in Section 2.3.4: Technologies and algorithms for recognizing facial expressions. 

2.4.3	Applications of facial detection and recognition
As mentioned in the introduction, facial detection is used within facial recognition. Facial detection and recognition can be used in various fields such as security, biometrics and law enforcement (Bernstein, 2020). These general applications will be discussed below: 
Security
Humans can only remember a certain number of faces, while computers with large memories can store thousands, if not millions of faces. This is a breakthrough in the field of security. Facial detection and recognition can be used in crowd surveillance, such as in airports and private security (Balla & Jadhao, 2018). 
Biometrics
Biometric recognition is the automatic recognition of people, using their physical features (Delac & Grgic, 2004) such as fingers, voice and face (Woodward Jr et al., 2003). Biometrics can be used at your home or business. Your face can be used to unlock a device such as your cell phone and your fingerprint can be used to clock into work. 
Law Enforcement
Facial detection and recognition are used by law enforcement, by using surveillance cameras to detect people’s faces and compare them to their database of criminals and people with arrest warrants (Garvie & Frankle, 2016). Some may see this as unethical, as discussed in the Advantages and Disadvantages of facial detection and recognition section, but according to a study done by Smith (2019), 59% of the people that participated in the survey said they find it acceptable that law enforcement can use facial recognition in public spaces. 

Other
Although facial detection and recognition are mostly used in the areas mentioned above, it is not limited to those areas. This proposed system is aimed to be implemented in classrooms to assist lecturers. It has nothing to do with security, biometrics, or law enforcement, but rather for educational purposes. 
2.4.4	Advantages and Disadvantages of facial detection and recognition
There are numerous advantages and disadvantages in various fields of the application when it comes to facial detection and recognition. Some advantages can also be disadvantages, which will be discussed below. 
Advantages
•	Security
o	Facial detection and recognition can be used for security purposes, in either airports or public spaces. Facial detection and recognition make it possible for a computer to detect humans by their facial characteristics (Owayjan et al., 2015). It can be used to detect any criminals or people with arrest warrants (Garvie & Frankle, 2016). 
•	Biometrics
o	Biometrics is an authentication method that automates the identification of a person based on their facial features (Omoyiola, 2018). Facial detection and recognition can be used as a password or two-factor authentication in protecting or accessing important information or devices. It is much more secure and convenient than the traditional password system (Vazquez-Fernandez & Gonzalez-Jimenez, 2016).
•	Medical
o	By evaluating subtle facial features, facial detection and recognition can, in some instances, pinpoint how specific genetic mutations caused a particular syndrome. The method may be faster and less expensive than standard genetic testing (Gargaro, 2022). 
•	Attendance
o	Facial detection and recognition can be used in schools or businesses to determine who attended the class or clocked into work on a specific morning. Facial detection and recognition are one of the main methods of recording attendance because it requires virtually zero effort from the students (Kar et al., 2012). Businesses use it because it is much less time-consuming than for the management to do it manually (Arsenovic et al., 2017).


Disadvantages
•	Privacy
o	Facial recognition is a form of surveillance technology that can be used to identify and track individuals. This allows a company or government to gather information about the habits, lifestyles, and interests of individuals (Wright, 2018). According to a study done by Institute (2019), 77% of people surveyed do not feel comfortable with companies using facial recognition technologies. Section 14 of the South African Constitution states that everyone has the right to privacy (Government, 1996), which facial detection and recognition can infringe. 
•	Racial Discrimination
o	Facial detection and recognition have a hard time identifying people of colour and in some cases even be biased against people of colour (Garvie & Frankle, 2016). This problem originates from machine learning on a particular database of faces (Libby & Ehrenfeld, 2021). This means that when Facial Recognition algorithms are trained using a database predominantly Caucasians, the facial recognition of Caucasians would be much more accurate than that of dark-skinned individuals. 
•	Fraud
o	Facial detection and recognition can be used to collect personal information, video, and images and that can be used in fraudulent activities (Gargaro, 2022). Programs and applications like “DeepFakes” can be used to imitate certain people and that can cause a lot of trouble including theft, fraud and even wars (Korshunov & Marcel, 2018). 

 
2.4.5	Summary
There are mainly two ways that a system can determine facial expressions and their connected emotions. Either through descriptors, using the appearance feature method or facial characteristics points, or by using geometric features. Numerous algorithms can be used with either of these methods which will be further discussed and researched in Chapter 3.
It was determined that facial detection and recognition technologies and applications are widely used in numerous environments such as biometrics, law enforcement and security. Facial detection and recognition are not limited to any field and can be found nearly everywhere in today’s world and are still being developed and improved upon every day. 
Although facial detection and recognition can improve and make our lives easier every day, there are several advantages and disadvantages that need to be considered when using these technologies and algorithms. Taking all the disadvantages into account, one can determine that the main concern of facial detection and recognition for this research is privacy. This issue can easily be addressed by informing participants of their rights and to stay within the law and Constitution. 
One key takeaway of all the advantages mentioned is that it is essential for the algorithm to detect a human face before it can recognise who it is, or in the case of this study, what expression they are expressing. 
It was established that the proposed system is applicable because it addresses the problem that current CFU techniques are not sufficient for large lecture halls, and how technologies and algorithms are used in the two FER methods. In the next Chapter, the technologies and algorithms will be researched more in detail, tested, and integrated into an artefact that will allow lecturers to use this proposed system to determine whether students understand the work being discussed. 
 
CHAPTER 3 – ARTEFACT PLANNING AND DEVELOPMENT
3.1	Introduction
During the study, it was determined that lecturers and other professionals need a tool in classrooms that can help them get a better understanding of whether the students understand the topic being discussed. A way to solve this problem is to use technology, such as a camera, and a program, otherwise known as the artefact. 
The artefact will aim to detect faces, determine whether the student is confused based on their expression and then provide feedback to the lecturer. The lecturer will be able to see on the screen the number of students that possibly do not understand the topics being discussed and can change the lecture accordingly. The artefact will also be able to keep track of the general sentiment for a given time period provided by the lecturer and save those values along with the date and time to a CSV file. A function will then create a line chart with these values to show a visual representation of the data. 
3.2	Description of artefact
The artefact will display the classroom of students, with their faces blurred out, and a text telling the lecturer how many students are confused at that current moment. The lecturer will specify a time that the program should keep track of the general sentiment, and the results will be displayed in the python console. It will also be written to a CSV file for later viewing. After the program has been terminated, a graph will be displayed with the emotions and the timestamps, giving a graphical representation of the sentiment results during class.
In order to address the problem identified in Chapter 1, the artefact should attempt to accomplish the following:
1.	Detect all the faces in the frame
2.	Determine each facial expression of the individual faces
3.	Determine whether the emotion expressed may be classified as confusion
4.	Show the general sentiment for that frame
5.	Keep track of the general sentiment for a determined time period
6.	Show and save the general sentiment in a comma-separated values file
 
3.3	The life cycle followed and its different phases
Throughout this project, the design science research method was followed as discussed in Section 1.5.3.2. It was determined that 6 steps needed to be followed to achieve the goal of developing an artefact that will address the problem identified. Step one, planning, and step two, defining requirements were addressed during Chapter 1 and Chapter 2. Step three is about planning, designing, and developing the artefact. For the development to be a success, a system development life cycle, also known as SDLC, needs to be followed. The term SDLC can be described as the phases an artefact or product passes through, from the original idea to the final product (Jirava, 2004). 
The SDLC has 6 main phases as illustrated in Table 6, which are planning, defining requirements,  designing, developing, testing and deployment (Shylesh, 2017). 
Table 6 - SDLC Phases
Phase	Description
Planning	During this phase, the terms of the project are determined which includes the goals and due dates of the artefact.
Defining Requirements	During this phase, the requirements of the artefact are determined. This is where it is established what the artefact should do in order to solve the problem identified. 
Designing	When the requirements are determined, the artefact should be planned out. This is where it is decided which programming languages, code editors, libraries etc. should be used in developing the artefact. 
Development	During this phase, the artefact will be developed according to the requirements and design criteria. 
Testing	Testing of the artefact to determine whether it conforms to the requirements established.  
Deployment	This is the last phase of the SDLC, where the artefact is deployed and implemented. 

 
Figure 3 -Software Development Life Cycle
There are different types of life cycles that can be used from different methodologies, and these life cycles are variations of the SDLC, such as the Waterfall life cycle, agile life cycle and iterative life cycle. The life cycle followed for developing the artefact for this study is the iterative life cycle. 
 
Figure 4 - Iterative Life Cycle (Okesola et al., 2020)
The iterative life cycle uses iterations called builds to develop the artefact. Each build is a series of repeated cycles used to add functionality to the artefact. Figure 4 is used to show how the iterative life cycle works (Okesola et al., 2020). Clear requirements are needed before the building cycles can start. Each build is a functionality that is added to the artefact and needs to be completed, tested and implemented before the next build can begin. The iterative life cycle is very easy to implement when using it with the Design Science Research methodology, and allows the following advantages (Shylesh, 2017):
•	Easy to measure development progress
•	Small iterations allow easy testing and debugging
•	Can easily add new functionalities and revise the requirements
•	After every iteration/build, a functional product is delivered
GitHub is used for version control throughout the project, and the iterative life cycle will enable anybody to co-create and add functionality to the artefact while having a functioning version available at all times. 
3.4	Description of the development of the artefact
During the planning phase, it was determined that the goal of the artefact is to determine whether students understand their work with the use of facial recognition. It was also determined that the due date for the artefact to be finished is the 20th of October. 
A goal and due date were determined, but before the development could begin, the requirements needed to be defined during the requirements phase. The following are the initial requirements of the artefact:
1.	Detect a human face
2.	Determine the expression the person is exhibiting and display the expression
3.	Determine whether the expression is considered confused, happy or neutral
4.	Display the number of students that are confused
One of the advantages of the iterative life cycle is that the requirements can change, and new functionalities can easily be added. Throughout the development, the following additional requirements were also added to the initial requirements list:
1.	Protect student privacy by blurring their faces
2.	Hide the emotion for each student and only display text indicating ‘student’
3.	Keep track of general sentiment for a time period the lecturer specifies
4.	Write the general sentiment for the time period in a comma-separated value file, also known as a ‘.csv’ file. 
5.	Display the CSV result in a graph for visual representation. 
After finalizing the goals, due dates and requirements, some design decisions needed to be made. The programming language, code editor, resource manager and technologies needed to be determined. Python was chosen as the programming language, seeing as a lot of libraries can be used in aiding the development of the artefact. It is also a simple and easy language to understand with a lot of advantages. The code editor that was used is Visual Studio Code since is easy to use and has all the functionality required to develop the artefact. Different extensions such as “Co-Pilot” and “GitHub” aided in the development of the artefact. The resource manager that was used is GitHub. GitHub makes version control easy and complements one of the iterative life cycle advantages of keeping track of progression and builds. Seeing as the goal is to use facial expression recognition, a Hikvision 1080p webcam and a general computer were used. 
It is important to note that not every build would satisfy a requirement in its entirety but could be part of the completion of a specific requirement. After making these decisions, the first build, known as ‘Build 1’, could begin. 
Build 1: Face Detection
The goal of the first build was to develop a function that could detect human faces. A lot of research was done and the ‘face_recognition’ and ‘OpenCV’ was used to create a function that will detect human faces. 
Using a camera, the artefact will open a window showing what the camera sees. The video is divided into frames, and each frame is processed. First, an algorithm will detect whether faces are present in the frame. The HOG facial detection algorithm, which was discussed in Section 2.4.2, will be used seeing it is an algorithm that will work on most computers. When a face is detected, it will draw a square around the face. These functions will be performed using the help of OpenCV, which is a free, open-source library used for computer vision and machine learning and the face_recognition library. It was continuously tested and once the program was successfully able to detect a person and draw a square around each face as shown in Figure 5, ‘Build 1’ was complete and a commit was done to GitHub. 
 
Figure 5 - Facial Detection

Build 2: Expression Recognition
In order to determine whether the detected person is confused, neutral or happy, it firstly needs to determine the expression that the person expresses, and therefore the requirement of this build was to determine the facial expression that was detected. 
In order to do this, AI needs to be used. More specifically, a neural network that trains on a dataset of facial expressions, and then creates a model with the correct weights that will predict any newly given images, such as a frame from a camera. Training a model with the correct weights can take weeks or months, and that is not the purpose of this project. The model and weights that were used for this project were obtained from Sefik Serengil, a software engineer from the United Kingdom. These files are part of his GitHub repository under the MIT free-to-use license. The model and weights were downloaded into the artefact dataset file and imported into the code. 



Figure 6 - Loading model and weights
The expression detection process is as follows:
1.	When a face is detected within a frame, it will convert it into an image and grayscale it. 
2.	The image is resized to 48 x 48 pixels for faster processing.
3.	The image is then converted into a NumPy array, and the shape of the array is expanded into a single row with multiple columns. 
4.	The pixels will then be divided by 255 to normalise it to a scale of [0, 1].
5.	These pixels are then given to a prediction function within the trained model, and that will return a prediction for each of the 7 possible expressions in the array. 
6.	The NumPy ‘argmax’ function will then be used to return the array index of the highest prediction, and that index will be used to get the corresponding expression from ‘emotion_label’. 
An ‘OpenCV’ function was used to display the predicted expression under the square that was drawn around the person’s face in ‘Build 1’. It was continuously tested, as seen in Figure 8 and Figure 7, and once the program detected the correct expressions of a person and displayed the associated expression, ‘Build 2’ was done, and committed to GitHub. 
  		 

Build 3: Confusion and UI
“Build 2” made use of a complex AI model to establish the 7 fundamental emotions that can be encountered in classrooms. However, a requirement of the artefact is to determine whether a person is confused.
As explained in Section 2.3.2, anger, disgust, fear and sadness have the same underlying facial characteristics as confusion; thus, if a student is exhibiting any of these emotions, it may be interpreted as confusion. There will be three counters that will count three classifications of ‘emotions’ relevant to the study, namely confused, happy and neural. If a student is detected as fearful, sad, angry or disgusted, the confusion counter will be incremented. If a student is happy, the happy counter will be incremented and if a student is neutral, the neutral counter will be incremented. The counter with the highest value will be referred to as the general sentiment. In each frame of the video, the general sentiment will be displayed, along with the count of the general sentiment in the console and the confusion count will be displayed on the screen as seen in Figure 9.
 
Figure 9 - Confused Counter UI
Additionally, there is a counter that counts the number of faces in the given frame. Figure 10 demonstrates how the confused counter will be divided by the 'totalFaces' counter and multiplied by 100 to obtain the percentage of confused students in a classroom, which will be shown in the console. 
 
Figure 10 - Console Confused Percentage
“Build 3” was continuously tested to see whether the counter works and displayed the correct results. After the testing phase, it was committed to GitHub. 
Build 4: Privacy
Although privacy was not part of the original requirements, it is necessary to consider the Protection of Personal Information Act No.4 of 2013, when using facial detection and recognition; hence, it was thought essential to obscure the students' faces. This was performed using the GaussianBlur function of OpenCV. It blurs the face inside the detection square from ‘Build 1’ and anything else within it.
It was also vital to remove the emotion/expression label beneath a student's detection square to prevent a lecturer from singling out students based on their emotions or expressions presented on screen. As a result, just the word 'student' would be displayed with a student's blurred detection square. Figure 11 depicts how students' privacy will be respected while yet achieving the objective of establishing whether they comprehend the material being presented in class. Unfortunately, one student’s face was not detected due to limitations in the quality of the camera used for testing purposes, and therefore manually removed to ensure their privacy. 
 
Figure 11 – Privacy
After successfully testing the artefact with fellow students, it was determined that the iteration of the artefact works, and privacy is ensured. ‘Build 4’ was then uploaded onto GitHub 

Build 5: Keeping track of general sentiment
The main goal of the artefact is to aid the lecturer in determining whether students comprehend the topic under discussion; nevertheless, the lecturer is sometimes required to work on the board or screen and cannot look at the artefact. This is where general sentiment tracking comes into play. When initiating the artefact, lecturers can specify the time period segments in which they want to monitor the general sentiment based on the topic(s) to be discussed. This is done with a graphical user interface, also known as GUI, with the use of the ‘Tkinter’ python package. 
 
Figure 12 - Time Period GUI
The following is how the artefact keeps track of the general sentiment within a specified time period: 
1.	Three counters called TimeConfused, TimeHappy and TimeNeutral will be initialized. 
2.	Retrieve the ‘presentDate’ using the DateTime function
3.	Obtain the ‘unix_timestamp’ using the DateTime function and the present date
4.	Obtain the ‘newUnixTime’ by rounding the ‘unix_timestamp’
5.	Obtain the ‘initialTime’, which is the same as the ‘newUnixTime’, but outside the while loop.
6.	Initialise a variable called ‘seconds’ with the time specified by a lecturer
7.	The ‘for’ loop continues to execute and increments the TimeConfused, TimeHappy and TimeNeutral depending on the emotion classified by ‘Build 3’. 
8.	The ‘if’ statement will keep track of the time. If the ‘newUnixTime’ is equal to the ‘initialTime’ plus ‘seconds’, then the ‘initialTime’ is set to the ‘newUnixTime’. Simply put, the if statement continuously checks whether five seconds passed, and when five seconds passed the following happens: 
a.	The ‘generalEmotion’ will be set to either Confused, Happy, or Neural depending on the counters that were incremented, and the variables will then be reset before the next time period begins. 
9.	The 'generalEmotion', which represents the period's general sentiment, will subsequently be presented alongside the counters in the console. 
 
Figure 13 - The console after a specified time period
After testing the sentiment tracking function, it was determined that the artefact keeps track of the general sentiment for the time period specified with accurate results, and thereafter committed to GitHub. 
Build 6: Write general sentiment to CSV file
After ‘Build 5’, the general sentiment for the time period is displayed in the console, but the lecturer may need it for future reference to improve classes. Therefore, the General sentiment will be written to an array called ‘genEmotionArr’ in the ‘if’ statement of ‘Build 5’, along with the date and time. After the artefact is terminated, the array values will then be written to a CSV file. If the CSV file already exists, it will append the new date, time and general emotion, otherwise the file will be created, and the data will be written. 
 
Figure 14 - CSV File
The artefact was tested, and after the termination of the artefact, a CSV file was added to the folder, with the data received from the artefact. After comparing the console values and time stamps, it was determined that this function does work. After the successful testing, this build was committed to GitHub. 
Build 7: Read CSV and show results visually
Having the data easily accessible to analyse classes is already highly useful for lecturers, but sometimes a visual representation of the findings is preferable. Therefore, a function was added to read the data from the CSV file and then plot it using the mathplotlib and CSV library.
When examining Figure 15, it is much simpler to understand how a class went and when students did not comprehend the discussed work. This pop-up graph enables you to zoom in and save the graphically presented results.
 
After ‘Build 6’, the function of ‘Build 7’ was tested, and the results were plotted on a graph after the termination of the artefact. This concluded not only ‘Build 7’, but the final artefact. ‘Build 7’ was committed to GitHub. 
3.5	Summary
The iterative life cycle assisted in achieving the goal by developing the artefact in small increments known as builds. It made planning, defining requirements, designing, developing, testing, and deploying the artefact extremely simple and manageable. Each of the seven builds contributed to the design, development, and deployment of the artefact to achieve the objective of creating a method for lecturers to determine if students comprehend the topics discussed in class. The final build of the artefact is uploaded to the GitHub repository. The following is a brief explanation of how the seven builds come together to provide the final artefact that helps address the specified problem.
The artefact utilizes a camera to collect data from the entire classroom. Using the OpenCV library, each face within the video frame will be detected and a square will be drawn around it. OpenCV's GaussianBlur function will be used to blur the faces of individuals for privacy purposes. Each student's facial expression will be determined by the artefact, which will then be used to categorize the student as confused, happy, or neutral. All confused students will be tallied and displayed in bold print at the top of the screen, allowing the lecturer to obtain real-time feedback regarding the number of confused students at any given time. The console will indicate the class's general sentiment as well as the percentage of the class that is currently confused. When initializing the artefact, a Windows form will prompt the lecturer to define the duration throughout which the artefact should monitor the general sentiment. This time period's general sentiment will be written to a CSV file and displayed in the console. After the artefact has been terminated, the data from the CSV file will be read and visualized using a graph. 
CHAPTER 4 – RESULTS
4.1	Evaluation
As stated in Chapter 1, lecturers need a way in large lecture halls to determine whether students understand the topics that are being discussed, seeing as current CFU techniques are mostly used for smaller classrooms. The aim was to create a system that will be helpful to lecturers, by providing them with the ability to “read the room” when it comes to teaching. 
Certain objectives needed to be met as mentioned in Section 1.5.2, in order for the artefact development to be a success. The final objective was to evaluate the artefact and see whether it could identify the common emotions linked to understanding. The artefact went above and beyond that objective. The artefact is a prototype that was thoroughly tested in a test environment, with willing participants. It was also evaluated at an Honours project day and received overwhelmingly positive feedback from lecturers and industry professionals. 
The artefact was successful in doing the following:
1.	Detect all the faces in a frame
a.	The artefact used different tools such as the face_recognition library, OpenCV library and the Histogram of Oriented Gradient algorithm to detect the faces and draw squares around them. 
2.	Determine the facial expressions of each individual student
a.	The artefact used a facial expression model and weights to accurately determine the facial expression of each individual student. Consequently, successfully determining whether the student is expressing either of the following expressions: Anger, disgust, fear, happiness, sadness, surprise, and neutral. 
3.	Classify students as either confused, happy or neutral
a.	The artefact classified whether students are either confused, happy or neutral using the facial expressions determined. Anger, disgust, fear and sadness have some of the same underlying facial characteristics as confusion and could thus be used to classify the students. 
4.	Display the general sentiment for any given time
a.	The artefact counted all the faces that are classified as confused and displayed the number of confused students at the top of the screen. The general sentiment for that moment and the percentage of the class that is confused are also displayed in the console. 
5.	Keep track of the general sentiment for a given time
a.	The lecturer can specify a time that the artefact should keep track of the general sentiment. This is done by using Unix time. It successfully captures the general sentiment of the classroom for a specified time period, displays it in the console and writes it to an array. 
6.	Save the general sentiment for later analysis
a.	After the artefact is terminated, the general sentiment data is written to a CSV file that the lecturer can use for later analysis. 
7.	Display the data on a graph
a.	The data contained within the CSV file is read and displayed on a graph to provide a graphical representation of the data. This is accomplished with the matplotlib library. The graph can be saved or modified at the lecturer's discretion. 
In short, the artefact displays the large lecture hall with students’ faces blurred out, it displays the number of confused students at any given time. It keeps track of the general sentiment for a specified time period and writes the data to a CSV file. After the artefact is terminated, the data is visually represented on a graph. This helps the lecturers to have a real-time understanding of whether the class is comprehending the topics being discussed, as well as storing that data for later analysis, which the lecturer can use to improve lectures or provide additional resources. 
The artefact successfully allows lecturers to determine the general sentiment of large classrooms, which would otherwise not be possible. It consequently achieved the aim of creating an artefact that will assist lecturers to “read the room” in large classrooms. The artefact is a very useful tool for lecturers and should be researched and developed further.
4.2	Limitations
The following limitations were experienced during the project:
•	Difficulty picking up the Neutral facial expression because of a phenomenon where a Neutral facial expression can be picked up as angry or sad. 
•	The camera used was a webcam, and therefore could not determine faces further away from the camera. 

4.3	Recommendations
The following are recommendations for any future research pertaining to this project:
•	Develop a model that uses a dataset specifically trained with the confusion emotion or expression for a more accurate reading of confusion
•	Use a high-resolution camera, rather than a webcam

4.4	Conclusion
This project was a huge success and showed it is possible to create a system that assists lecturers in large lecture halls. The project had a few limitations such as the difficulty classifying some people with neutral facial expressions, and the technology such as the webcam that was used to capture the classroom and test environment. 
This project's artefact is a proof-of-concept to demonstrate that it is possible to design a program that will assist instructors in determining whether or not students in large lecture halls comprehend the topics being discussed. This study should serve as a basis for future research that attempts to construct a full-scale version. 


 
CHAPTER 5 – REFLECTION
5.1	What did I learn while completing this project
The completion of a project is not something that can be accomplished in a week, nor can it be rushed. Various choices had to be taken, such as what issue to address and what objectives to pursue in order to address the problem. What paradigms, methodologies, and life cycles will be utilized? All of these choices were crucial to the creation of a successful artefact.
It handles the issue of assisting lecturers in large lecture halls, which is one of the many benefits of the artefact. It enables lecturers to obtain both real-time feedback and data that can be utilized to analyse the classes. The project plan was an advantage of the procedures. It enabled me to keep on schedule and deliver the required documentation as necessary.
Limitations like as technology and the availability of confused datasets constituted downsides of the artefact. For the procedures, it was difficult to determine which life cycle best suited the development of the artefact.
If I had to redo the project, I would use a dashboard to present and analyse the captured data in a more effective manner. I would attempt to integrate it with a Kafka database to enable data streaming in real-time to a dashboard.  I would also try to develop a new dataset that incorporates the confused emotion, to build an even more accurate model for predicting the expressions. 
5.2	Did I achieve the objectives that were set for the project?
There were seven objectives that were set out in Chapter 1, which needed to be completed in order for this project to be a success and for the artefact to address the research problem. The following are the objectives that needed to be met: 
•	Research current methods to determine if students understand their work
•	Analyse facial expressions and how those emotions are displayed
•	Exploring facial detection techniques and how it works
•	Survey lecturers and students to get a better understanding of what would be beneficial to them
•	Comparison between different types of facial detection and expression programs 
•	Developing an artefact using the above-mentioned programs 
•	Evaluating if the proposed artefact will be able to identify common emotions linked to understanding 
All the above-mentioned objectives were met, and consequently, a successful artefact was developed. 
5.3	How successful was I in managing the project and meeting target dates?  
Although it was a year project, time management was crucial in successfully completing the project. It may seem like a lot of time, but most students lost track of time. Putting in the necessary time each day, weekends and recesses were needed to meet all the target dates set out for us. You need to decide when you’ll be working on that project and stick to the planning you made. 
It wasn’t always easy and there definitely were some days or even weeks that little or no progress was made. These times were mostly during exams and semester weeks. I then had to work some late nights at my apartment, or the labs in times of loadshedding to catch up, to ensure that I will reach the deadlines. Overall, I believe I was successful in managing the project and met all the target dates. 
5.4	Conclusion
Throughout the duration of this project and the creation of an artefact, I gained a great deal of knowledge. It was ultimately worthwhile to design an artefact that would aid lecturers in large lecture rooms, despite the arduous effort and late nights required. Throughout this undertaking, my decision-making, time management, and willpower increased. All aims and objectives were accomplished, and I received excellent feedback from lecturers and industry professionals.
Overall, the project was a resounding success, since the goal of developing an artefact that will assist lecturers in determining whether students in a large lecture hall understand the topics was accomplished. Working with my supervisor to produce this artefact has helped me realize that I want to continue my studies at the university, conducting other research and developing additional artefacts that will better the lives of others. 
 
BIBLIOGRAPHY
Alshamrani, M.  2019.  An investigation of the advantages and disadvantages of online education.  Auckland University of Technology. - 
Andersson, A. 2008. Assessment: A continuous process that takes place at the end? Sweden: School of Education and
Communication.
Angelo, T. A. and Cross, K. P.  1993.  Classroom assessment techniques - a handbook for college teachers. 2 ed.  San Francisco: Jossey-Bass Wiley.
Anil, J. and Suresh, L. P.  2016.  Literature survey on face and face expression recognition. (In 2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT): IEEE, p. 1-6).
Apuke, O. D.  2017.  Quantitative research methods: A synopsis approach.  Kuwait Chapter of Arabian Journal of Business and Management Review, 33(5471):1-8.
Arguel, A., Lockyer, L., Kennedy, G., Lodge, J. M. and Pachman, M.  2019.  Seeking optimal confusion: A review on epistemic emotion management in interactive digital learning environments.  Interactive Learning Environments, 27(2):200-210.
Arsenovic, M., Sladojevic, S., Anderla, A. and Stefanovic, D.  2017.  Facetime—deep learning based face recognition attendance system. (In 2017 IEEE 15th International symposium on intelligent systems and informatics (SISY): IEEE, p. 000053-000058).
Balla, P. B. and Jadhao, K.  2018.  Iot based facial recognition security system. (In 2018 international conference on smart city and emerging technology (ICSCET): IEEE, p. 1-4).
Bernstein, C.  2020.  Face detection. https://www.techtarget.com/searchenterpriseai/definition/face-detection#:~:text=Face%20detection%20%2D%2D%20also%20called,human%20faces%20in%20digital%20images. 2022.
Bhat, B. and Bhat, G.  2019.  Formative and summative evaluation techniques for improvement of learning process.  European Journal of Business & Social Sciences, 7(5):776-785.
Böheim, R.  2020.  The behavior of student hand-raising as an observable indicator of student engagement: Exploring the role of hand-raising in classroom learning and its relation to student motivation.  Universität München. - 
Butt, M. N. and Iqbal, M.  2011.  Teachers' perception regarding facial expressions as an effective teaching tool.  Contemporary Issues in Education Research, 4(2):11-14.
Cherney, K.  2021.  All about alexithymia, or difficulty recognizing feelings. https://www.healthline.com/health/autism/alexithymia 
D’Mello, S. and Graesser, A.  2012.  Dynamics of affective states during complex learning.  Learning and Instruction, 22(2):145-157.
D’Mello, S. K. and Graesser, A. C.  2014.  Confusion. (In  International handbook of emotions in education: Routledge.  p. 299-320).
Delac, K. and Grgic, M.  2004.  A survey of biometric recognition methods. (In Proceedings. Elmar-2004. 46th International Symposium on Electronics in Marine: IEEE, p. 184-193).
Dilawar, M. M. and Siddiqui, S.  2016.  Face label graph matching for character identification.  INTERNATIONAL JOURNAL, 1(1).
Dimberg, U., Thunberg, M. and Elmehed, K.  2000.  Unconscious facial reactions to emotional facial expressions.  Psychological science, 11(1):86-89.
Dixson, D. D. and Worrell, F. C.  2016.  Formative and summative assessment in the classroom.  Theory into practice, 55(2):153-159.
Dresch, A., Lacerda, D. P. and Antunes, J. A. V.  2015.  Design science research. (In  Design science research: Springer.  p. V).
Dufour, C.  2021.  The new teacher's guide to overcoming common challenges. Ticker: The academic business librarianship review  New York: Routledge.
Ekman, P. and Keltner, D.  1997.  Universal facial expressions of emotion.  Segerstrale U, P. Molnar P, eds. Nonverbal communication: Where nature meets culture, 27:46.
Finley, T.  2014.  Dipsticks efficient ways to check for understanding. https://static1.squarespace.com/static/5ade38cf7e3c3a8e0fd03b28/t/5afc377570a6ad438c26e9e3/1526478710344/dipsticks__efficient_ways_to_check_for_understanding___edutopia.pdf 
Fisher, D. and Frey, N.  2014.  Checking for understanding: Formative assessment techniques for your classroom. 2 ed.: Association for Supervision and Curriculum Development.
Gargaro, D.  2022.  The pros and cons of facial recognition technology. https://www.itpro.com/security/privacy/356882/the-pros-and-cons-of-facial-recognition-technology 
Garrison, C. and Ehringhaus, M.  2007.  Formative and summative assessments in the classroom, (In, Louisville: NMSA).
Garvie, C. and Frankle, J.  2016.  Facial-recognition software might have a racial bias problem.  The Atlantic, 7.
Geng, C. and Jiang, X.  2009.  Face recognition using sift features. (In 2009 16th IEEE international conference on image processing (ICIP): IEEE, p. 3313-3316).
Government.  1996.  The constitution of the republic of south africa.  South Africa: South African Government, p. 11.
Government.  2019.  Protection of personal information act. https://popia.co.za/section-13-collection-for-specific-purpose/ Date of access: 2022/04/10 
Gupta, S., Thakur, K. and Kumar, M.  2021.  2d-human face recognition using sift and surf descriptors of face’s feature regions.  The Visual Computer, 37(3):447-456.
Heale, R. and Twycross, A.  2015.  Validity and reliability in quantitative studies.  Evidence-based nursing, 18(3):66-67.
Hunt, E. and Pellegrino, J. W.  2002.  Issues, examples, and challenges in formative assessment.  New directions for Teaching and Learning, 2002(89):73-85.
Institute, A. L.  2019.  Beyond face value: Public attitudes to facial recognition technology.
Ishaq, K., Rana, A. M. K. and Zin, N. A. M.  2020.  Exploring summative assessment and effects: Primary to higher education.  Bulletin of Education and Research, 42(3):23-50.
Jirava, P.  2004.  System development life cycle.  Scientific papers of the University of Pardubice. Series D Faculty of Economics and Administration. 9 (2004).
Kar, N., Debbarma, M. K., Saha, A. and Pal, D. R.  2012.  Study of implementing automated attendance system using face recognition technique.  International Journal of computer and communication engineering, 1(2):100.
Kettner, J. L.  2015.  Clickers versus hand-raising in the physics college classroom: Do clickers make a difference?
Khan, M., Chakraborty, S., Astya, R. and Khepra, S.  2019.  Face detection and recognition using opencv. (In 2019 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS): IEEE, p. 116-119).
Kivunja, C. and Kuyini, A. B.  2017.  Understanding and applying research paradigms in educational contexts.  International Journal of higher education, 6(5):26-41.
Kohler, C. G., Turner, T., Stolar, N. M., Bilker, W. B., Brensinger, C. M., Gur, R. E. and Gur, R. C.  2004.  Differences in facial expressions of four universal emotions.  Psychiatry research, 128(3):235-244.
Kopf, S., Scheele, N. and Effelsberg, W.  2005.  The interactive lecture: Teaching and learning technologies for large classrooms.
Korshunov, P. and Marcel, S.  2018.  Deepfakes: A new threat to face recognition? Assessment and detection.  arXiv preprint arXiv:1812.08685.
Kuhn, T. S.  1970.  The structure of scientific revolutions. Chicago University of Chicago Press.
Kumar, A., Kaur, A. and Kumar, M.  2019.  Face detection techniques: A review.  Artificial Intelligence Review, 52(2):927-948.
Lamanauskas, V. and Makarskaite-Petkeviciene, R.  2021.  Distance lectures in university studies: Advantages, disadvantages, improvement.  Contemporary Educational Technology, 13(3).
Li, T., Hou, W., Lyu, F., Lei, Y. and Xiao, C.  2016.  Face detection based on depth information using hog-lbp. (In 2016 Sixth International Conference on Instrumentation & Measurement, Computer, Communication and Control (IMCCC): IEEE, p. 779-784).
Libby, C. and Ehrenfeld, J.  2021.  Facial recognition technology in 2021: Masks, bias, and the future of healthcare.  Journal of Medical Systems, 45(4):1-3.
Lindeberg, T.  2012.  Scale invariant feature transform.
McTighe, J.  2021.  8 quick checks for understanding. https://www.edutopia.org/article/8-quick-checks-understanding 
Mohammad, S. M.  2020.  Facial recognition technology.  Available at SSRN 3622882.
Nagpal, G. S., Singh, G., Singh, J. and Yadav, N.  2018.  Facial detection and recognition using opencv on raspberry pi zero. (In 2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN): IEEE, p. 945-950).
Okesola, O. J., Adebiyi, A. A., Owoade, A. A., Adeaga, O., Adeyemi, O. and Odun-Ayo, I.  2020.  Software requirement in iterative sdlc model. (In Computer Science On-line Conference: Springer, p. 26-34).
Omoyiola, B. O.  2018.  Overview of biometric and facial recognition techniques.  IOSR journal of computer engineering (IOSRJCE), 20(4):1-5.
Owayjan, M., Dergham, A., Haber, G., Fakih, N., Hamoush, A. and Abdo, E.  2015.  Face recognition security system.  New trends in networking, computing, E-learning, systems sciences, and engineering:343-348.
Pang, Y., Yuan, Y., Li, X. and Pan, J.  2011.  Efficient hog human detection.  Signal processing, 91(4):773-781.
Park, Y. S., Konge, L. and Artino, A. R.  2020.  The positivism paradigm of research.  Academic Medicine, 95(5):690-694.
Paudel, P.  2021.  Online education: Benefits, challenges and strategies during and after covid-19 in higher education.  International Journal on Studies in Education, 3(2):70-85.
Peffers, K., Tuunanen, T., Rothenberger, M. A. and Chatterjee, S.  2007.  A design science research methodology for information systems research.  Journal of management information systems, 24(3):45-77.
Ping, S. T. Y., Weng, C. H. and Lau, B.  2003.  Face detection through template matching and color segmentation.  Nevim: Nevim, 89.
Pochedly, J. T., Widen, S. C. and Russell, J. A.  2012.  What emotion does the “facial expression of disgust” express?  Emotion, 12(6):1315.
Price, J. H. and Murnan, J.  2004.  Research limitations and the necessity of reporting them.  American journal of health education, 35(2):66.
Qu, W. and Zhang, C.  2013.  The analysis of summative assessment and formative assessment and their roles in college english assessment system.  Journal of Language Teaching and Research, 4(2):335.
Quinzio-Zafran, A. M. and Wilkins, E. A.  2020.  The new teacher's guide to overcoming common challenges: Curated advice from award-winning teachers. Routledge.
Reed, L. I., Zeglen, K. N. and Schmidt, K. L.  2012.  Facial expressions as honest signals of cooperative intent in a one-shot anonymous prisoner's dilemma game.  Evolution and Human Behavior, 33(3):200-209.
Rehman, A. A. and Alharthi, K.  2016.  An introduction to research paradigms.  International Journal of Educational Investigations, 3(8):51-59.
Revina, I. M. and Emmanuel, W. S.  2021.  A survey on human face expression recognition techniques.  Journal of King Saud University-Computer and Information Sciences, 33(6):619-628.
Russel, S. J. and Norvig, P.  2021.  Artificial intelligence - a modern approach. Pearson Fourth ed.  New Jersey: Pearson Education Inc.
Sathik, M. and Jonathan, S. G.  2013.  Effect of facial expressions on student’s comprehension recognition in virtual educational environments.  SpringerPlus, 2(1):1-9.
Shylesh, S.  2017.  A study of software development life cycle process models. (In National Conference on Reinventing Opportunities in Management, IT, and Social Sciences, p. 534-541).
Silvia, P.  2013.  Knowledge emotions: Feelings that foster learning, exploring, and reflecting. https://nobaproject.com/modules/knowledge-emotions-feelings-that-foster-learning-exploring-and-reflecting Date of access: 2022/10/01 2022.
Smith, A.  2019.  More than half of us adults trust law enforcement to use facial recognition responsibly.  Pew Research Center, 5.
Sousa, F. J.  2010.  Metatheories in research: Positivism, postmodernism, and critical realism. (In  Organizational culture, business-to-business relationships, and interfirm networks: Emerald Group Publishing Limited.).
Sporer, J. (2022) 'Is summative or formative more important?'. Available at: https://faq-blog.com/is-summative-or-formative-more-important.
Sukamolson, S.  2007.  Fundamentals of quantitative research.  Language Institute Chulalongkorn University, 1(3):1-20.
Sürücü, L. and Maslakçi, A.  2020.  Validity and reliability in quantitative research.  Business & Management Studies: An International Journal, 8(3):2694-2726.
Taras, M.  2008.  Summative and formative assessment: Perceptions and realities.  Active learning in higher education, 9(2):172-192.
Taylor, G. J.  2000.  Recent developments in alexithymia theory and research.  The Canadian Journal of Psychiatry, 45(2):134-142.
Tosuncuoglu, I.  2018.  Importance of assessment in elt.  Journal of Education and Training Studies, 6(9):163-167.
üge Çarıkçı, M. and Özen, F.  2012.  A face recognition system based on eigenfaces method.  Procedia Technology, 1:118-123.
Vazquez-Fernandez, E. and Gonzalez-Jimenez, D.  2016.  Face recognition for authentication on mobile devices.  Image and Vision Computing, 55:31-33.
Woodward Jr, J. D., Horn, C., Gatune, J. and Thomas, A.  2003.  Biometrics: A look at facial recognition. (In. Santa Monica: RAND CORP SANTA MONICA CA.).
Wright, E.  2018.  The future of facial recognition is not fully known: Developing privacy and security regulatory mechanisms for facial recognition in the retail sector.  Fordham Intell. Prop. Media & Ent. LJ, 29:611.
Yang, Z. and Hirschberg, J.  2018.  Predicting arousal and valence from waveforms and spectrograms using deep neural networks. (In Interspeech, p. 3092-3096).
Youssif, A. A. and Asker, W. A.  2011.  Automatic facial expression recognition system based on geometric and appearance features.  Computer and Information Science, 4(2):115.
Yu, H. and Liu, H.  2015.  Combining appearance and geometric features for facial expression recognition. (In Sixth International Conference on Graphic and Image Processing (ICGIP 2014): SPIE, p. 40-45).

 
APPENDIX A – Questionnaire 
A survey was done among Bachelor of Science in Information Technology students from the North-West University in order to get a better understanding of whether the proposed system would be useful. The survey link was uploaded to Efundi which is a portal and website students, and the university uses. The survey was non-compulsory, and the students were also informed thereof. 
The following Table 7 shows the close-ended questions asked and the survey results.
Table 7 - Close-ended Survey Questions
Question	Yes	No
Are you aware this Survey is optional and not compulsory?	136
100%	0
0%
Do you give consent for me, Hano Strydom, to use the information gathered as part of my research?	136
100%	0
0%
Are you a shy person?	84
62%	52
38%
Will you raise your hand to ask a question in a classroom with more than sixty people?	88
65%	48
35%
Would it be convenient if a lecturer would know whether you do not understand a topic? (Without direct communication)	129
95%	7
5%
Would you give consent to be recorded in real-time to collect facial features and expressions, without storing any video files?	107
79%	29
21%

There was also one that gave an option to provide their own answers shown in Table 8. The first four answers were given, and the final three answers were entered by the students. 
Table 8 - Survey Question
Which of these methods do you prefer when you do not understand a topic?
Answers	Results	Percentage of Students
Put up your hand in class	21	15%
Review work alone after class	107	79%
Consult with your peers/students	89	65%
See the lecturer in his/her office time	69	51%
Additional resources on Efundi / Internet	1	1%
Google	3	2%
YouTube	4	3%

 
APPENDIX B – Ethics Form
ITRI 671 Research ethics form: Honours project 
 
Name: Hano Strydom 
Title of project: Facial Recognition as a tool to determine if students understand a topic 
Supervisor: Mnr. Henri van Rensburg 
Starting and end dates of project:  24/02/2022 – 30/10/2022 
 
1. Have you read the information available related to research ethics (Chapter 5 of Researching Information Systems and Computing; BJ Oates and Chapter 13 of Writing for computer science, J Zobel; Manual for post graduate studies, available on efundi)? 	Yes 
 
X 	No 
 
2. Do you make use of people as source of data in your project (for example the completion of questionnaires or evaluation of products)? 	Yes 
X 	No 

3. Are there any aspects of your research that you need permission from another party to use (for example use of property or tools)? If yes, provide more detail.  	Yes 	No
X

4. Describe your research question and give a short description of your plans for the collection of data. 
 
Research Question: 
Lecturers, teachers, and professionals need to be able to “read the room” without physically having to pay attention to each individual student.  
This study will attempt to answer the question of whether there is a a way for lecturers to determine how many students understand the current topic at hand, without physically paying attention to the individual students.  
 
Data Collection: 
Questionnaire 
-	During this research, students will be asked to participate in an anonymous questionnaire. 
-	https://forms.gle/K97FSaHBnQSVDFef9 Face ‘recording’ 
-	During my artifact stage, participants will be used in order to determine the accuracy of the facial expression recognition software.  
 
 
5. Describe how you plan to provide information about yourself and the goals of your research to participants. 
 
1.	I will firstly introduce myself and tell them what I am studying. 
a.	Hano Strydom studying Hons. Computer Science 
 
2.	I will make them aware about what the project and my research is about. 
a.	“Facial Recognition as a tool to determine if students understand a topic” 
 
3.	I will provide information about the survey 
 
a.	Time it takes to complete the survey (2 minutes) 
b.	It is entirely optional, and not personal information such as names and emails will be stored 
c.	All data gathered from the questions will be strictly confidential, and only be used for research purposes.  
 
 
 
6. Describe what methods you will use to get permission from participants in your study. 
 
Seeing the survey is entirely optional, and the users first needs to answer 2 questions before they are allowed to answer the survey: 
1.	Are you aware this Survey is optional and not compulsory?
2.	Do you give consent for me, Hano Strydom, to use the information gathered as part of my research?

For the software I will ask participants permission, and if they do not accept, they do not have to participate.  
 
 
7. Will you be able to ensure that participants’ information will be used in an anonymous, private and confidential way? How? 
 
The survey is a google docs form. Google is very secure, and the statistics gathered from the survey will only be used in my research and not stored locally or shared with any other person.  
 
The software doesn’t store any user data, and the faces are blurred out. Only facial expressions are extracted and used to determine their emotion.  
 	Yes 
 
 
X 	No 
 
8. Are there any foreseeable risks of damage (physical, social or psychological) to participants or the environment? If you answer yes, give detail of the preventative measures you will follow. 
	Yes 
 
 
 	No 
X
 
9. Are there any foreseeable risks to the NWU, for example lawful actions that may follow the research, or damage to the image of the university? If yes, give detail.	Yes 	No 
X
 
10. Are there any other ethical issues that may occur during the execution of the research (for example conflicting interests)? If yes, provide detail and explain how you plan to handle them. 	Yes 	No X 
 
 
 
 
 		
 
I declare that the information contained in this form is accurate. I have attempted to identify the risks that may arise in conducting this research and acknowledge my obligations and the rights of the participants. I confirm that the research will be conducted in line with all University, legal and ethical standards. 
 
Name of student: Hano Strydom 
Signature:  
Date: 2022/08/25 
 
Name of study leader: Mnr. Henri van Rensburg 
Signature: 
Date:  2022/08/25



