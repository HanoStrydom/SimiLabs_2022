ITRI626: ARTIFICIAL 			INTELLIGENCEASSIGNMENT 4: DEEP NEURAL NETWORKS AND BACKPROPAGATION			28 August 2022Llewellyn Anthony	32969694Prof. T. du ToitLIST OF FIGURESLIST OF TABLESNo table of figures entries found.INTRODUCTIONDeep networks are efficient due to their ability to execute local computations followed by pointwise non-linearities over progressively wider receptive fields, as well as due to the ease of use and scalability of the gradient-descent training method based on backpropagation (Ionescu et al., 2015:2965). Successful utilization of the backpropagation principle is a crucial step because the computation of the gradient is the only prerequisite for neural network learning (Ionescu et al., 2015:2967). Backpropagation is an algorithm for quickly calculating the gradient of loss with respect to the provided parameters. The technique computes gradients in a recursive manner according to the inputs to the layers as well as their parameters (Ionescu et al., 2015:2967). A backpropagation network combines basic neurons, also known as nodes, in a layered hierarchical architecture with high interlayer connections (Wythoff, 1993:119). BACKPROPAGATION AS NEURAL NETWORK LEARNING TECHNIQUEA multi-layered perceptron network consists of the following components:A network can consist of three layers: input, hidden and output layers (Kishore & Kaur, 2012:3).These layers are made up of the neurons that join to form the network as a whole (Kishore & Kaur, 2012:3).On the connections that indicate the signal strength, weights are assigned (Kishore & Kaur, 2012:3). Based on the input signal and the error function that has been returned to the input layer, the weight values are calculated (Kishore & Kaur, 2012:3).The function of the hidden layer is to modify the connection weights in accordance with the input signal and error signal (Kishore & Kaur, 2012:3).A multi-layer perceptron network can be trained as follow:This algorithm can operate in batch mode, where the weight updates happen after several propagations, or via using incremental mode, where the weight updates happen after each propagation (Kishore & Kaur, 2012:3). Due to smaller time consumption and fewer propagative iterations, batch mode is typically used (Kishore & Kaur, 2012:3). In order to train the network with a pre-set collection of data classes, training data samples must first be delivered at the input layer (Kishore & Kaur, 2012:3).Finally, during the testing step, test data are delivered to the input layer to enable prediction of the applied patterns (Kishore & Kaur, 2012:3).The network is already aware of the expected output. Therefore, if the computed output differs from the desired output, the difference in the result is backpropagated to the input layer so that the perceptrons' connection weights can be changed to minimize error (Kishore & Kaur, 2012:3). The procedure is repeated until the inaccuracy is hardly detectable (Kishore & Kaur, 2012:3).This algorithm's simplicity and suitability to offer a solution to all complex patterns make it beneficial to utilize in training neural networks (Kishore & Kaur, 2012:3).Furthermore, depending on the quantity of data included in the layers, this algorithm's execution is quicker and more effective (Kishore & Kaur, 2012:3).Figure 1: A backpropagation neural network (Kishore & Kaur, 2012:3)Backpropagation as a neural network training technique does have its drawbacks unfortunately:It requires a lot of iterations to converge, making it unsuitable for online real-time learning (this forces the use of small networks, making it impossible to solve many challenging problems of current interest) (Miller et al., 1990:1561).Requires a lot of computations per iteration, making the algorithm slow unless it is implemented on expensive custom hardware (Miller et al., 1990:1561).It can have an error surface that can have relative minima (Miller et al., 1990:1561).It does not permit successful incremental learning (if one has finite time to train), in that all inputs must be seen before any weight change can occur in order to achieve reasonable convergence (Miller et al., 1990:1561).Applying the gradient search technique results in a delayed convergence to an ideal solution (Örkcü & Bal, 2011:3704).RESULTSThe code below demonstrates a practical example of training a neural network via using backpropagation:Figure 2: Neural Network Training ExampleThe results of running the neural network are below:Figure 3: Neural Network OutputCONCLUSIONBy adding more layers to the network, we can increase the accuracy of predictions being made. However, a large number of iterations must occur to properly train the network. The cause of errors being so large is a result of overfitting brought on by the relationship between the input and output layer's sizes. We can observe that the excessive use of hidden nodes is determined by the matrix satisfied by the hidden layer sizes. This will lead to overfitting; hence it is crucial to choose the number of hidden nodes appropriately to offer lower error rates and higher accuracy. If the number of hidden layer nodes is likewise insufficient, it will result in underfittingBIBLIOGRAPHYIonescu, C., Vantzos, O. & Sminchisescu, C. 2015. Matrix backpropagation for deep networks with structured layers. In. Proceedings of the IEEE international conference on computer vision. pp. 2965-2973.Kishore, R. & Kaur, T. 2012. Backpropagation algorithm: An artificial neural network approach for pattern recognition. International Journal of Scientific & Engineering Research, 3(6):1-4.  Date of access: 27 Sept 2022. Miller, W.T., Glanz, F.H. & Kraft, L.G. 1990. Cmas: An associative neural network alternative to backpropagation. Proceedings of the IEEE, 78(10):1561-1567.  Date of access: 27 Sept 2022. Örkcü, H.H. & Bal, H. 2011. Comparing performances of backpropagation and genetic algorithms in the data classification. Expert systems with applications, 38(4):3703-3709.  Date of access: 27 Sept 2022. Wythoff, B.J. 1993. Backpropagation neural networks: A tutorial. Chemometrics and Intelligent Laboratory Systems, 18(2):115-155.  Date of access: 27 Sept 2022.
ITRI614 INDUSTRY ASSIGNMENT:RESEARCH REPORT: PROJECT MANAGEMENT METHODOLOGIES AND FAILURESL. Anthony orcid.org/0000-0000-0000-000XThesis submitted for the degree Bachelor of Sciences in Computer Science and Information Systems at the North-West UniversitySupervisor:	Prof. Neels KrugerCo-supervisor:	N/AGraduation: 31 December 2022Student number: 3296969Table of contentsList of TablesNo table of figures entries found.List of FiguresABSTRACTWithin most organisations the desire for financial gain and competitive advantage drives technological advancement. Shorter project life spans, rapid replacements, and obsoleteness are becoming more frequent in Information and Communications Technology (ICT) advancements. Most organisations struggle to efficiently and effectively manage and commit ICT initiatives, including to develop a lasting ICT environment that is beneficial to human and social demands. Many ICT projects have failed miserably, and CEOs are beginning to demand explanations as to why. The researcher has been named the new Chief Information Officer (CIO) of company XYZ and have been given the task to discuss which relevant project management approaches the individual think will be the most beneficial in the next three to five years. The researcher needs to debate why organisations, portfolio, program, and project managers should take notice of current relevant project management methodologies in addition to how the Management of company XYZ should strategize and conduct planning, catering, and manage projects to maximize potential benefits and eliminate all risks related with the chosen project management approaches, while also considering the requirements of internal and external clients and the forces they represent. The focus should not only fall on existing and future project management methodologies like projects in controlled environments, such as Six Sigma, Rational Unified Process (RUP), Agile, PRINCE2, and so on, but also on bodies of knowledge like PMBOK and governance frameworks like COBIT and ITIL. The general idea is to overcome the constraints typically associated with projects, namely time, scope and budget.Keywords: Project Management, Project Management Methodologies, Bodies of Knowledge, Governance Frameworks, Risk, SCRUM, APM, PRINCE2, COBIT 2019, ITIL, CMM/CMMI, PMBOK, P2M, APMBOKChapter 1: INTRODUCTIONA Quick OverviewThe main aim of this report is to address constraints typically associated with projects in the industry, and to find the most applicable project management methodologies, bodies of knowledge and governance frameworks to follow in order to mitigate risks and maximize opportunities for projects. Extensive research on project management trends, project management methodologies and their capabilities, bodies of knowledge (project management standards) as well as governance frameworks exist. According to Paredes and Ribeiro (2018:641), sustainability and management of projects along with the value and benefit of projects is becoming an increasing important factor when managing projects. Projects are becoming increasingly complex and thus more suspectable to changes within the scope of the project (Paredes & Ribeiro, 2018:642). Growth and diversification of stakeholders, extended project scopes, higher diversities of cultures within organisations, virtualization of projects as well as the increasing amount of information have a lasting impact on modern projects (Paredes & Ribeiro, 2018:642). Change management have become more important to address changes within projects as well as to communicate the changes to stakeholders (Paredes & Ribeiro, 2018:642). Agile and hybrid methods of managing projects and their lifecycle is a key trend within the current industry. Agile methods are more suited for the current industry, because it supports change management, teams can consist of fewer members, increased delivery times, changes to source code is less expensive and rapid development is beneficiary (Paredes & Ribeiro, 2018:642). SCRUM, Kanban and XP are but a few of the examples. Benefits management is important, because benefits are connected to changes a project undergoes as well as the deliveries (Paredes & Ribeiro, 2018:643). Benefit management aims to allow project to meet the specified stakeholder requirements while also increasing the benefits a project provides (Paredes & Ribeiro, 2018:643). To assure accuracy and relevancy, a paradigm shift has also occurred in the sustainable project management stages of planning, monitoring, implementing, and assessment. (El Khatib et al., 2020:1278). Artificial intelligence and other modern technologies have been implemented in the industry to mitigate project risks and successfully manage projects (El Khatib et al., 2020:1278).  In the modern era, the range of skills required by a project manager determines a project's viability. (El Khatib et al., 2020:1278). Projects in the future should be able to support multilingual and multicultural capabilities (Jugdev et al., 2009:234), be more remotely based (Jugdev et al., 2009:234), specialized project managers will be needed in accordance to political and legal factors (Jugdev et al., 2009:235), sustainable and green projects that contributes to ecological preservation (Jugdev et al., 2009:235), a demand in project managers that knows how to implement new technologies that supports project management will increase (Jugdev et al., 2009:236). Numerous projects fail because they do not meet the abovementioned requirements. According to Whittaker (1999:23), KPMG's 1997 Survey of Unsuccessful Information Technology Projects discovered the three most prevalent causes regarding project failure are poor project planning, flawed business cases as well as a lack of involvement and support from senior management. Among other failures the survey highlighted were that projects were more prone to failing due to schedule than budget overruns (Whittaker, 1999:23). Many projects also fail because of the implementation of new/unproven technology, especially when they are poorly comprehended by developers (Whittaker, 1999:23). Failing to stipulate project requirements and failing to understand them is also a major contribution to project failures (Whittaker, 1999:23). It is also important to note that nearly 60% of unsuccessful projects were scheduled to be completed within a year (Whittaker, 1999:23). Other sources, like Attarzadeh and Ow (2008:236), state that other major causes of project failures are cost and schedule estimations and planning that went awry, project scope changes (scope creeping) and the incorrect use of project management methodologies (Attarzadeh & Ow, 2008:236). There is also a human factor involved i.e. project managers with inadequate skills and that are not trained to manage projects properly (Attarzadeh & Ow, 2008:236). Lack of communication between stakeholders and the project team is also a major issue. Nelson (2007) mentions that employee morale plays a critical role in a project’s success, and that motivation, skills and team member relationships plays a key role and can lead to disastrous results if one of the forementioned is not present in project team dynamics. The human inclination to underestimate and overestimate timetables, among other issues, makes the failure of a project more eminent by not properly defining the scope of a project, which has a negative impact on project planning (Nelson, 2007). Requirements shortcomings and/or quality assurance also negatively impact projects (Nelson, 2007).  Poor estimating also places undue strain on team members, resulting in lower morale and productivity (Nelson, 2007). Failing to assess risks properly and implementing a contingency plan also results in project failures (Nelson, 2007).From the sources above, one can speculate why projects can fail so easily and why it can be challenging to effectively manage projects, aim for a successful completion, and also provide the promised project benefits. This study aims to study how project management approaches and project success are related. The Abstract section provided a quick overview of the research problem, the Introduction provided a quick overview of the study with background information. The Related Works section will cover relevant literature that exists on the research topic. The System Model discusses hypotheses about the research topic. The hypotheses will be based on the content from the Introduction and Relevant Works. The Problem Statement will give a brief overview of the research problem. Project Management Methodologies, Bodies of Knowledge and Governance Frameworks will respectively have sections, the essence of each as well as the pros and cons will be discussed in those sections. The Solution section provides an overview of the functionality of the proposed solution: what PM methodologies/frameworks/bodies of knowledge are best suited to manage modern projects. The Analysis section will provide statistics on the project management methodologies, comparing them to each other and pointing out their success rates. Finally, a conclusion will be derived from the content in this report, indicating the most applicable project management methodology that will contribute to project success.Chapter 2: RELATED WORKSA General OverviewAs mentioned in the Introduction, extensive research exists on the research question. This section covers a few existing literature samples on the research question. Does there exist a relationship between project management methodologies being implemented by project managers and the success rate of projects? According to Pace (2019:62), there is not always a link between the use of a project management approach and the success rate of a project. It is difficult to maintain a consistent project success rate (Pace, 2019:62). Projects can often fail due to scope variance, failing to understand the problem and uncertainty, complexity as well as vaguely defined requirements (Pace, 2019:62). It is difficult to set a performance criteria standard, as projects differ in scope and requirements (Pace, 2019:62). One project methodology implemented in one project successfully might not be suited at all for another project (Pace, 2019:62). Thus, it remains a challenge to gain consistent project success (Pace, 2019:62). Project Management Methodologies are only as good as the clarity of objectives and the clarity of procedures of projects (Pace, 2019:62). The majority of these approaches are chosen and implemented at the beginning of a project and are not modified as the project develops to meet new scope needs (Pace, 2019:62). Shifting from a Six-Sigma methodology to a Critical Chain methodology, for example, is an extremely challenging transition (Pace, 2019:62). Static implementation of methodologies is effective when there is little ambiguity or complexity within a project, but it is useless in today's environment in terms of volatility, uncertainty, complexity and ambiguity (Pace, 2019:62). As trends tended to change during the COVID-19 pandemic, systems became more digitized, and more people started to work and communicate remotely. The pandemic also had an impact on how to manage projects effectively and efficiently. The pandemic caused business owners and project managers to question how they can sell their services/goods through an online channel, how they can offer products/services that are in demand with their existing infrastructure, and how to increase the rate at which products/services are produced or distributed (Savio, 2021:9). Savio (2021) states that the pandemic brought forth the following changes in project management: expansion of AI and Automation (Savio, 2021:9), more frequent implementation of Agile approaches in projects (Savio, 2021:9), increased remoteness of jobs (Savio, 2021:10), Emotional Intelligence (EQ) by means of project managers being able to manage their employees and raise their morale/productivity (Savio, 2021:11). Finally, IoT and smart systems also started to play a larger role in managing projects (Savio, 2021:11). Project managers can employ automation to accomplish difficult activities such as scheduling and data visualization, and then make informed decisions based on the gathered insights (Savio, 2021:9). By employing Agile methodologies, organisations can manage changing priorities more effectively, enjoy better project visibility and produce faster delivery times for projects (Savio, 2021:10). Agile methodologies also make use of an iterative process, which is much more adaptable to modern projects, ensuring that there is constant communication with stakeholders and iterative improvements occurs within a project (Savio, 2021:9). Virtual meetings and video conferences are becoming ever more present as jobs tend to get more remotely, file sharing and synchronization becomes more essential to share resources remotely, project status tracking and reporting also becomes more important to help project members complete task and activities on time from any location in the world (Savio, 2021:10). Cybersecurity attacks and risks are also increasing as more jobs become remotely, which means businesses should train their employees and make them aware of possible cyber-attacks and threats (Savio, 2021:10). Implementing IoT systems to assist project managers in planning/scheduling/managing projects have some perks (Savio, 2021:11). It helps with real-time productivity, the optimization and automation of business processes as well as real-time monitoring and tracking of projects (Savio, 2021:11).The comprehension of when to implement a specific project management methodology is an extremely valuable skill in the project management field. Notwithstanding the fact that there is currently no ideal solution that aids in project management and success, company executives and project managers are shifting to agile project management as a major solution to help them overcome this strain (Salameh, 2014:71). The growing necessity to provide high-quality products in a dynamic and rapidly changing global market compelled professionals to develop APM methodologies. (Salameh, 2014:71). Traditional project approaches have been noted by scholars for their long term use as a source of formality in project management and their success in guiding and managing projects (Salameh, 2014:71). Due to the fact that project requirements can be intangible and volatile, traditional project management approaches are typically not well suited for complex projects, such as  IT and software projects (Salameh, 2014:71). APM has arisen as a very iterative and incremental approach in which Since the project team and stakeholders work together to comprehend the domain, define project requirements, and prioritize feature requirements (Salameh, 2014:71). Agile methodologies are more suitable for modern projects, due to their ability to be adaptable to smaller scopes and they provide rapid delivery times (Salameh, 2014:71). APM yields increased productivity and quality, mainly due to iterative communication with stakeholders, which eliminates scope creeping and requirements misunderstandings (Salameh, 2014:71). It is worth noting that projects using Agile approaches were shown to be five times more effective than those using traditional methodologies. (Salameh, 2014:71). Projects that implemented Agile methodologies also had eleven times greater ROIs (Salameh, 2014:71). APM has proven to be a valuable approach for managing high-risk, time-sensitive research-and-development projects due to its lightweight processes that lead to effective decision making and productivity (Salameh, 2014:71). Regular client interaction and early concept testing leads to speedy and outputs that are market-sensitive (Salameh, 2014:71). These outcomes, in order, promote customer satisfaction, which improves the customer’s trust in the business, companies’ client retention remains at a higher level, customer loyalty is ensured, which leads to several satisfactory results for businesses such as increased sales, revenues, and overall profitability (Salameh, 2014:71). Regarding governance frameworks: COBIT 2019 is an enterprise-wide framework for the governance and management of information and technology (Audit & Association, 2018:9).  Enterprise I&T refers to all of the technology and information processing that an enterprise uses to achieve its goals, regardless of where it occurs inside the enterprise (Audit & Association, 2018:9). Thus, corporate I&T is does not only include an organization's IT department (Audit & Association, 2018:9). The COBIT (Control Objectives for Information Technologies) framework ensures that mutually agreed-upon corporate goals is established via the assessment of stakeholder requirements, prioritization and decision making determine the course of action managers take, and that performance and compliance are assessed against agreed-upon goals/objectives (Audit & Association, 2018:9). COBIT guarantees that an organization has a governance system in place via explicitly defined components, design factors are considered to build the most efficient governance system and incorporates critical governance mechanisms into governance/management goals that can be managed to the required competence levels (Audit & Association, 2018:9). According to Magalhaes (2019), some of the benefits provided by the COBIT 2019 framework includes improved orientations with global standards, frameworks as well as best practices. The framework provides frequent updates and improvements, continuous feedback/advancements, and provides flexible guidance on how to efficiently/effectively govern IT projects (Magalhaes, 2019). The framework provides clarity on the features that enterprises require for a solid governance structure (Magalhaes, 2019). COBIT 2019 also integrates well with other governance standards/frameworks such as ITIL, ISO/IEC 2000, and CMMI (Magalhaes, 2019). The maturity of other standards and practices are guaranteed by implementing a governance framework like COBIT 2019 (Magalhaes, 2019). A key benefit is that the framework complies to government standards (Magalhaes, 2019). The framework aids in the alignment current frameworks with overarching strategy and performance monitoring (Magalhaes, 2019). Finally, in order to fulfil objectives, it aligns IT with enterprise goals, which produce value to businesses, and mitigate business risks (Magalhaes, 2019). By analysing the content of this section, one can deduce that by choosing the optimal governance framework, body of knowledge as well as project management methodology, one’s organisation can gain a considerable competitive advantage and lower business risk rates/possibilities.Chapter 3: SYSTEM MODELSystem Model Formulation and DesignThe system model will be based on hypotheses derived from the Introductory sections. The system model has three main components: governance framework, body of knowledge and project management methodology. In order to present Company XYZ with a solution as to which project management methodology to choose to manage and plan their projects, a good overview will be needed of a governance framework, which implements a body of knowledge, and each type of project will need a respective project management methodology, which is guided by standards and principles from the body of knowledge and governance framework. Thus, all hypotheses lie on three pillars: governance framework, body of knowledge and project management methodologies. There are three hypotheses regarding the research question: COBIT 2019 will be the best suited IT governance framework to implement, since it is a standard governance framework and used by a variety of companies, it also provides structure and a lot of benefits to manage projects. PMBOK will be the most suited body of knowledge to provide standards for managing and planning projects. Finally, based on the Introductory content, an Agile methodology (APM) will be the most suited approach to implement project management on a project. The figure below describes the system model: Governance frameworks will be discussed, from which the body of knowledge will flow that is implemented in the governance framework (provides project management standards), from the body of knowledge flows the project management methodology (traditional and agile methodologies will be discussed). At the end of the day, the system model should help to answer the research question.Figure 1: System Model ProposalChapter 4: PROBLEM STATEMENTA Quick OverviewThe Research Question can be defined as the following: what project management methodology, standard and governance framework will be the most applicable to manage and plan modern projects within a business-IT context? The research in this report should help project managers to manage changes in projects as well as providing guidelines to successfully reduce risks within a project and provide a competitive advantage to an organisation. Sustainability and maturity levels of a project should also be a key focus point. The selected governance framework, standard and project management methodology should contribute to mitigating project failures and possible provide a kind of “guideline” to standardize project success. Stakeholder requirements and organisational structure should be considered while trying to provide a solution to the research problem. Internal and external factors that impacts an organisation’s capability to efficiently and effectively manage projects cannot be overlooked and contributes to the formulation of the solution. The research problem also involves the three major project constraints: schedule, time and budget. So how can project managers successfully implement project management by looking at standards and methodologies? How can project managers mitigate risks and maximize business opportunities by implementing a selected project management strategy?Chapter 5: SOLUTIONStructure and FunctionalitiesThe hypotheses based on the research question is that modern projects should be governed by implementing the COBIT 2019 governance framework. PMBOK should be used as the body of knowledge that provides standards for guiding and managing projects (and is supported by the COBIT 2019 framework, but not limited to). The most applicable project management methodology will be APM, as it is iterative and adaptable to modern projects that frequently undergo scope changes with faster delivery times. The solution to the research question and attempt to prove the hypotheses will be formulated via analysing three sections:  Governance Frameworks, Bodies of Knowledge (Standards) as well as Project Management Methodologies. The three sections should be studied thoroughly, their principles and guidelines will be discussed. Comparisons will be provided between each governance framework, standard and project management methodology. The benefits and drawbacks of each will be discussed and an analysis will be provided after a thorough discussion on each section. While formulating the solution, stakeholder requirements, organisational structures, external/internal risks and opportunities must be considered. The solution should be able to maximize business opportunities while minimizing business and project risks. The proposed solution must be implemented by an organisation’s management to such a degree that it can support a business’s organisational and governing structure while also providing clear guidelines and principles on how to manage projects. When choosing a type of approach to implement, the type of project should also be taken into consideration. To encapsulate the solution: a governance framework should be provided that ensures a company complies to government/authority regulations as well as defining clear objectives and approaches to risk management. A body of knowledge (standard) should be implemented that provides precise guidelines/rules on how to manage and complete projects (best practise). The applicable project management methodology should be chosen as such to support the management of a specific type of project, and should guide project managers into executing, planning and managing projects. The methodology can provide tools and techniques to complete projects.Chapter 6: GOVERNANCE FRAMEWORKSThe generation of business value from IT-enabled business investments is done via business and IT professionals performing their duties with the guidance from a governance framework (Ghildyal & Chang, 2017:256). Business/IT alignment should be followed when implementing a governance framework (Ghildyal & Chang, 2017:256). The four key domains of strategic choice is business strategy, information technology strategy, organizational infrastructure and processes, and information technology infrastructure and processes, each with its own set of fundamental characteristics (Ghildyal & Chang, 2017:256). These key domains provide the interdependence of external and internal components, as well as the integration of business and function domains (Ghildyal & Chang, 2017:256). DiscussionCOBIT 2019Structure/PrinciplesThe aim of COBIT 2019 is to monitor all initiated projects inside the organization in accordance with corporate strategy and in a synchronized approach using a standard project management approach (Audit & Association, 2018:221). Projects should be initiated, planned, controlled, and executed before concluding with a post-implementation evaluation (Audit & Association, 2018:221). Key objectives are to promote communication with and the participation of business and clients in order to achieve project outcomes while avoiding the risk of unforeseen delays, costs, and value depreciation (Audit & Association, 2018:221). The framework should guarantee the quality and value of project outcomes while also maximizing their contribution to the programs and investment portfolios outlined (Audit & Association, 2018:221). The framework should contribute towards enterprise and alignment goals (Audit & Association, 2018:221). The optimization of business processes and management of digital transformation programs is common enterprise goals (Audit & Association, 2018:221). Benefits that become actual from IT-established benefits and services portfolios, as well as on-time delivery of programs, according to budget and according to specified requirements and quality standards is common alignment goals (Audit & Association, 2018:221). Key management practices within the COBIT 2019 framework includes, in the following order: Maintaining a standard approach towards project management, starting/initiating a project, managing stakeholder engagements, developing/maintaining a project plan, managing project quality and project risk, monitoring and controlling projects, managing project resources and the closure of a project and/or activities (Audit & Association, 2018:226). Related guidance is based on the PMBOK standard (Audit & Association, 2018). To reap the benefits of the COBIT architecture, a deeper understanding of the enterprise is necessary (De Haes et al., 2020:36).BenefitsOnce priorities in governance and management objectives have been established, the organization can conduct a quick assessment of its current maturity in attaining governance and management goals (De Haes et al., 2020:37). COBIT 2019 provides structures for IT alignment, the framework goes beyond the immediate demands of users and consumers to ensure that IT is coordinated with bigger business ambitions (Simmonds). To accomplish this while also managing the complexities of day-to-day operations, a complete and well-defined top-level perspective is required (Simmonds). COBIT provides this as well as a common lexicon to ensure that IT experts, teams, departments, and stakeholders are all on the same page (Simmonds). COBIT ensures that enterprises remain complaint to regulations: modern information technology must remain compliant with data and business regulations, such as the GDPR (Simmonds). When developing and revising frameworks, COBIT considers such regulations to be critical details, ensuring that they are addressed at all levels of IT (Simmonds). Customers are already confident in COBIT 2019 for risk management, based on the success of COBIT 5 in this capacity (Simmonds). The framework also assists in optimizations with business processes and management activities (Simmonds). COBIT 2019 is also ideal for day-to-day IT, assisting practitioners in identifying priorities and offering the tools and best practices needed to improve where appropriate (Simmonds). Consequently, practitioners can benefit from more efficient, targeted, and profitable IT operations, with distinctly defined roles and responsibilities across teams and departments (Simmonds). Considering the rise of COBIT’s popularity, implementing it can assist in enhancing stakeholders,’ end-users’, and clients' trust (Simmonds). The stability that COBIT offers in IT can also help a company's reputation with customers. COBIT 2019 is innovative, providing information on the most recent IT tools, best practices, and so forth (Simmonds). Nevertheless, because of its emphasis on framework reassessments and open-source format, it also prepares practitioners for future changes (Simmonds). As a result, businesses can be fully prepared to evolve as needed (Simmonds).DrawbacksCOBIT 2019 possesses a complicated structure and processes (Zhang et al., 2013:393). The essence of COBIT simply cannot be captured in a few sentences or a paragraph (Zhang et al., 2013:393). The framework consists of lengthy principles and a lot of tools, which can take an organization a while to adapt to and learn (Zhang et al., 2013:393). For example, there are 34 IT processes with 222 control objectives and more than 300 controls, only for the Control Objectives alone (Zhang et al., 2013:393). According to (Zhang et al., 2013:393), the generic character COBIT is challenging for corporations to understand and apply. Although the COBIT Management Principles and Implementation Instructions emphasize that COBIT must be customized to a specific environment, they do not provide particular methods or guidelines to help organizations do so. (Zhang et al., 2013:393). There are only a few case studies available from its publishers, ITGI and ISACA, and they are brief (Zhang et al., 2013:393). In comparison to IT standards that are more mature, such as ITIL and ISO27000, the usefulness of COBIT is difficult to discern (Zhang et al., 2013:393). There are no established statistics or research to back up its claimed benefits (Zhang et al., 2013:393). Many executives felt that, while a COBIT program was clear, they would rather place emphasis on and implement ISO27000 and ITL, which provided higher significance values (Zhang et al., 2013:393). The control objectives are given in a less-structured manner, which is one clear source of complexity (Zhang et al., 2013:393). Despite being divided into four major areas (Strategy, Finances, Stakeholders, Processes and Learning), many of them overlap in content or have structural relationships (Zhang et al., 2013:393). The IT Governance Institute (ITGI)  and the Information Systems Audit and Control Association (ISACA) has stated that the main hindrance of COBIT is that it takes a considerable level of expertise to grasp the framework before it can be implemented as an efficient/effective IT governance framework (Zhang et al., 2013:391). Many executives agreed that while they believe COBIT is a useful framework, they prefer to focus on ITIL and ISO27000 (Zhang et al., 2013:391).ITILStructure/PrinciplesITIL (Information Technology Infrastructure Library) is an eight-book series that offers consistent and comprehensive best practices for IT service management and delivery (Zhang et al., 2013:392). ITIL lays the groundwork for quality IT service management (Zhang et al., 2013:392). It provides comprehensive best practices for planning, designing, and implementing effective service management capabilities, as well as detailed methodologies, functions, roles, and processes for businesses to base their own practices on (Zhang et al., 2013:392). ITIL's third version tries to transition from a process-based framework to a more comprehensive structure that reflects the entire life cycle of IT services (Zhang et al., 2013:392). ITIL outlines processes, procedures, tasks, and checklists that are not organization or technology specific, but can be used by an organization to strategize, provide value delivery, and helps enterprises maintaining a minimal level of competency (Nakakawa et al., 2019). It enables the organization to set a baseline from which to plan, implement, and measure its efforts (Nakakawa et al., 2019). It is used to demonstrate compliance and to track progress (Nakakawa et al., 2019). ITIL certification is offered to individuals only (Nakakawa et al., 2019). BenefitsOrganisations all throughout the world understand the value of the ITIL framework (Cook et al., 2021:189). The most typical reasons organizations use ITIL are to increase the efficiency of operations and the improvement of quality services as well as customer happiness (Cook et al., 2021:189). Additionally, ITIL methodologies are linked to lower IT expenditures (Cook et al., 2021:189). Furthermore, improved Information Technology operational management, including adherence to business rules and specifications, has a direct and favourable influence on certification and compliance with both domestic and international regulations (Cook et al., 2021:189). Another key benefit of adopting and executing ITIL: it can assist project managers in aligning IT operations with business objectives (Cook et al., 2021:189). Research was conducted: a Strategic Alignment literature review in, concentrating on ITIL and different Strategic Alignment Models (SAM) (Cook et al., 2021:189). The holistic impact on businesses was studied (Cook et al., 2021:189). It was discovered that ITIL implementation boosted alignment by improving communication between business and IT professionals, removing impediments and enabling knowledge sharing, and strengthening ITIL's aptitude to support business initiatives (Cook et al., 2021:189). A statistically significant association between mature ITIL adoption and elevated levels of IT/business alignment was also identified. (Cook et al., 2021:189). It was also observed among the “lower-level” class within organisations that ITIL increased service quality (Cook et al., 2021:189). ITIL improved workday stability, product quality, and service predictability for the working class (Cook et al., 2021:189). In addition, ITIL implementation led to more balanced workplaces, causing employees to be more productive and lead to higher levels of morale in the workplace (Cook et al., 2021:189). To provide an example to illustrate the benefits of implementing ITIL: Procter & Gamble saved $125 000 000 in IT costs, within less than 72 hours Shell Oil could upgrade their software, in theory saving 6000 days of work and $5 000 000, and Caterpillar enhanced their responsive times for reporting Web incidents from thirty to ninety percent of the time (Cook et al., 2021:189).DrawbacksEmployees that are opposed to change, oppose constraints imposed on their processes and procedures, disagree with executive objectives, prefer ad hoc creative solutions over proven approaches, and are anxious about the complexity of the new processes will face resistance to ITIL adoption in the majority of organizations (Cook et al., 2021:190). Likewise, firms that have not historically used tight criteria will see a cultural shift (Cook et al., 2021:190). Employees who are used to manage their own schedules may be irritated by the loss of independence (Cook et al., 2021:190). Such opposition could lead to the project's downfall (Cook et al., 2021:190). ITIL encourages the necessity for service quality monitoring and reporting, and it makes IT personnel feel like they are constantly being monitored (Cook et al., 2021:190). Furthermore, adoption of ITIL represents a significant cultural shift; personnel associated with ITIL might fear that their roles will become superseded, and this reluctance may stymie the implementation process (Cook et al., 2021:190). A further study revealed common aspects of change resistance when implementing ITIL (Cook et al., 2021:190). According to the survey, the presentation of modern technology and solutions conflicted with previously established procedures and habits (Cook et al., 2021:190). The problem of resistance to change is exemplified by a common unfavourable attitude among practitioners over the inconvenience of tracking cases during periods of severe activities. (Cook et al., 2021:190). These procedures, which were expected by managers who wanted concerns resolved quickly, resulted in a complex and bureaucratic processes that was equally demanding and time consuming to accomplish. (Cook et al., 2021:190). With these facts in mind, it is possible to identify resistance to change as a prevalent problem in the implementation and acceptance of ITIL (Cook et al., 2021:190). Another problem that users confront while adopting and implementing ITIL is successfully engaging them in accepting the revised procedures (Cook et al., 2021:190). Even though ITIL improves the distribution of IT services to clients, allows IT processes to business objectives migrations, and promotes the effective management of incidents, it does not provide a clear way to engage folks who are involved in activities outside of IT. (Cook et al., 2021:190). If it happens that not all system users cooperate with an accepted system, service quality suffers, and incident handling becomes delayed (Cook et al., 2021:190). Furthermore, when people are presented with new processes for dealing with IT-related incidents, their execution can be irregular and inconsistent. (Cook et al., 2021:190). Customers were discovered to frequently circumvent the new procedures by calling acquaintances in the IT department, since those acquaintances had a better chance at solving the problem as soon as possible (Cook et al., 2021:190). Encouraging stakeholders to engage and become motivated about the implementation of ITIL proved to be troublesome for project managers (Cook et al., 2021:190). The project will fail if management is unable to gain the needed support and collaboration from the stakeholders (Cook et al., 2021:190). Technical issues that can (inevitably) occur during the implementation of ITIL must be addressed by organisations (Cook et al., 2021:190). These issues include the integration of new service delivery applications, incorporating new into existing procedures, the development of measurement tools that provides great accuracy and the training of employees to reduce errors within a system caused by users (Cook et al., 2021:190).CMM/CMMIStructure/PrinciplesCMMI (Capability Maturity Model Integration) is an improved version of CMM that includes elements for analysing and effectively managing a business (Khoja et al., 2010:21). Companies were having difficulties implementing CMM at various levels of their company, and the models happen to lack integration (Khoja et al., 2010:21). CMMI overcomes this shortcoming by incorporating the complete organizational processes, allowing them to be controlled more effectively (Khoja et al., 2010:21). CMMI, like CMM, has five maturity stages (namely Initial, Managed, Defined, Quantitatively Managed, and Optimized) which the model uses to analyse and evaluate the maturity of an organization (Khoja et al., 2010:21). Each CMMI is composed of numerous process areas (such as project management, process management, engineering and support), assessment of these process areas ensures that enterprises are meeting their business objectives (Khoja et al., 2010:21). This allows them to study, assess, and enhance further (Khoja et al., 2010:21). It is believed that CMMI favours large enterprises since it necessitates extensive documentation and procedures (Khoja et al., 2010:21).BenefitsA variety of companies reported gaining benefits from implementing CMMI within their company. Accenture reported that they have built management skills by focusing early on Measurement and Analysis and "thinking level 4 and 5" (Goldenson & Gibson, 2003:11). They, like many others, placed emphasis on treating process improvement activities as projects, the necessity of obtaining management sponsorship and providing support to initiatives during transitions (Goldenson & Gibson, 2003:11). According to Accenture, hours invested in quality activities might have resulted in a 5:1 return on investments (Goldenson & Gibson, 2003:11). As an example of relevant CMMI results, consider Boeing's submarine project (Goldenson & Gibson, 2003:12). Over an 18-month period, the average cost to fix a fault was reduced by 33% (Goldenson & Gibson, 2003:12). With an improvement in the flexibility to customise builds, the time required to deliver a release was cut in half (Goldenson & Gibson, 2003:12).  There have been claims that a 60% reduction in pre-test and post-test audit preparation, conduct, and rework occurred, which resulted in audits passing with little to no significant activities (Goldenson & Gibson, 2003:12). There have been observed that developers at Boeing Australia have placed a heavy emphasis on removing faults from all their products, enhancing quality, and finding methods to improve their processes (Goldenson & Gibson, 2003:12). Lockheed Martin Management & Data Systems deployed a variety of technical approaches connected to design, tools, and process initiatives during the early 2000’s, during which time they attained maturity level 5 against both the SW-CMM and CMMI models (Goldenson & Gibson, 2003:14). They also worked to improve the quality of their processes for detecting, preventing, and eliminating defects. During these three years, notable performance improvements include a 30 percent software productivity increase, a 20 percent unit software cost decrease, and a 15 percent costs reduction to discover and resolve faults (Goldenson & Gibson, 2003:14). Thales Air Traffic Management (ATM) has advanced to CMMI maturity level 4 from SW-CMM maturity level 3 (Goldenson & Gibson, 2003:17). Training and an organizational metrics dictionaries helped to support their measurement procedures 3 (Goldenson & Gibson, 2003:17). Within their process diagrams, they have found decisions that necessitate Decision Analysis and Resolution approaches 3 (Goldenson & Gibson, 2003:17). Risks were tracked, monitored, and kept in a database, and proper steps were taken 3 (Goldenson & Gibson, 2003:18). A rising variety of processes were started to being statistically controlled, such as requirement production and confirmation, design, and coding processes 3 (Goldenson & Gibson, 2003:18). They also direct their process improvement efforts based on measurement outcomes. Program, company, and team objectives are defined and measured based on process performance 3 (Goldenson & Gibson, 2003:18). Business needs and customer negotiation determined the objectives, which were achieved through improvement efforts 3 (Goldenson & Gibson, 2003:18). Thales ATM highlighted early defects discoveries, enhanced risk management, and better project control is one of the key benefits of Thales' ATM process improvement endeavour 3 (Goldenson & Gibson, 2003:18).DrawbacksSome research conducted looked into why organizations prefer to not use CMMI. The study looked at the population of organizations that have chosen not to embrace CMMI and their reasons for doing so (Staples et al., 2007:893). The sample population consisted of 40 software-development organizations that did not use the process capability maturity model SPI or wanted a CMMI Class A appraisal and decided not to purchase CMMI Level 2 Class B or C appraisal and process improvement services within a two-month recording period and provided at least one substantive reason for their decision (Staples et al., 2007:893). The most common reasons found why organisations didn’t implement the CMMI model were the following: their organisation was too small to efficiently and effectively implement the model, it was very costly to implement the model, time constraints were too rigid to implement the model, other forms of SPIs were already used within a company (Staples et al., 2007:893). As one can deduct from the benefits of CMMI, this model is obviously more beneficial towards larger companies to determine their maturity levels or companies that value quality control more than structure and architecture (Staples et al., 2007:893). The number of documentation and processes that must be followed to implement CMMI can be a tedious job for smaller companies that simply do not have the required infrastructure to support CMMI processes. Within smaller companies, CMMI was often found to be infeasible, or employees displayed resistance towards the implementation of CMMI within their company (Staples et al., 2007:893). Other problems that were identified with the CMMI model are that there were no coverage of safety engineering aspects, there also happened to be no extensive documentation as how to implement processes and the complex structure of the model (Ghildyal & Chang, 2017:258).chapter 7: bodies of knowledge (standards)DiscussionPMBOKStructure/PrinciplesIn an attempt to standardize widely accepted project management knowledge and procedures, the Project Management Institute (PMI) published the PMBOK Guide for the first time in 1987 (Ilieş et al., 2010:48). In 2008, the PMBOK Guide was redrafted for the fourth time, and it is now one of the most widely used project management standards (Ilieş et al., 2010:48). The Project Management Body of Knowledge (PMBOK) is a collection of project management methodologies and knowledge areas typically recognized as best practices (Ilieş et al., 2010:48). The PMBOK Guide is a universally known guideline that describes the fundamentals of project management as they apply to various projects (Ilieş et al., 2010:48). Specialized standards were developed as an addition to PMBOK to meet the needs of specific sectors. (Ilieş et al., 2010:48). The COBIT 2019 framework also provides integration with PMBOK. In the first section of this guide, it divides 47 processes into 10 Knowledge Areas (KA) concerning the professional field, project management field, or area of specialization, such as project scope and cost management (Xue et al., 2015:16). The ten knowledge areas are Integration, Scope, Time, Cost, Quality, Human Resource, Communication, Risk and Procurement (Ghosh et al., 2012:2). This guide reorganizes the processes in the second section to produce a project management standard (Xue et al., 2015:16). It categorizes processes into five groups based on the five stages of project implementation: initiating, planning, executing, monitoring and controlling, and closing process groups (Xue et al., 2015:16). It describes the input, methodologies and tools, output, and data flow diagram of each of the ten KA processes (Xue et al., 2015:16). All or some of the five process groups can be used at each step of systems engineering (SE), reusing any set of the processes within any stage of systems engineering is acceptable (Xue et al., 2015:16).BenefitsPMBOK is suitable for large-scale projects and large enterprises due to its structure and principles (Xue et al., 2015:20). PMBOK is a very detailed standard and provides a diversity of techniques and tools project managers can use to manage and control projects, as well as to assessing risks and evaluating company/project benefits (Xue et al., 2015:21). It also integrates well with the COBIT 2019 framework, which is an upcoming popular framework used by a variety of companies to govern and control projects. PMBOK possess a strong communications management mechanism (Karaman & Kurt, 2015:578). PMBOK is preferred for Information Technology projects that require significant client dedication, massive and complicated project teams, outsourcing at higher rates, extensive contracts, and a prominent level of shareholder interaction (Karaman & Kurt, 2015:578).DrawbacksVery small entities will not easily benefit from implementing PMBOK, as it is very costly and takes a huge amount of time to implement (Xue et al., 2015:20). The PMBOK technique is the most effective for teaching each knowledge area's topic content, but it is less effective for guiding the execution of a single project. (Abdullah et al., 2021:47).P2MStructure/PrinciplesAnother resource for project managers is a guidebook on Project and Program Management for Enterprise Innovation (P2M) (Abdullah et al., 2021:42). The Engineering Advancement Association of Japan created and first published it in 2003 (ENAA) (Abdullah et al., 2021:42). Goals of the Project Management Standard is made up of the eleven elements as follows: Project/Program Management, Integration and Stakeholder Management, Scope and Resource Management, Time and Cost Management, Risk and Quality management, Procurement and Communications Management (Abdullah et al., 2021:42). A business project and program management guideline derived from Japanese commercial practice, it manages projects and programs from the perspective of the enterprise rather than a traditional project (Peng et al., 2007:110). P2M, unlike its European counterparts, changes existing project management paradigms (Peng et al., 2007:110). P2M creates a new commercial model based on corporate, system, and application innovation (Peng et al., 2007:110). P2M places emphasis on applying external change to project teams (Peng et al., 2007:110). P2M also outlines the activities on project supplies, knowledge data acquisition, and broadens the area of previous project management (Peng et al., 2007:110). P2M is more focused on portfolios and programs, in contrast to PMBOK which focuses on single projects (Ghosh et al., 2012:19).BenefitsP2M/KPM project and program management is based on a mission-driven approach and is suggested to support project management development through creating value in a complex and dynamic environment (Siang & Yih, 2012:26). It is a complete project management technique that is responsive to changing environmental conditions (Siang & Yih, 2012:26). It has been demonstrated that economic crisis survival requires flexibility, adaptability, and reformation (Siang & Yih, 2012:26). P2M/KPM techniques and methodologies have proven its value via offering learning opportunities in organizations, increasing involvement, and driving cooperation and comprehension among key leaders (Siang & Yih, 2012:26). Siang and Yih (2012:26) also state that throughout the recession, the profitable businesses were those who used P2M/KPM tactics. The flexibility to adapt to environmental changes is strongly emphasised by Japanese organizations, and their models are built around this concept (Siang & Yih, 2012:29). P2M was the first big step toward actual business integration and acceptance of project and program management, and one of the first to provide patterns for allowing integration with managing programs and portfolios at enterprise level (Siang & Yih, 2012:29). P2M is already extensively utilized as a standard guide, and through its respect for other standards and unique approach to project and program management for value creation in organizations, it provides a stable framework for continued development and advancement of project management. (Siang & Yih, 2012:30).DrawbacksJust like any other, there is no guarantee that by implementing the standard, a program will be successful (Low, 2015:33). Many factors influence whether such projects succeed or fail (Low, 2015:33). Employees, for example, are inclined to detest and resist radical changes unless their fears and anxieties are addressed, or radical changes are reasonable enough for them to accept, because environmental changes necessitate new approaches towards adaptation, such as reorganizing or reassigning employees to other positions in the workplace (Low, 2015:33). As a result, efforts are required to alleviate such issues (Low, 2015:33). When Japan's economy soured in the 1990s, modifications were made to some areas of traditional Japanese management in order to survive the recession (Low, 2015:34). Reformation was implemented, which also included layoffs, production and inventory cutbacks, and factory closures (Low, 2015:34). The conventional pay model based on seniority and lifetime employment implemented in Japan has been updated and decreased, almost ceasing to exist in certain extreme circumstances, and the Toyota group, for instance, has expanded its use of contract workers and temporary staff (Low, 2015:34). Other activities to combat the crisis included drastic adjustments or new reforms, such as joint venture arrangements with foreign corporations (Low, 2015:34). APMBOKStructure/PrinciplesThe Association for Project Management Body of Knowledge (APMBOK) from The Association for Project Management (APM), a project management professional association situated in the United Kingdom, is another prominent PM Guide (Abdullah et al., 2021:43). APMBOK is continually improved to reflect developments in the project management area, for example evolving practice patterns, changing terminology, studies, and publications (Abdullah et al., 2021:43). The Association for Project Management (APM) from the UK originally distributed the standard in 1992 (Abdullah et al., 2021:43). Since 2019, the latest version of the Guide (7th Edition) has been accessible (Abdullah et al., 2021:43). APMBOK recognises 68 knowledge domains, and each one of them is covered in a subject (Abdullah et al., 2021:43). APMBOK focuses on four sections, namely Organisational Perspective, Change Organisation, People and Behaviours, Delivering Projects (Abdullah et al., 2021:43). APMBOK defines thirty technical capabilities, nine behavioural competencies, and eight contextual competencies (Ghosh et al., 2012:23). These competencies showcase a wide range of skills for a project manager, ranging from technical/application skills to social skills (interpersonal) with personnel and stakeholders (Ghosh et al., 2012:23). APMBOK includes ideas and knowledge that may only be applicable to certain projects, which leads to a wider approach to the expertise on project management (Ghosh et al., 2012:23). For instance, APMBOK addresses project safety, whereas PMBOK does not (Ghosh et al., 2012:23). APMBOK, conversely, covers a wider range of topics with less details, considering that detailed descriptions and approaches can be available elsewhere (Ghosh et al., 2012:23). BenefitsAs a result of its ever-increasing and/or diminishing character, the APMBOK is more in line with modern practices than the PRINCE2 project management methodology. (Famuwagun, 2020:3). It is more thorough than other standards such as the PMBOK (American Standard) and the IPMA Competency Baseline (Famuwagun, 2020:3). The United States, United Kingdom, Brazil and Australia employed APMBOK to as a standard to guide projects, especially with regards to managing the realisation of benefits alongside other methodologies and standards (Famuwagun, 2020:3). According to a study, APMBOK was used to identify and practise changes (67.70 – 77.00%), it was also used to manage changes (70.10 – 75.80%), it was used to conclude and embed changes to ensure the realisation of benefits (57.70 – 76.40%) and finally allowed the alignment of change strategies with project goals (57.7 – 76.40%) (Famuwagun, 2020:3). These practices was implemented by 371 project managers working on projects in Brazil, the UK and USA (Famuwagun, 2020:3). In general, APMBOK is descriptive (Famuwagun, 2020:3). APMBOK is well-suited for managing risks effectively and efficiently (Famuwagun, 2020:8). APMBOK as a standard is competent in answering questions on how to implement processes and procedures regarding project management (Famuwagun, 2020:8). APMBOK have been noted to provide project managers with miscellaneous knowledge about projects, assisting project managers in developing the necessary skills to manage projects, ensuring that projects meet the requirements of stakeholders and finally assisting the Project Team and Steering Group in maintaining a beneficial and positive relationship with respective stakeholders and authorities at all times (Famuwagun, 2020:7).DrawbacksDespite APMBOK’s wide coverage on a range of topics regarding project management, it lacks an in-depth description of the content (Famuwagun, 2020:3). It provides no given, methodical, and complete method for managing projects and, consequently, cannot be called a legitimate standard for project management. (Famuwagun, 2020:7). It has also been observed that APMBOK used to be unable to keep up with changing knowledge and abilities between editions in real time (e.g., the 6th edition was introduced in 2012, whilst the 7th edition was only introduced in 2019) (Famuwagun, 2020:7). Yet, due to the Internet and transformations of the digital era last-mentioned is not a problem anymore (Famuwagun, 2020:7).chapter 8: project management methodologiesDiscussionAPMStructure/PrinciplesAgile Project Management (APM) is based on traditional project management concepts that are blended with flexible, lightweight, collaborative, adaptable to frequent change, yet highly disciplined processes (Salameh, 2014:55). The APM approach is built on short delivery iterations and continuous learning (Salameh, 2014:56). To kick off the project, the project team conducts simplified planning, the specification of requirements, and the design and implementation of solutions (Salameh, 2014:56). Following the kick-off, the team participates in succeeding waves of repetitions (iterations), which include more comprehensive planning, the analysis of requirements, designing processes, execution of processes, testing functionalities of projects/programs, and finally delivering the results to customers and stakeholders (Salameh, 2014:56). APM tends to be more principle-based than rule-based (Salameh, 2014:56). At each stage (iteration), needs are assessed and analysed, the APM technique allows projects to be immediately adjusted according to stakeholder requirements/new project objectives (Salameh, 2014:56). Furthermore, APM employs a feature-driven management strategy, focusing on defining the objectives and scope of a project by prioritizing a list of project features and requirements based on their value, for example, additional income or market share (Salameh, 2014:56). Consequently, a customer's engagement in the scope and analysis of the project's requirements is critical (Salameh, 2014:56). APM emphasizes collaborative development and management to provide results, customer feedback, and continual improvement and additions (Salameh, 2014:56). APM methods are extremely iterative and incremental, with project team members and stakeholders actively working together to understand the project domain, define what needs to be produced, and establish priority functionality (Salameh, 2014:56). APM crept into use around a decade ago and quickly rose to become the de facto norm for managing IT projects (Salameh, 2014:57).BenefitsAPM provides the advantage of more flexibility and collaboration, which facilitates its adoption across multiple industries, including the public sector (Salameh, 2014:57). In today's corporate world, consistently changing business demands, drivers, and requirements create a challenge for project managers to manage the scope of a project, performing budgeting and proper schedule management (Salameh, 2014:58). Furthermore, business processes are becoming more complicated and intertwined than ever, and projects deal with increasingly complicated organisational constructions involving multifaceted groups, comprised of alliances with strategic suppliers, outsourcing vendors, various types of customers, partnership, and competitors (Salameh, 2014:58). These problems highlight the importance of having a flexible and adaptable approach in order to deliver projects, products, and services faster and meet market completion and customer satisfaction requirements (Salameh, 2014:58). APM’s structure addresses these issues. APM improves management and interpersonal skills, as well as responsiveness, velocity, adaptability, reliability, and consistency (Salameh, 2014:59). These enhancements may result in various benefits for enterprises, including cost savings, shorter delivery times, and enhanced client and customer satisfaction and retention (Salameh, 2014:59). APM initiatives were five times more effective in terms of cost and quality than traditional project management programs; additionally, APM projects had an 11-fold higher return on investment (Salameh, 2014:71). Furthermore, due to its lightweight processes that lead to effective decision making and productivity, APM has proven to be a realistic technique to handle high-risk, time-sensitive research-and-development projects (Salameh, 2014:71). The regular client interaction and early concept testing leads in speedy and market-sensitive outputs (Salameh, 2014:71). These outcomes boost customer satisfaction, which in turn promotes customer trust, retention and commitment, and results into economic benefits such as increased sales, revenues, and overall profitability (Salameh, 2014:71).DrawbacksAPM's key drawbacks are related to paperwork, interpersonal skills, experience and commitment, communication and involvement, agile roles, and team locations (Nuotilla et al., 2016:82). With the exception of a few high-tech research firms, public agencies and governmental organizations have been sluggish to adopt agile principles (Nuotilla et al., 2016:66). Challenges for adopting APM within smaller organisations include employees having the self-discipline and mindset to adapt multiple roles (Nuotilla et al., 2016:67). The increased dependency on interpersonal skills and heavy focus on teamwork can affect some team members negatively (Nuotilla et al., 2016:67). There might also be some resistance from employees to adapt the APM approach if they are used to traditional project management methods (Nuotilla et al., 2016:67). Management participation throughout the project life cycle can put a strain on the management/project managers of the organisation (Nuotilla et al., 2016:67). Developers must comprehend numerous technologies as well as the company; this necessitates more senior and experienced developers (Nuotilla et al., 2016:67). When team members are replaced, knowledge transfer might be difficult due to a lack of documentation (Nuotilla et al., 2016:68). The lack of agile-developed recruitment policies makes the review and appointment process more difficult (Nuotilla et al., 2016:68). Teams that are distributed can become a huge challenge and requires proper communication tools (Nuotilla et al., 2016:68). Customers must be actively and consistently involved throughout the process, which may not always be achievable (Nuotilla et al., 2016:69). Finally, the adoption of APM can prove troublesome with legacy systems, as the integration can be complex (Nuotilla et al., 2016:69).PRINCE2Structure/PrinciplesPRojects IN Controlled Environments (PRINCE) was based on PROMPT, a project management method developed in 1975 by Simpact Systems Ltd. (Ghosh et al., 2012:11) The Central Computer and Telecommunications Agency (CCTA), later renamed the Office of Government Commerce (OGC), established PRINCE in 1989 (Ghosh et al., 2012:11). It has supplanted PROMPT since its beginnings (Ghosh et al., 2012:11). Since 1989, it has been revised seven times (Ghosh et al., 2012:11). PRINCE2 was first released in 1996 (Ghosh et al., 2012:11). A collaboration of 150 European organizations contributed to it (Ghosh et al., 2012:11). PRINCE2's most recent edition was launched in 2009 (Ghosh et al., 2012:11). The impetus for the development of PRINCE2 was the desire to establish a standard for IT projects in the United Kingdom (Ghosh et al., 2012:11). It was one of the first standards created with IT projects in mind (Ghosh et al., 2012:11). It is a structured but flexible process-based project management standard designed to increase project management effectiveness (Ghosh et al., 2012:11). PRINCE2's framework is made up of four elements: seven principles, seven themes, seven processes, and tailoring to the individual needs of the project (Ghosh et al., 2012:11). The seven processes are divided into 40 activities (Ghosh et al., 2012:11). PRINCE2 achieves control in three ways: separating the project into manageable, controllable stages, controlling milestones, and defining the project team's organizational structure. Product-based planning in PRINCE2 focuses on the project's product (Ghosh et al., 2012:11). It also emphasizes quality control and change management strategies (Ghosh et al., 2012:11). PRINCE2 is considered as an inadequate project management standard for small projects (Ghosh et al., 2012:17). The OGC claims that PRINCE2 can be simplified (Ghosh et al., 2012:17). This is achievable because PRINCE2 is adaptable and customizable (Ghosh et al., 2012:17). The PRINCE2 project management methodology can be applied to non-IT projects to manage and control product delivery, increase success rates, and achieve economic value (Ghosh et al., 2012:17). PRINCE2 is a process-based approach to project management that is widely used as a generic project management method that is neither a guide nor a standard (Abdullah et al., 2021:39). The seven themes of PRINCE2 are as follows: Business Case, Organisation, Quality, Plans, Risks, Change and Progress (Abdullah et al., 2021:40). PRINCE2 is based on the following principles: business justification is maintained, learn from one’s mistakes, clearly defined roles and responsibilities, stage-by-stage management, managing a project according to expectations, concentrating on products and adaption to the project environment  (Abdullah et al., 2021:40). As one can notice, PRINCE2 is more traditional by nature.BenefitsAdvantages of this project management methodology are ensuring projects are completed on time due to its dependable, organized, and structured approach to project management; and improving communication within and between the project management organization, the project initiator, the Project Board, suppliers, stakeholders, and product users (Famuwagun, 2020:6). It also aids in the demarcation of roles and responsibilities across a project, allowing the Project Board to manage by exception, ensuring that they are only involved in the project when necessary (e.g., when an event/activity exceeds agreed upon thresholds) (Famuwagun, 2020:6). Furthermore, it promotes the development of team members across a project because experience, knowledge, and skills are smoothly transferred and in real-time without any additional costs (Famuwagun, 2020:6). The Project Board and project stakeholders to concentrate solely on the end product and its delivery while simultaneously assisting them in saving time and reducing waste throughout the project lifecycle (Famuwagun, 2020:7).DrawbacksThe challenges that face the use of the PRINCE2 methodology, amongst others, are only partially covering issues related to organizational assets, health and safety, stakeholder management, and communications and integration management during projects, having a limited capacity to manage existing and emerging risks appropriately, and not covering procurement management and conflict management at all (Famuwagun, 2020:6). Furthermore, when not understood and executed effectively, it can be unduly bureaucratic, and while its main goal is to ensure a project fit with the business objectives of the Project Initiators, it does not outline how this is to be accomplished (Famuwagun, 2020:6). Based on the above disadvantages, PRINCE2 is in general not recommended for largescale IT projects (Famuwagun, 2020:6). According to a study, it is for this key reason that many UK government projects have failed over the years (for example, the failed NHS IT project, the scrapped eBorder project, and the failed Research Councils' Back Office Service and Process Unification project), and it has also been recorded that it is one of the reasons why 65 - 78 percent of megaprojects around the world deviated from their original business objectives at completion (Famuwagun, 2020:6). Furthermore, its stringent documentation requirements make rapid changes difficult to implement and resource re-allocation problematic (Famuwagun, 2020:6). SCRUMStructure/PrinciplesScrum is an agile software development process used by businesses in projects with increased end-user participation and requirements that continue to evolve until the project is completed (Khoja et al., 2010:20). Scrum operates in iterations, and each iteration is known as a sprint, which lasts 30 continuous days (Khoja et al., 2010:20). The team, the product owner, and the scrum master are the three roles in the process (Khoja et al., 2010:20). Scrum's beauty is that it believes in self-managing and self-organizing teams, and there are mechanisms in place to monitor and control the project even when the needs are ambiguous (Khoja et al., 2010:20). Few businesses keep a burn-down chart to track the time remaining between sprints (Khoja et al., 2010:20). The time to complete each sprint is fixed, however as the project becomes more complex, the functionality or quality may be reduced, but the sprint is expected to end on the agreed-upon date (Khoja et al., 2010:20). Scrum and other agile approaches are extremely compatible with quality standards such as CMMI (Khoja et al., 2010:23). Thus, it is not necessary for the company to change, but rather how the company fits into the model, enhances it, and makes the best of it (Khoja et al., 2010:23). The main activities of the SCRUM methodology include product backlog refinement, during which the team estimates the items and confirms the order of significance (Obrutsky, 2015). Sprint planning entails a timed meeting that lasts around two hours per week of sprint duration (Obrutsky, 2015). A high-quality product backlog is critical to the success of sprint planning (Obrutsky, 2015). The daily Scrum is a 15-minute meeting held every day in which each team member elaborates on what they have done since the last meeting, what they plan to do between now and the next meeting, and the blockers (Obrutsky, 2015). At the conclusion of the sprint, there is a demonstration of the product increment (Obrutsky, 2015). The product owner updates the product backlog once the team and stakeholders examine it (Obrutsky, 2015). Finally, the sprint retrospective occurs during which the team evaluates how the process was carried out, noting issues and opportunities for improvement (Obrutsky, 2015).BenefitsEven though Scrum was created and is intended for usage in the IT industry, many of its concepts may be adapted to other areas such as engineering, finance, health care, and others (Ghosh et al., 2012:33). Every project might benefit from having their high-value base features come first, followed by additional features (Ghosh et al., 2012:33). This enables for scope changes, feedback inclusion, risk minimization, and increased customer satisfaction (Ghosh et al., 2012:33). Many firms throughout the world have successfully embraced Scrum, and even more have successfully applied Scrum concepts to traditional project management models to shorten development time (Ghosh et al., 2012:33). Other advantages of implementing the Scrum project management methodology includes faster delivery times of projects, thus customers can receive results much quicker (Obrutsky, 2015). Projects become more adaptable and flexible to user changes and requirements (Obrutsky, 2015). User feedback can continue throughout the whole Scrum process, enhancing communication between the team and stakeholders as well as ensuring proper end-delivery functionalities of projects (Obrutsky, 2015).  The user requirements are collected after each sprint, which help team members to stay on track with the project schedule and functionalities (Obrutsky, 2015). The requirements are prioritized in a systematic manner (Rao et al., 2011:42). DrawbacksOne of the main disadvantages of implementing a Scrum project management methodology, is that the scope of a project can change constantly, due to the fact that users can keep adding more functionality to a project, as user requirements are collected after each sprint (Obrutsky, 2015). The Scrum methodology is more applicable to smaller companies, larger companies with planned architectures and structures will struggle to implement the methodology properly (Obrutsky, 2015). If a team member leaves or is replaced by a junior/new member, it can decrease the team’s productivity (Obrutsky, 2015). It is also harder to implement quality controls due to the fact that changes in a project is a defined constant (Obrutsky, 2015). Because the client is not always on site and close to the project team, close customer collaboration is not always possible (Rao et al., 2011:42). Documentation on the methodology is limited, an can thus sometimes become challenging to implement (Despa, 2014:52). Small teams necessitate the usage of skilled developers on a project (Despa, 2014:52). For large implementations, project estimations are limited at the start of the project, making cost estimates erroneous (Despa, 2014:52).chapter 9: analysisStatistics and ExaminationsFigure 2: Comparison of various methodologies from a project management perspective (Charvat, 2003:5)Figure 3: Agile Adoption Challenges (Anthony, 2022)Figure 4: Project Management Failures (Cohen, 2019)Figure 5: Project Management Approaches (Harrin, 2022)Based on the figures above, one can deduce what project management methodologies are important, why projects fail and why some companies struggle to adapt to the Agile methodology. It would appear that the traditional project management methodologies can manage projects better in terms of scope, time, quality and budget. Due to their traditional approach and emphasis on architecture and structure, traditional project management methodologies are more suited for large-scale projects and large companies. However, traditional project management methodologies lack the flexibility and adaptability of Agile methodologies. Agile methodologies are more suitable for smaller and medium-sized companies, where the emphasis on structure and architecture might be non-existent. Project phases are also more prevalent in traditional methodologies than agile methodologies. According to , the top three reasons why projects failed in 2019 are organisations’ decision to change their priorities, changes in project objectives and failing to understand the project functionality and requirements according to stakeholders. Even though Agile methodologies are more suited to manage modern projects, a lot of companies struggle to adapt to the methodologies to manage projects, due to employees’ resistance to change management, lack of the management’s involvement, varying practises and processes that are followed by teams and different departments, instead of having one clear standard and guideline to follow. Lack of training project managers to implement the methodologies efficiently and cultural misalignment also contributes as constraints on implementing Agile methodologies. Finally, it would appear that most project managers actually prefer a hybrid approach to manage projects, instead of respectively following a traditional or Agile approach. The more experienced project managers become, the more they tend to follow a predictive approach than an iterative approach.chapter 10: CONCLUSIONDeductionsBased on the content of this report, the following recommendations can be made to Company XYZ as to which project management methodologies are the most applicable to implement to manage modern projects, while also mitigating risks and maximizing business opportunities. The hypotheses of the System Model have in actuality been fully disproven by the research conducted. COBIT 2019 is not necessarily the best practise governance framework to implement within a company. COBIT 2019 needs a lot of expertise to be implemented efficiently/effectively within a company, even though it provides clear standards and guidelines on how to manage and govern project according to government regulations. Due to this fact and given that the COBIT 2019 documentation is quite extensive, it is not in general suited for smaller companies, which may lack the resources and personnel to implement this governance framework. In fact, even though COBIT 2019 popularity has risen the past few years, and is implemented by a variety of companies, it would appear that ITIL/ISO27000 is the governance frameworks that is still preferred as the de facto standard by project managers. COBIT 2019 is more suited for large companies whose management needs to comply to government regulations and specify business objectives and alignments alongside organisational structure and Business/IT alignments. CMM/CMMI is a good governance framework if the organisation’s emphasis is on maturity levels rather than organisation and structure/architecture. However, there have been found that smaller companies struggle to follow CMMI practices due to the framework being too costly to implement as well as rigid time constraints to implement the framework. The documentation on CMMI is indeed extensive and can take smaller companies a long time to finish the admin. PMBOK as a project management standard integrates well with the COBIT 2019 framework and provide clear guidelines/best practises for how to manage projects. It thus works well for larger companies that needs to follow government regulations and a specific architecture. However, PMBOK is costly to implement and does not work well with rigid time constraints. The P2M standard works well with project management in Asian-aligned cultures, as the standard places emphasis on culture and trust. In fact, a lot of Western companies also started to implement the standard due to the principles of P2M. P2M is a quite adaptable standard, which means it can easily be implemented in most companies to manage projects. However, P2M emphasises radical changes, and as such change management might be resisted by employees. APMBOK is also a suitable standard for managing modern projects. APMBOK is well versed in guiding project managers on how to execute and manage projects. The guidelines are quite extensive, and risk management and assessment are easy to implement within this standard. Some of the knowledge areas in APMBOK lack depth however, and as such it might sometimes be difficult to implement the standard within companies. Regarding project management methodologies, traditional project management methodologies, such as PRINCE2, is more suited for large-scale projects and companies due to providing structure/architecture. Agile methodologies, such as APM or Scrum is more suited towards smaller companies where time constraints are tight and project objectives should be rapidly delivered, and requirements are improved iteratively. Agile methodologies can practically guarantee improved quality and productivity. Knowledge transfer in Agile methodologies can be troublesome though, and the ability to adapt quickly to changes can be challenging for some employees and they may eventually resist change management. The main reason projects fail is due to project management teams failing to understand stakeholder requirements, processes and methodologies that are too expensive to implement/overbudgeting or mismanagement of funds on projects, and poor schedule planning/time management. Lack of communication between stakeholders/team members and lack of management’s involvement in projects have also contributed to project failures. To conclude: in reality there does not exist a best practise project management methodology and standards. It all depends on the type of organisation, projects and the specified stakeholder requirements, as well as the type of architecture and organisational structure a company has.Bibliography Abdullah, A.A., Abdul-Samad, Z., Abdul-Rahman, H. & Salleh, H. 2021. Project management standards, guides and methods: A critical overview. Journal of Project Management Practice, 1(1):35-51. Date of access: 8 May 2022. Anthony, J. 2022. 95 essential project management statistics: 2022 market share & data analysis.  Date of access: 10 May 2022.Attarzadeh, I. & Ow, S.H. 2008. Project management practices: The criteria for success or failure. Communications of the IBIMA, 1(28):234-241.  Date of access: 1 May 2022. Audit, I.S. & Association, C. 2018. Cobit 2019 framework: Governance and management objectives. ISACA., viewed 7 May 2022 Charvat, J. 2003. Project management methodologies: Selecting, implementing, and supporting methodologies and processes for projects. In: Wiley, NJ. pp. 1-7. Cohen, H. 2019. Project management statistics: 45 stats you can't ignore.  Date of access: 10 May 2022.Cook, A.E., Gann, A.S., Ray, D.A. & Zhang, X. 2021. Advantages, challenges, and success factors in implementing information technology infrastructure library. Issues in Information Systems, 22(2):187-198.  Date of access: 7 May 2022. De Haes, S., Van Grembergen, W., Joshi, A. & Huygh, T. 2020. Cobit as a framework for enterprise governance of it. In. Enterprise governance of information technology: Springer. pp. 1-42.  Date of Access: 7 May 2022Despa, M.L. 2014. Comparative study on software development methodologies Database Systems Journal, 5(3):37-56.  Date of access: 10 May 2022. El Khatib, M., Alabdooli, K., AlKaabi, A. & Al Harmoodi, S. 2020. Sustainable project management: Trends and alignment. Theoretical Economics Letters, 10:1276-1291.  Date of access: 1 May 2022. 10.4236/tel.2020.106078 Famuwagun, O.S. 2020. Project management methodologies and bodies of knowledge in contemporary global projects.1-15.  Date of access: 8 May 2022. 10.13140/RG.2.2.11247.87204Ghildyal, A. & Chang, E. 2017. It governance, it/business alignment and organization performance for public sectors. Journal of Economics, Business and Management, 5(6):255-260.  Date of access: 6 May 2022. Ghosh, S., Forrest, D., DiNetta, T., Wolfe, B. & Lambert, D.C. 2012. Enhance pmbok® by comparing it with p2m, icb, prince2, apm and scrum project management standards. PM World Today, 14(1):1-77.  Date of access: 8 May 2022. Goldenson, D.R. & Gibson, D.L. 2003. Demonstrating the impact and benefits of cmmi: An update and preliminary results. Pittsburgh, PA 15213-3890: University, C.M.  Date of access: 8 May 2022.Harrin, E. 2022. The 2021 project management report.  Date of access: 10 May 2022.Ilieş, L., Crişan, E. & Mureşan, I.N. 2010. Best practices in project management. Review of International Comparative Management, 11(1):43-51.  Date of access: 8 May 2022. Jugdev, K., Müller, R. & Hutchinson, M. 2009. Future trends in project management: A macro-environmental analysis. Project management circa 2025:229-240.  Date of access: 1 May 2022. Karaman, E. & Kurt, M. 2015. Comparison of project management methodologies: Prince 2 versus pmbok for it projects. Int. Journal of Applied Sciences and Engineering Research, 4(4):572-579.  10.6088.ijaser.04059Khoja, S.A., Dhirani, L.L., Chowdhary, B. & Kalhoro, Q. 2010. Quality control and risk mitigation: A comparison of project management methodologies in practice. In. 2010 International Conference on Education and Management Technology. IEEE. pp. 19-23. Date of Access: 7 May 2022Low, F.S. 2015. Application of japanese project management methods (p2m/kpm) in japanese organisations in japan and malaysia. UTAR.  Date of access: 8 May 2022.Magalhaes, M. 2019. Cobit 2019: An effective governance framework for it pros.  Date of access: 4 May 2022.Nakakawa, A., Lutaaya, S., Apio, E., Kibuukamusoke, F. & Odong, S.P. 2019. It management approaches (it governance, (cobit), it delivery and support (itil), it implementation (cmm and cmmi). ResearchGate.net.  Date of access: 7 May 2022.Nelson, R.R. 2007. It project management: Infamous failures, classic mistakes, and best practices. MIS Quarterly executive, 6(2),  Date of access: 1 May 2022. Nuotilla, J., Aaltonen, K. & Kujala, J. 2016. Challenges of adopting agile methods in a public organization. International Journal of Information Systems and Project Management, 4(3):65-85.  Date of access: 9 May 2022. Obrutsky, S. 2015. Comparison and contrast of project management methodologies pmbok and scrum.  Date of access: 10 May 2022.Pace, M. 2019. A correlational study on project management methodology and project success. Journal of Engineering, Project, and Production Management, 9(2):56.  Date of access: 2 May 2022. 10.2478/jeppm-2019-0007Paredes, C. & Ribeiro, P. 2018. Future trends in project management. In. 2018 International Conference on Intelligent Systems (IS). IEEE. pp. 637-644. Date of Access: 7 May 2022Peng, G., FENG, J.-w. & WANG, H.-t. 2007. Development and comparative analysis of the project management bodies of knowledge. Management science and engineering, 1(1):106-111.  Date of access: 8 May 2022. Rao, K.N., Naidu, G.K. & Chakka, P. 2011. A study of the agile software development methods, applicability and implications in industry. International Journal of Software Engineering and its applications, 5(2):35-45.  Date of access: 10 May 2022. Salameh, H. 2014. What, when, why, and how? A comparison between agile project management and traditional project management methods. International Journal of Business and Management Review, 2(5):52-74.  Date of access: 2 May 2022. Savio, R. 2021. Latest project management trends and challenges with covid-19. Eximia, 2(1):8-11. Date of access: 2 May 2022. Siang, L.F. & Yih, C.H. 2012. A review towards the new japanese project management: P2m and kpm. Trends and Developments in management Studies, 1(1):25-41. A REVIEW TOWARDS THE NEW JAPANESE PROJECT MANAGEMENT: P2M AND KPM Date of access: 8 May 2022. Simmonds, A. What is cobit 2019? Everything you need to know.  Date of access: 7 May 2022.Staples, M., Niazi, M., Jeffery, R., Abrahams, A., Byatt, P. & Murphy, R. 2007. An exploratory study of why organizations do not adopt cmmi. Journal of systems and software, 80(6):883-895.  Date of access: 8 May 2022. 10.1016/j.jss.2006.09.008Whittaker, B. 1999. What went wrong? Unsuccessful information technology projects. Information Management & Computer Security, 7(1):23-29.  Date of access: 1 May 2022. Xue, R., Baron, C., Esteban, P. & Zheng, L. 2015. Analysis and comparison of project management standards and guides. In. Proceedings of the International Conference on Mechanics, Materials, Mechanical Engineering and Chemical Engineering (MMMCE 2015) Barcelona, Spain. Recent Advances on Mechanics, Materials, Mechanical Engineering and Chemical Engineering. pp. 15-22. Date of Access: 9 May 2022Zhang, S., le Fever, H. & le Zhang S, F.H. 2013. An examination of the practicability of cobit framework and the proposal of a cobit-bsc model. Journal of Economics, Business and Management, 1(4):391-395.  Date of access: 7 May 2022. 10.7763/JOEBM.2013.V1.84Annexures
19 October 2022Llewellyn Anthony	32969694Prof. N. KrugerLIST OF FIGURESLIST OF TABLESABSTRACTIn most, if not all, organizations, the desire for financial gains and a competitive edge is driving technological advancements. ICT breakthroughs are becoming ever more prone to shorter life spans, rapid replacements, and obsolescence. Several Chief Executive Officers, Chief Information Officers, and Government Information Technology Officers struggle to effectively manage ICT and to establish a lasting ICT environment that supports human and social demands. This article includes a comparison between the global IT job market and South Africa's, highlighting the most popular software development methodologies utilized by both. In order to substantiate evidence as to what the most popular software development methodologies in the modern era, various small, medium, and large business case studies will be explored. An overview of at least five South African companies and the software development methodologies they have utilized will be discussed in this article. In order to illustrate the differences between these organizations in terms of the projects they completed, the software methodologies they adopted, successes and failures of the companies and implementations, and furthermore, the differences must be visualized via using tables and/or figures. A conclusion must be drawn about the most popular software methodologies utilized in the South African IT industry, based on reputable evidence provided by this article. An evaluation must occur as to why the methodologies are effective/ineffective in each case and suggestions must be offered for the future regarding software development approaches within companies, keeping in mind the effect Covid-19 had on South African industries. The essence of this article is to determine which software development methodologies are the most suitable and efficient for the international IT industry, specifically the South African technology industries.Keywords: Software development methodologies, Agile, SCRUM, DevOps, Framework, project management, organization, technology, methodology, Information Technology, South AfricaCHAPTER 1: INTRODUCTION This section introduces the content about the article. Chapter 1 introduces the structure of the article. Chapter 2 discusses the related work to the article, highlighting previous research that have been conducted on the topic. Chapter 3 demonstrates the System Model, which lists all the hypotheses and assumptions of the environment, based on the research problem. A system model diagram represents the flow between arguments and sections in the article. Chapter 4 highlights the problem statement, which states the research question/problem. Chapter 5 presents the solution to the research question: A thorough justification is provided of the solution's principles, ideas, and functionalities. One should ensure that the solution does not rely on a theorem or other unproven ideas, they need to be clarified before moving on to the specific descriptions. This section's primary component is a comprehensive overview of the problem and the essence of the solution. The solution is broken down into components and described, respectively. Chapter 6 discusses the difference between a framework and a methodology, regarding the IT industry and project management. Chapter 7 lists the different software development methodologies that is utilized in the IT industry, within each subsection there is a focus on the life cycle, perks and issues regarding a certain methodology. Section 7.1 introduces the Agile SCRUM methodology, the functionalities thereof and a description of the South African companies that utilizes it. Section 7.2 introduces the Waterfall methodology, the functionalities thereof and a description of the South African companies that utilizes it. Section 7.3 introduces the DevOps methodology, the functionalities thereof and a description of the South African companies that utilizes it. Section 7.4 focuses on a hybrid Agile and Waterfall methodology, the functionalities thereof and a description of the South African companies that utilizes it. Chapter 8 visualizes the results and analysis conducted on the previous chapters and the results from a survey on software development methodologies, from five South African IT companies. Finally, a conclusion is drawn based on the research conducted and evaluation of the survey results. This article identifies the most feasible and efficient software development methodologies that happens to be a trend in the current IT industry.CHAPTER 2: RELATED WORKSThe ongoing failure of both agile and traditional software development projects has prompted discussion, debate, and study of critical success criteria, which are the elements most essential to the success of a software engineering process (Chiyangwa & Mnkandla, 2017:1). Though crucial success elements and approaches are becoming more diverse, there are only a few conceptual frameworks that can explain how they relate causally (Chiyangwa & Mnkandla, 2017:1).Software development professionals have been sluggish to adopt agile approaches despite the potential advantages (Chiyangwa & Mnkandla, 2017:2). In a global study, Chan and Thong found that 60% of the organizations they examined were neither utilizing agile nor any traditional approaches, only 6% were adhering to a methodology by book, and 79% had no intention of adopting one (Chiyangwa & Mnkandla, 2017:2). The fact that early adopters of technology are adamantly opposed to changes to new technology is one of the factors contributing to the lack of acceptance of agile in particular (Chiyangwa & Mnkandla, 2017:2). Additionally, it is believed that agile approaches are adaptable to software development during a project and that they are generically applicable (Chiyangwa & Mnkandla, 2017:2). The adoption of agile software development approaches is still a problem that agile practitioners are focused on (Chiyangwa & Mnkandla, 2017:2).It seems that over 65% of software initiatives in South Africa were seen as failures and experiencing difficulties (Chiyangwa & Mnkandla, 2017:2). This may have occurred as a result of software development initiatives that were not finished on schedule or within the projected budget (Chiyangwa & Mnkandla, 2017:2). Thirty-four percent of the software development initiatives were thought to be successful (Chiyangwa & Mnkandla, 2017:2). In a related study, it was shown that in South Africa, 59% of the software development projects evaluated in 2011 were regarded as successful, while 41% were considered unsuccessful and suffering challenges (Chiyangwa & Mnkandla, 2017:2).It was discovered that process factors and performance expectancy factors are causally related to success factors (Chiyangwa & Mnkandla, 2017:8). Additionally, organizational, process, people, and project aspects all directly influence how well agile software development projects are executed (Chiyangwa & Mnkandla, 2017:8). Findings from the study conducted by Chiyangwa and Mnkandla (2017:8) demonstrate that organizational variables and performance expectancy factors are the key indicators, which determines whether people become engaged in agile software development projects. The performance expectancy is the single most crucial aspect that needs to be considered while promoting software projects in South Africa, even though organizational characteristics involving agile software development projects were determined to be statistically significant (Chiyangwa & Mnkandla, 2017:8).There happens to be insufficient proof to prove that organizations only employ one particular SDM (Systems Development Methodology) type (Moyo et al., 2022). According to a study conducted by Moyo et al. (2022),  the results indicate that SDMs cannot be divided into the plan-driven SDM and the agile SDM in the context of systems development in South Africa. The dataset for the study revealed the utilization of hybrid SDMs (Moyo et al., 2022). According to the findings, organizations implement basic SDMs and generate hybrid SDMs in response to the contextual stresses unique to each systems development project (Moyo et al., 2022). The base SDM is combined with SDM components from the same class of SDMs or a different class of SDMs to create the hybrid SDMs (Moyo et al., 2022).CHAPTER 3: SYSTEM MODELSThe system model is derived from hypothesis derived from the introductory sections. A hypothesis is derived from the introductory section: a system development methodology is chosen based on the type of company and projects that is being managed by that company. The system model is structured as follows: an introduction to the article, followed by related works, system model description and the problem statement. The three aforementioned sections are interrelated. The solution to the problem statement is explored afterwards. A discussion is followed on frameworks VS methodologies, which clearly indicates the differences between a framework and a methodology. Three popular software development methodologies are discussed, which consists of Agile (SCRUM), Waterfall and DevOps. Each methodology is provided a description, the life cycle of the methodologies and the functionalities thereof, as well as the known benefits and issues associated with each methodology. A description of five South African companies will be provided that utilizes one of those methodologies to manage their software projects. An analysis occurs after the discussion of the methodologies, tables and figures will be used to substantiate the evaluations. The results from surveys filled in by the five South African companies will be thoroughly analysed and visualized, as well as more generic statistics found on the software methodologies. Finally, a definite conclusion will be derived based on the analysis of the results and research conducted in this article. Figure 1.1 demonstrates the system model: Figure 1: The System Model Proposal CHAPTER 4: PROBLEM STATEMENTThe research question can be described as follows: one needs to describe the differences between popular software development methodologies, and based on industry research, determine which proves to be the most popular methodologies employed by IT companies in South Africa. Research must be conducted on at least five South African companies, examining which software development methodologies they utilize via a survey. Given the impact Covid-19 has on South African industry, an assessment of why the techniques is successful or unsuccessful in each situation and recommendations for the future in terms of software development methods within organizations are required. The primary objective of this article is to identify the software development approaches that are most effective and appropriate for the global IT sector, in particular the South African technology sectors.CHAPTER 5: SOLUTIONThe solution to answer the research question is structured as follow: firstly, a discussion will occur on the differences between a framework and a methodology, to avoid confusion of using terms and clearly highlighting the definition of a framework and a methodology. A discussion follows on three popular software development methodologies utilized in the industry: Agile (SCRUM), Waterfall and DevOps. Each methodology’s discussion is structured as follows: an introduction to the methodology, the life cycle of the methodology, the methodology’s functionalities, as well as the benefits and issues associated with each methodology. After a discussion on the methodologies, a description will be provided of South African tech companies which utilizes those software development methodologies. This serves as an introduction to the companies that will be researched, and their survey results will be used to analyse and determine the most popular methodologies employed in the South African IT industry. After the discussion of the methodologies, an analysis will occur on the software development methodologies employed by the five South African companies. This will be done via analysing the survey results and visually representing it as graphs. More generic statistics on the software development methodologies will be used to substantiate the evidence from the survey, this can be done via providing tables and figures to support the results from the survey. The results received from conducting the research must attempt to solve the research question and indicate which software development methodologies are currently preferred in the industry, especially in South Africa. Finally, a conclusion must be derived as to which is the preferred methodology utilized in the industry, based on the research conducted, the methodology’s functionalities, benefits, issues and adaption to the modern era of project management and other constraints such as COVID-19.CHAPTER 6: FRAMEWORKS VS METHODOLOGIESIn general, a framework offers a basic abstraction of a solution to a variety of problems that have some similarities (Mnkandla, 2009:2). Without going into specifics about what must be done during each phase of implementation, a framework will often identify the actions or phases that must be followed to solve a problem (Mnkandla, 2009:2).Where there is ambiguity, a framework and a methodology may be integrated into one entity (Mnkandla, 2009:2). When dealing with complicated challenges, the architectural overview of solution designs suffers in such circumstances (Mnkandla, 2009:2). The practical realities of implementing software development projects call for the adoption of approaches that can be quickly adjusted to the various and changing business environments without impacting the architecture of the system (Mnkandla, 2009:2). Therefore, if the methodology is entwined with the framework, the result is a monolithic structure rather than a more ideal modular or layered one (Mnkandla, 2009:2).A group of approaches utilized in the creation of applications might be referred to as software engineering methodologies (Mnkandla, 2009:2). The specifics of what should be done during each stage of the software development process are provided by methodologies (Mnkandla, 2009:2). Methodologies do not always provide instructions on how something should be done (Mnkandla, 2009:2). This level of detail typically enables the business to adapt the methodology to its surroundings, for instance by creating templates, and additional documents outlining the proper way to do tasks (Mnkandla, 2009:2). A methodology is just a method of executing tasks; in this case, it refers to the culture that is adhered to when designing software systems (Mnkandla, 2009:2).A framework should be used at a more abstract level; therefore, one requires a methodology to implement a framework (Mnkandla, 2009:3). However, sometimes people confuse the meaning of a methodology and a framework to the point where one may find that a methodology and a framework is used interchangeably (Mnkandla, 2009:3). A project management framework like PMBOK, which enables users to utilize their own methodology to implement the process in each knowledge area, comes to mind as an illustration (Mnkandla, 2009:3).At the most basic level, a framework merely states that the task or project should be completed in stages or phases, but it typically does not mention the specific phases (Mnkandla, 2009:5). We can therefore conclude that the framework's main purpose is to provide instructions on how things should be done (Mnkandla, 2009:5).Methodologies, on the other hand, dives into the stages and provide details of what those phases are and the related tasks in each phase (Mnkandla, 2009:5). Methodologies provide more precise instructions on what should be done (Mnkandla, 2009:5). To individuals who are interested in maximizing simplicity in systems design, implementation, and maintenance, frameworks and methodologies are not the same concept (Mnkandla, 2009:5).CHAPTER 7: SOFTWARE MANAGEMENT METHODOLOGIESThis section introduces the Agile (Scrum), Waterfall and DevOps software development methodologies. Each methodology’s life cycle will be described, as well as some of their functionalities and known benefits and issues when utilizing this methodology to manage a project. Finally, a description will be provided of a South African company known to employ the specific methodology to manage their projects. AGILE/SCRUMAgile is a hybrid strategy based on the renowned "Agile Manifesto" and an evolutionary methodology focused on iterative development (Jones, 2017:49). Around 17 renowned software specialists gathered in 2001 at the Snowbird resort in Utah to discuss the problems with software development and possible solutions (Jones, 2017:49). Agile is the most widely used software development methodology as of 2016 (Jones, 2017:49). The Agile Manifesto, which was released in February 2001, was the outcome of this conference (Jones, 2017:49). The following are the key tenets of the Agile Manifesto:People and relationships are preferable to formally established procedures and instruments (Jones, 2017:49).A working program is preferable to extensive documentation (Jones, 2017:49).Collaboration with the customer is preferable to extensive contracts (Jones, 2017:49).Following a strict plan is preferable to adapting to change (Jones, 2017:49).The Agile Manifesto and the widely accepted Agile methods are designed for relatively small internal projects where the user community and the developers are housed in the same building (Jones, 2017:50).Life Cycle Scrum is an approach for developing software progressively in complicated contexts (Despa, 2014:44). The product owner creates stories, which are a prioritized list of software requirements (Despa, 2014:45). The Product Backlog is made up of all the stories (Despa, 2014:45). The Scrum methodology implements a time-boxed strategy in which development cycles, known as sprints, are limited to 4 weeks and conclude with a functional version of the product (Despa, 2014:45). The Backlog is made up of all the stories for each sprint (Despa, 2014:45). Daily Scrum meetings, which are only allowed to last 15 minutes, are done to evaluate the progress being made (Despa, 2014:45). The project manager and other team members are not in charge of assigning tasks (Despa, 2014:45). Each team member participates in the work assignment process in self-organized Scrum development teams (Despa, 2014:45). A Scrum Master directs the team's work and keeps them on track (Despa, 2014:45). The project owner provides comments at the conclusion of each sprint (Despa, 2014:45).Figure 2: The Agile Scrum Life Cycle sprint (Despa, 2014:45)FunctionalitiesThe following is how Agile Scrum is typically implemented:Users who are embedded and supply requirements, are part of the development process (Jones, 2017:50)One makes use of user stories to study requirements (Jones, 2017:50)Dividing larger projects into about two-week sprints (Jones, 2017:50)Daily status reports in Scrum meetings (Jones, 2017:50)Pair programming, or two programmers going back and forth between navigating and coding (Jones, 2017:50)Writing test cases before writing code, often known as test-driven development (Jones, 2017:50)Agile-specific metrics like burn down, burn up, and velocity, etc. (Jones, 2017:50)Introducing agile concepts are usually done via agile “coaches”. (Jones, 2017:50)An Agile sprint is a compact work package that can be completed in a few weeks and once finished, offers users the features they have requested (Jones, 2017:50). Over a string of numerous sprints, the entire project will be developed (Jones, 2017:50). A daily status meeting presided over by a Scrum master is referred to as a Scrum meeting. Progress, issues, and fascinating technical subjects will be discussed as necessary (Jones, 2017:50).Agile requirements are frequently obtained as user stories or scenarios that describe how particular features will function when the project is in use (Jones, 2017:50). An epic is a collection of user stories (Jones, 2017:50). Embedded users are one or more application users that collaborates on a daily basis with the development team to help define the requirements for each sprint rather than being kept apart from them (Jones, 2017:50). Typically, firms hire agile coaches when they first decide to embrace the methodology. They often take part in the first sprints and planning meetings while also educating managers and team members about the fundamentals of Agile (Jones, 2017:50).BenefitsTable 1: Scrum Benefits and corresponding codesSource: De Carvalho and Mello (2011:44)IssuesThe Agile methodology loses effectiveness for larger projects with geographically dispersed teams (particularly across numerous time zones) (Jones, 2017:51). Agile can also be challenging when working on projects with numerous subcontractors (Jones, 2017:51). Agile is not ideal for distributed development, particularly if foreign development departments are involved, where there may be numerous time zone inconsistencies that make even Skype or web-based Scrum meetings challenging (Jones, 2017:51). Agile is not the best approach for improving legacy programs or maintaining and fixing bugs because it is largely focused on newer projects (Jones, 2017:52). It is also clear that Agile is a poor fit for projects requiring official government certifications, such as avionics kits and medical devices (Jones, 2017:52). The informal Agile approach to documentation is inappropriate for the legally required paperwork that government regulations impose on projects (Jones, 2017:52). Planning and estimating for Agile projects have proven to be a little more challenging than for other methodologies due to the numerous sprints and Agile's ability to accommodate frequent requirements changes (Jones, 2017:52). Agile also struggles to measure outcomes, which provides little information on quality and productivity (Jones, 2017:52). Comparisons of the productivity of Agile projects to those of other methodologies are challenging due to the peculiar structure of several sprints (Jones, 2017:52).Companies from South Africa employing the methodologyRETRO RABBITRetro Rabbit is a UI/UX company that creates outstanding software. They provide specialized solutions that foster brilliance and creativity. They produce innovative products and provide clients with the most comprehensive information technology design solutions by fusing data science, human-centred design, and the most recent software development methodologies.They are the epitome of agile development, collaborating directly with their clients to build a solution that will meet the needs of the users and backed by their team of skilled individuals. Their specialties include game development, experience design, cloud computing, artificial intelligence, and mobile/web development. From engineering to big data to financial services, they have experience in every industry. Standard Bank is one of their most notable clients. The company’s headquarters is based in Wapadrand, Pretoria.ENTELECTEntelect is an innovative software engineering firm that combines technology and best practices to provide innovative solutions to a variety of industries. Entelect has an unrivalled level of individuals and depth of knowledge dedicated to answering customers' needs on their terms. Entelect was founded on the uncompromising philosophy of highly educated, best-in-class professionals.Leading publicly traded firms as well as specialized private businesses in need of specialized software solutions make up the majority of their clients. The best software developers in South Africa work for them, and they have successfully designed and put hundreds of unique systems into operation both in South Africa and abroad.With numerous internal training programs, from leadership development to specialized technology courses, Entelect provides its employees with unequalled professional growth options. Their 1up rewards program provides a range of exciting advantages for high performers, whether they help out co-workers or the foundation's youth, work together on initiatives, go to social events, or simply do a wonderful job. Their headquarters is based in Melrose, Johannesburg.AGILE BRIDGEThe software consulting firm Agile Bridge is situated in Hazelwood, Pretoria. The Agile Bridge team is still dedicated to studying in order to stay on the forefront of programming and software development techniques. They take pleasure in their capacity to challenge one another to improve, and they are constantly learning from one another, their clients, and the top professionals in their industry, both locally and globally.The development of integrated and enabling software solutions for medium and big corporate clients in South Africa is driven by their special combination of expertise, certification, and in-depth business understanding.The term Agile Bridge contains the ideology they uphold. The word Agile has two different meanings. On the one hand, it shows how adaptable the business is when trying to meet the needs of client. The meeting location where they co-create business software solutions with clients is indicated by the moniker Bridge. The bridge is another way of saying the software bridge pattern language. The concept of a bridge is extremely common in the language of software and systems. A bridge is employed at an elevated level to connect various domains. A bridge is used to separate abstraction from implementation at a lower level.WATERFALLAfter cowboy, this methodology is the second-oldest approach (Jones, 2017:523). It first appeared in the late 1960s, when software projects expanded to the point that they needed teams and were therefore expensive (Jones, 2017:523). A previous approach with a similar purpose was established in 1956 for the Semi-Automatic Ground Environment (SAGE) defence system (Jones, 2017:523). Between roughly 1970 and 2014, waterfall development was utilized on more than 2,000,000 projects, including some significant applications that are still in use today (Jones, 2017:523). Although several alternative methodologies, like Agile, team software process (TSP), and Rational Unified Process (RUP), have replaced waterfall, it is still frequently used (Jones, 2017:523). The waterfall method's quality and productivity results fall short of those of its superior replacement approaches (Jones, 2017:523). Due to the fact that this technique's visual depiction mimics a stream flowing over a string of waterfalls, it is known as waterfall development (Jones, 2017:523). Despite the name, the waterfall stages don't truly end before the next one starts in reality (Jones, 2017:523). As an illustration, requirements are typically only around 50% complete when design, 60% complete when coding, and only about 35% complete when testing begins (Jones, 2017:523). Due to the phases' overlap, it is difficult to plan and estimate projects because the overall number of phases does not add up to the whole number of project schedule hours (Jones, 2017:523). Despite being used frequently in many countries today, waterfall is seen as something of an antique that is steadily losing favour as of 2016 (Jones, 2017:525).Life Cycle Waterfall places a focus on rigorous preparation and produces thorough documentation (Despa, 2014:41). The Waterfall methodology is a sequential, linear process in which each step begins only after the one before it is complete (Despa, 2014:41). Deliverables are specific to each level (Despa, 2014:41). The Waterfall methodology emphasizes meticulous software planning and architecture and is predictable (Despa, 2014:41). After the software application has been fully developed and tested, the project owner is consulted (Despa, 2014:41). The Waterfall methodology is appropriate for small-scale software development projects where requirements are clear and thorough project planning is simple to create (Despa, 2014:41).Figure 3: The Waterfall Life Cycle (Despa, 2014:41)FunctionalitiesAnalysis Phase - software requirements specification (SRS):  thorough and detailed explanation of how the software's behaviour will be described. System and business analysts are involved in defining both needs for both functional and non-functional requirements (Bassil, 2012). Usually, use cases are used to specify functional requirements (Bassil, 2012). This describes how users interact with the program (Bassil, 2012). They include specifications like purpose, scope, and perspective, capabilities, software characteristics, and user features, functions, requirements, and any other requirements (Bassil, 2012). Non-functional requirements are any criteria, constraints, limitations, or requirements placed on the software's operation and design as opposed to specific actions (Bassil, 2012). Reliability, scalability, testability, availability, maintainability, performance, and quality standards are some of its characteristics (Bassil, 2012).The planning and problem-solving process for a software solution is called the design phase (Bassil, 2012). To describe the strategy for a solution, which comprises algorithm design, software architecture design, database conceptual schema and logical diagram design, concept design, graphical user interface design, and data structure definition, it involves software developers and designers (Bassil, 2012).Phase of implementation: Through programming and deployment, a concrete executable program, database, website, or software component is created from the realisation of business needs and design standards (Bassil, 2012). The database and text files are developed during this stage, and the actual code is written and assembled into a usable program (Bassil, 2012). In other words, it is the process of transferring all of the specifications and plans into a real-world setting (Bassil, 2012).The testing phase, sometimes referred to as verification and validation, is the procedure used to ensure that a software solution complies with the original requirements and specifications and serves the intended purpose (Bassil, 2012). In reality, validation is the process of evaluating software during or after the development process to determine whether it satisfies specific requirements, whereas verification is the process of evaluating software to determine whether the products of a given development phase satisfy the conditions imposed at the start of that phase (Bassil, 2012). Additionally, the testing stage is where debugging is carried out, in which problems and system malfunctions are identified, fixed, and improved as necessary (Bassil, 2012).The maintenance phase involves making changes to a software solution to improve performance and quality, refine output, and fix problems after it has been delivered and deployed (Bassil, 2012). This stage also allows for the performance of additional maintenance tasks such software environment adaptation, user requirement accommodation, and software reliability improvement (Bassil, 2012).Companies and projects that still utilize waterfall can supplement it with more contemporary techniques like automated testing and static analysis (Jones, 2017:525). In a waterfall setting, formal inspections of important deliverable subjects including requirements, design, and code inspections are particularly helpful (Jones, 2017:525).BenefitsWaterfall is supported by all parametric estimating tools and has more valid benchmark data than any other methodology (Jones, 2017:525). This means that estimating and measuring waterfall projects is fairly easy to do (Jones, 2017:525). The benefits of using the Waterfall Method are undeniable because it delivered the discipline and organization that were most required at the time the method was developed (Simão, 2011:43). It introduced organization, preparation, and discipline to a world where coding and fixing was the primary tactic (Simão, 2011:43). The cost of a project increases significantly when unanticipated requirements are found (Simão, 2011:43).IssuesThe primary flaw of waterfall, which appears to have some merit, is that it strives to do too much too soon, such as developing complete requirements before beginning design (Jones, 2017:525). When design work begins, requirements are rarely more than 50% firm (Jones, 2017:525). The fundamental tenet of the waterfall is "measure twice, cut once," which is both its most cherished and despised concept (Simão, 2011:43). The Waterfall approach is referred to as Big Design Up Front (BDUF), where first it is carefully determined what/how and whether it is worthwhile to be done (measure"), and then start coding the product ("cut") (Simão, 2011:43). An ideal model that can only be roughly approximated is the Waterfall approach (Simão, 2011:44). The methodology's worst naysayer is the discipline and perfectionism it promotes (Simão, 2011:44). A project shouldn't waste a lot of time in the pre-coding phases, as most clients and users will not become aware of certain needs until later in the project (Simão, 2011:44). On the other hand, moving up the Waterfall processes is far more expensive than moving through the phases (Simão, 2011:44). A feedback loop between phases is also recommended by some authors in order for the Waterfall technique to be effective (Simão, 2011:44). This will prevent errors made early on from affecting later phases, and the linearity of the process is the key to success when employing the model (Simão, 2011:44).Common criticisms of the model include the idea that it is difficult to strike a balance between resources and requirements and that documents or specifications should not be changeable because software development uses a static document that can be changed without having to return to a previous phase (Simão, 2011:44). Additionally, the needs for real-world projects change more frequently than expected, and the method's monolithic architecture makes it difficult for it to adapt to these changes (Simão, 2011:44). Despite the fact that it has drawbacks, Waterfall is still regarded as one of the most significant techniques that has significantly impacted the software industry (Simão, 2011:44).Companies from South Africa employing the methodologyDIMENSION DATAWith a portfolio of services that includes systems integration, managed services infrastructure, cloud solutions, business applications, customer experience, and intelligent security solutions, Dimension Data is a top African-born technology firm with operations in the Middle East and Africa. Dimension Data offers innovative solutions that maximize the rapidly changing technological landscapes of today and empower customers to utilize data in the digital age.Dimension Data, a proud member of the NTT Group, one of the top information and communication technology (ICT) firms in the world, was founded in 1983 and is headquartered in Johannesburg. The NTT Group is made up of a number of different international technology enterprises.In 2020, Dimension Data finished integrating all of its companies under a single Dimension Data brand, employing approximately 10,000 people in fifteen different countries. With the best technology from across the world, Dimension Data continues to aggressively invest in innovation, offering everything from consultancy, technical, and support services to a completely managed solution.After successfully completing a big BBBEE transaction in 2019, Dimension Data is now a level 2 BBBEE contributor. Dimension Data will keep putting initiatives into action to make sure it makes a meaningful contribution to the shift toward an inclusive society.DEVOPSThe words "development" and "operations" are combined to form the term "DevOps" (Jones, 2017:149). One of the more recent approaches is called DevOps, and it is built on cooperation between the development team and the operations and maintenance teams (Jones, 2017:149). DevOps is a rather modern approach, which started in Belgium in 2009 (Jones, 2017:149). Modern techniques like continuous integration and continuous delivery are used in DevOps (Jones, 2017:149). DevOps aims to produce operations that are more stable and reliable while also requiring minimal maintenance (Jones, 2017:149). This is in addition to faster development and higher quality deliverables than Waterfall (Jones, 2017:149). DevOps is obviously targeted for software projects where operations people are located in the same building as development personnel (Jones, 2017:149). DevOps is not intended to be applied to commercial software that will be installed at countless remote sites in hundreds or possibly thousands of businesses (Jones, 2017:149). Many of the largest software developers in the world, including IBM, Computer Associates, Netflix, Facebook, Amazon, Google, and Twitter, as well as thousands of medium-sized and small businesses, have quickly embraced DevOps concepts (Jones, 2017:149). Two of the modern software development approaches that are expanding the quickest are DevOps and container development (Jones, 2017:150). They are not appropriate for all sizes and types of software applications because both are slightly specialized (Jones, 2017:150).Life Cycle The DevOps cycles provide cyclic, natural cycles that the practitioners use to mark their lifecycles (Yarlagadda, 2021:115). The loop emphasizes the need for regular communication and repetition for phase improvement despite the DevOps life cycle's sequential flow (Yarlagadda, 2021:115). A DevOps lifecycle consists of six iterative stages that represent the skills, procedures, and methods essential for development (Yarlagadda, 2021:115). The left and right sides of the loop represent various operating talents, procedures, and tools. To ensure that the process's speed, alignment, and quality are maintained, each facet faces independent but ongoing team collaboration and communication (Yarlagadda, 2021:115). Planning, construction, integration and deployment, monitoring, operations, and responding to feedback are part of the life cycle (Yarlagadda, 2021:115). Each team should have access to open tools and guidelines at every step of development before beginning any work (Yarlagadda, 2021:115). The devices can be modified to meet the demands and objectives of the developer (Yarlagadda, 2021:115). The technique leads to the rapid development of trustworthy, high-quality software (Yarlagadda, 2021:115). Because the stages are iterative, identifying bugs becomes simpler and faster (Yarlagadda, 2021:115).Figure 4: The DevOps Life Cycle (Yarlagadda, 2021:115)FunctionalitiesDevOps has the sense of a start-up business where developers go beyond just developing and act as quality assurance (QA) specialists, testers, and even have job supporting operations (Jones, 2017:149). These numerous roles are now known collectively under the new title full stack developer (Jones, 2017:149). DevOps also has a strong Agile character and frequently applies several Agile principles, such as test-driven development, embedded users for requirements, and occasionally Scrum sessions (Jones, 2017:149). Some of the functionalities of DevOps include:Regular communication between development and operations staff (Jones, 2017:150)Ongoing communication between developers and stakeholders (Jones, 2017:150)Constant improvement of deliverables (Jones, 2017:150)Every day, continuous deliveries (Jones, 2017:150)Initial QA (Quality Assurance) (Jones, 2017:150)A bigger focus on developers' quality roles (Jones, 2017:150)Early development testing (Jones, 2017:150)The absence of commonly used productivity measures like function points (Jones, 2017:150)The absence of uniform quality metrics (Jones, 2017:150)Limited information on defect elimination effectiveness and defect removal efficiencies (DRE) (Jones, 2017:150)BenefitsDevOps has demonstrated decent performance through 2016 in its primary niche of IT applications with high transaction rates and frequent or continuous runs (Jones, 2017:150). DevOps is becoming increasingly popular, which has sparked the development of dozens of specialized new tools and even specialized new businesses that provide training and support for emerging DevOps organizations (Jones, 2017:150). Agile experienced slower growth in 2014 than DevOps (Jones, 2017:150). Business software applications with large transaction rates and frequent runs, possibly continuous 24/7 runs, are the major focus of DevOps (Jones, 2017:150). Massive solutions have been introduced to the industrial platform via DevOps (Yarlagadda, 2021:116). Through the rapid adoption of methodology and technologies in their development processes, the majority of industries and organizations, particularly those in the United States, have reaped the benefits of DevOps (Yarlagadda, 2021:116). For instance, the American company Enhanced Automation has eliminated the usual aches associated with the cumbersome IT processes (Yarlagadda, 2021:116). Through the advantages of agile, the business has placed its full attention on automation, cooperation, and adaptability (Yarlagadda, 2021:116). The following advantages have been garnered by these organizations: firstly, they spend less time marketing their goods or services since they are current and address current problems (Yarlagadda, 2021:116). As a result, they put their deliverables much quicker into production (Yarlagadda, 2021:116). Secondly, throughout time, the quality of return on investment has greatly improved (Yarlagadda, 2021:117). Thirdly, real-time purchases of services and goods with quicker delivery have resulted in great client satisfaction (Yarlagadda, 2021:117). Fourthly, there has been excellent operational efficiency with less cost and effort due to shorter operational times (Yarlagadda, 2021:117). Automation is used to make that happen. Fifth, the increased collaboration has given developers and IT managers a platform to use straightforward codes to fix current problems (Yarlagadda, 2021:117). Finally, these codes aid in the quick identification and correction of faults and problems (Yarlagadda, 2021:117). The methods and culture of these development teams limit the risks of misunderstandings and process misalignments, enabling them to anticipate changes with ease (Yarlagadda, 2021:117). Efficiency and product quality both improve with clear and regular communication. Regular testing, deployment, and integration also encourage quick process development and error discoveries (Yarlagadda, 2021:117).IssuesSimilar to Agile, DevOps is plagued by subpar measurement techniques and a lack of trustworthy quantitative data on output, timelines, quality, and customer satisfaction (Jones, 2017:150). The DevOps methodology is controversial, and some claim that it lessens the impact of competent programmers by putting them in test and QA jobs for which they may be unprepared, unqualified, or both (Jones, 2017:150). Since DevOps concepts are still evolving and maturing, the technique is still considered to be "new" and not "stable" (Jones, 2017:150). DevOps, as the name suggests, necessitates access to the software's operations team and data centres (Jones, 2017:150). This indicates that DevOps targets internal IT or online initiatives specifically (Jones, 2017:150). It is obviously inappropriate for embedded applications and systems that run unattended after deployment (Jones, 2017:150). DevOps is also not suited for modern computer-controlled medical equipment like pacemakers, cochlear implants, MRI machines, computed tomography scanners, and other types of implants (Jones, 2017:150). The same is true for many types of military software utilized in combat situations as well as weapon systems (Jones, 2017:150). It is advisable for anyone interested in DevOps concepts to perform frequent Google searches to get the most recent information because DevOps concepts are in flux, which seems to fluctuate on a weekly basis (Jones, 2017:150).Companies from South Africa employing the methodologyBBDWith 38 years of technical and developer experience, BBD is a provider of software development and application design solutions for the education, financial services, insurance, gaming, public, and telecommunications industries. BBD is the industry leader in independent software development, employing over one thousand highly qualified, driven, and experienced IT experts.With operations in Amsterdam, Burgas, Cape Town, Johannesburg, London, Porto, Pretoria, and Pune, they are able to address near- and co-shoring requirements by bringing in the top analysts and software engineers from across the world. Because they have access to in-demand skills, they ensure that they deliver on their commitment to each customer. Their headquarters is based in Amsterdam, the Netherlands.CHAPTER 8: RESULTS AND ANALYSISTable 2: Survey Results from South African CompaniesFrom the survey results from the five South African companies, the following have been determined:Agile (Scrum) is the dominant software development methodology, although others followed additional software methodologies such as Waterfall and DevOps, or hybrid methodologies. More than half of the companies indicated that there is a clear distinction between their project management and software management methodologies. Agile appears to be the dominant project management methodology according to the survey results. COVID-19 also had an enormous impact on these companies. They had to start working on their projects remotely. It also appears that post-COVID-19, companies preferred to switch to a hybrid workplace model between working remotely and being on site. Most of the companies agreed that COVID-19 impacted the way companies manage projects and develop software. COVID-19 also impacted the organisational productivity of these companies. Most of the companies agreed that COVID-19 had a positive impact on project management productivity. All the companies agreed that the pandemic had a positive effect on software development processes. More than half of the companies agreed that COVID-19 had a positive impact on employees' productivity. Employees seem to prefer remote working environments. All companies agreed that COVID-19 had an enormous impact on project management and the development of software products. Due to this fact, it affected the types of technologies employed by companies to develop software and enabling communication between development teams. COVID-19 impacted the way these companies govern projects and strategize. More than half of the companies agreed that COVID-19 had a positive effect on company culture and maturity. COVID-19 had a positive impact on company infrastructure and service deliveries within the companies.COVID-19 enabled teams to collaborate more efficiently in a remote work environment. Even though communication has seemed to improve, it appears that most employees indicated that they found it more challenging to build personal work relationships given the remote work circumstances. The remote work environment also brought some other issues: most employees found it challenging to balance their work and personal life. Security also seems to be an issue that become more prevalent during COVID times. Even though security became a bigger threat, it appears that all of the companies and employees were well suited to combat security threats. A reason companies might have been able to combat security threats more efficiently was due to security protocol updates to adapt to the COVID-19 pandemic. It also appears that the companies experienced few to no cyber-attacks during the pandemic. Security threats were a bigger issue during the COVID-19 pandemic, however. More than half of the companies indicated that employees used their own resources to finish and collaborate on projects remotely. More than half of the companies indicated that employees utilized cyber security tools in their home and work environment. Software appears to be maintained regularly to avoid vulnerabilities against cyber-attacks. Other tools such as VPNs and 2FA were utilized to ensure that employees work in a safe cyber environment.CHAPTER 9: CONCLUSIONThe following deductions were made based on the research from the article and survey results:Although other software development approaches, including Waterfall and DevOps, as well as hybrid methodologies, were also used, Agile (Scrum) is currently the most popular. There is a notable difference between their project management and software management techniques, according to more than half of the firms. The study results suggest that agile project management is the most common methodology. Although other software development approaches, including Waterfall and DevOps, as well as hybrid methodologies, were also used, Agile (Scrum) is currently the most popular. There is a notable difference between their project management and software management techniques, according to more than half of the firms. The study results suggest that agile project management is the most common methodology.COVID-19 also had a significant effect on these businesses. They had to begin conducting their project work remotely. Additionally, it seems that businesses choose to adopt a hybrid workplace model that combines working remotely and on-site labour after COVID-19. The majority of businesses concurred that COVID-19 had an impact on how they manage projects and create software. The organizational productivity of these businesses was similarly impacted by COVID-19. Employees appear to prefer remote working settings. All businesses concurred that COVID-19 had a significant impact on software product development and project management. As a result, it had an impact on the kinds of technologies used by businesses to create software and facilitate communication between development teams. COVID-19 has an impact on how these businesses plan and manage their projects.Teams were able to operate more productively together in a remote setting thanks to COVID-19. Even while it appears that communication has improved, the majority of employees seem to have stated that the remote work environment makes it harder to forge individualized working relationships. The majority of employees found it difficult to manage their job and personal lives, which was another challenge brought on by the remote work environment. During the COVID era, security also appears to be a problem that becomes more widespread. Even if security concerns increased, it seems that all of the businesses and personnel were well-equipped to manage them. Due to security protocol modifications to address the COVID-19 pandemic, businesses as well as employees may have been better prepared to tackle security risks.It would appear that during the modern era, the Agile (Scrum) methodology is preferred to manage software projects, due to being adaptable and iterative, as well as taking changing project requirements into account. Traditional software development methodologies like Waterfall are bust to fade away, due to its rigid structure and substantial number of formal procedures. It would appear, however, that most large companies prefer to employ a traditional software development methodology or a hybrid selection of methodologies to manage projects. One needs to keep in mind that the survey results are based on only five companies, and therefore does not provide an accurate overview of software management methodologies, since it is an exceedingly small sample of companies within the industry. It is also important to keep in mind, even though Agile (Scrum) is the most preferred software development methodology, at the end of the day it is not the standard and depends on the size of the company and the projects that is being worked on by those companies.BIBLIOGRAPHYBassil, Y. 2012. A simulation model for the waterfall software development life cycle. International Journal of Engineering & Technology (iJET), 2(5),  Date of access: 15 Oct 2022. Chiyangwa, T.B. & Mnkandla, E. 2017. Modelling the critical success factors of agile software development projects in south africa. South African Journal of Information Management, 19(1):1-8.  Date of access: 14 Oct 2022. De Carvalho, B.V. & Mello, C.H.P. 2011. Scrum agile product development method-literature review, analysis and classification. Product: Management and Development, 9(1):39-49.  Date of access: 15 Oct 2022. 10.4322/pmd.2011.005Despa, M.L. 2014. Comparative study on software development methodologies Database Systems Journal, 5(3):37-56.  Date of access: 10 May 2022. Jones, C. 2017. Software methodologies: A quantitative guide. 6000 Broken Sound Parkway NW, Suite 300Boca Raton, FL 33487-2742: CRC Press - Taylor and Francis Group.Mnkandla, E. 2009. About software engineering frameworks and methodologies. In. AFRICON 2009. IEEE. pp. 1-5.Moyo, B., Huisman, M. & Drevin, L. 2022. The state of systems development methodologies use within the systems development organisations in south africa. South African Computer Journal, 34(1):103-123.  Date of access: 15 Oct 2022. Simão, E.M. 2011. Comparison of software development methodologies based on the swebok. University of Minho (Portugal) School of Engineering.  Date of access: 15 Oct 2022.Yarlagadda, R.T. 2021. Devops and its practices. International Journal of Creative Research Thoughts (IJCRT), 9(3):2320-2882.  Date of access: 16 Oct 2022.
Stream data from Arduino to streaming platform to perform data analytics on dashboardL. ANTHONY orcid.org/0000-0000-0000-000XITRI671: Literature Review for the degree Honours: Bachelor of Sciences in Computer Science and Information Systems at the North-West University, Potchefstroom Campus 2022Supervisor:	Prof. R. GoedeCo-supervisor:	N/AGraduation:  31 December 2022Student number: 32969694	Table of contentsList of TablesList of FiguresABSTRACT Streaming databases are data repositories that works with real-time data, with the goal in mind to enrich, process, and store data streams. Data streams are streams or flows of data generated uninterruptedly from a variety of data sources. Without the need to first requiring access to all of the data, data streams are processed consecutively using stream processing methods. The analysis of real-time data is made possible through the use of streaming databases. Data streams are instantly processed, and the results of related registered queries are updated real-time. This is one of the most important capabilities of streaming databases: the analysis of real-time data and measuring how the data evolved over time, unlike a traditional RDBMS. A variety of database categories exists that can manage data streams in real-time, such as NoSQL databases, NewSQL databases, time-series databases, in-memory databases, or in-memory data grids.The main objective of this study is to design and implement a functional IoT system via the use of a streaming database, using Design Science Research principles. The design and implementation of the IoT system should be carefully documented to contribute to a knowledge base on how to efficiently design and implement a practical IoT system via using a streaming database. The generation and analysis of real-time data is a key objective of the study. The study will discuss and analyse the use of efficient tools and platforms to develop a streaming database as well as the transfer/processing of real-time data. The IoT system contains three important components: an Arduino Uno R3 with sensors, used to receive real-time data streams, the Kafka streaming platform (streaming database) and finally the dashboard environment (Power BI) used to visualize the real-time data and to perform data analytics. The research question at hand is how to develop an efficient/effective IoT system using the tree abovementioned components. It is also important to conduct research and discuss how the flow of data is managed from the sensors and pipelining it through the streaming database to the dashboard environment. A thorough study will be conducted on Arduino sensors, the Kafka streaming platform and Dashboard environments. Results from the literature study and design/implementation of the IoT system will be visually represented. The study must contribute as a knowledge base for future generations of developers to easily design and implement smart systems via using a streaming database that eases the handling of real-time data. Keywords: Streaming database, real-time data, Kafka, Tableau Desktop, dashboards, IoT, data analysis, Design Science Research (DSR), Critical Realism (CR), Power BIABSTRAKStroomdatabasisse is databewaarplekke wat met die mees huidige data werk, met die doel in gedagte om datastrome te verryk, te verwerk en te stoor. Datastrome is vloeie van data wat ononderbroke uit 'n verskeidenheid databronne gegenereer kan word. Sonder die behoefte om eers toegang tot al die data te vereis, kan datastrome opeenvolgend verwerk word deur van stroomverwerkingsmetodes gebruik te maak. Die ontleding van intydse data word moontlik gemaak deur die benutting van stroomdatabasisse. Datastrome word onmiddellik verwerk, en die resultate van verwante geregistreerde navrae word intyds opgedateer. Dit is een van die belangrikste eienskappe van stroomdatabasisse: die ontleding van intydse data en die analisering van hoe die data oor tyd ontwikkel het, anders as met 'n tradisionele relasionele databasis. 'n Verskeidenheid databasiskategorieë bestaan ​​wat datastrome intyds kan bestuur, soos NoSQL-databasisse, NewSQL-databasisse, tydreeksdatabasisse, in-geheue-databasisse of in-geheue-dataroosters.Die hoofdoelwit van hierdie studie is om 'n werkende IoT-stelsel te ontwerp en te implementeer deur gebruik te maak van 'n stroomdatabasis en DSR-beginsels. Die ontwerp en implementering van die IoT-stelsel moet noukeurig gedokumenteer word om by te dra tot 'n kennisbasis oor hoe om 'n praktiese IoT-stelsel doeltreffend te ontwerp en te implementeer deur van 'n stroomdatabasis gebruik te maak. Die generering en ontleding van intydse data is ook 'n sleuteldoelwit van die studie. Die studie sal die gebruik van doeltreffende gereedskap en platforms bespreek en ontleed om 'n stroomdatabasis te ontwikkel asook die oordrag/verwerking van intydse data.Die IoT-stelsel bevat drie belangrike komponente: 'n Arduino Uno R3 met sensors, wat gebruik word om intydse datastrome te ontvang, die Kafka-stroomplatform (stroomdatabasis) en laastens die data visualiserings-omgewing (Power BI) wat gebruik word om die intydse data te visualiseer en om data-analise uit te voer. Die navorsingsvraag ter sprake is hoe om 'n doeltreffende/effektiewe IoT-stelsel te ontwikkel deur die boom bogenoemde komponente te gebruik. Dit is ook belangrik om navorsing te doen en te bespreek hoe die vloei van data vanaf die sensors bestuur word en dit deur die stroomdatabasis na die dashboard-omgewing te pyplyn.Sleutelwoorde: Stroomdatabasis, intydse data, Kafka, Tableau Desktop, verslae, IoT, data analise, DSR, CR, Power BIEXTRACTChapter 1: IntroductionIntroduction to ResearchThis study is being conducted to contribute to a knowledge base on how to design and implement an IoT system. The background of the study mentions that few IoT systems gets deployed real-time. The relevant question is how to stream data from sensory input via using an Arduino to the Kafka streaming platform and convert the data into a pivot-table format. How should one then transfer the formatted data to a dashboard (like Tableau Desktop) and perform data analytics? To put the problem another way: what research must be conducted to determine the tools/developer environment that will be a necessity in order to design and implement an effective/efficient IoT System? The study should contribute to the effort of enabling more IoT systems to get deployed in the future, smart and automated systems should eventually become a daily occurrence in the future generation’s everyday lives, such as breathing or walking. This study aims to find a unique perspective on how to implement and design an IoT system, contributing to gaps currently in this research area. This research should enable junior developers and individuals with little to no experience in the IoT or software development field to quickly setup and run an IoT system. The study aims to provide overly simplistic documentation as well as tools and architecture to design and implement an IoT system. As such anyone should be able to build and deploy their very own IoT system with just a few instructions and guidelines. The study should provide documentation on how to resolve errors within an IoT system or how to find a different approach to resolve an obstacle when trying to design and implement an IoT system.Section 1 introduces the study, Section 1.1 provides an overview of the study’s description, Section 1.2 provides a basic background on the study and Section 1.3 lists the goals and objectives of the study. Section 1.4 provides additional objectives of the study, Section 1.5 introduces the Research paradigm applied to this study, Section 1.6 describes the research methodology applied to this study and Sections 1. 7 – 1.10 contain the project management objectives of the study, which includes respectively: the Scope, Limitations, Risk Analysis and finally the Project Plan.  Section 1.11 provides a description of the development platform, tools and environments that will be utilized in this study, Section 1.12 contains a preliminary chapter division, which outlines the structure of this research paper. The chapter concludes with Section 1.13: the executive summary, which provides a summary of this whole chapter. Project DescriptionThe purpose of this study is to obtain knowledge on how to read sensory input from an Arduino R3 Uno as a stream and send it to a streaming platform (Kafka is preferable in this study). To undertake data analysis, the data must be translated into a pivot-table format and displayed on a dashboard (preferably Desktop Tableau) after it has been stored on the platform. Based on the results of the tableau, one must be able to make certain business decisions as a result of the analysis. Thus, the study focuses on the design and implementation of an artefact that can receive sensory input, store data received as streams and display the results in a tableau in order to perform analytics. Experiences and observations should be documented and used as recommendations for designing and implementing an IoT system, as well as providing insights into how to approach difficulties from a different perspective.Sections that will follow is a background of the study, which provides an overview of the existing literature that concerns this study, as well as a description and goals of this study. The problem statement provides a more in-depth overview of the study and what problems this study aims to solve. The paradigmatic perspective mentions the research paradigm applicable to this study and its essence. Methods within the paradigmatic perspective will be discussed that helps to conduct research. Aims and objectives of the study will be discussed: the main goals of the study and tasks that flows from the main goals. The research methodology implemented in the study will be discussed, wherein the artefact and lifecycle of the project is a key aspect. Methods on how to sample data and conduct research will be discussed. The approach to project management and the project plan involves the scope, limitations and risks associated with the study. The project plan will be a Gantt Chart that lists activities/tasks that needs to be completed during the entire lifecycle of the project. A provisional chapter division will be provided that provides the framework for the literature study. Finally, an executive summary will be provided that provides an overall “conclusion” of the study.Project BackgroundIn recent years, the Internet of Things (IoT) concept has been widely adopted within a variety of disciplines, ranging from the industry to smart cities (Calvillo-Arbizu et al., 2021:1). In the health sector, IoT enables new healthcare delivery scenarios as well as the collecting and analysis of real-time health data via sensors so that better judgments can be made on how to solve health issues (Calvillo-Arbizu et al., 2021:1). This sector, however, is complex and involves a number of technological obstacles (Calvillo-Arbizu et al., 2021:1). Despite the abundant literature on the subject, IoT applications in healthcare just scratch the surface of the sector's needs (Calvillo-Arbizu et al., 2021:1). The majority of IoT technologies are multipurpose and must be customized to meet the specific needs of each industry (Calvillo-Arbizu et al., 2021:1). These requirements are frequently overlooked in health IoT-driven solutions, and there happens to be no systematic evaluation of the burgeoning literature from the standpoint of health IoT-driven solutions (Calvillo-Arbizu et al., 2021:1).As indicated by Calvillo-Arbizu et al. (2021:4), extensive research and material does exist on IoT topics, specifically the healthcare sector. A total of 12108 items were found after searching databases (Calvillo-Arbizu et al., 2021:4). After the process of removing duplicates (3 602 papers), the remaining literature were sorted by title (5 934 papers) and abstract (2 376 papers were excluded) (Calvillo-Arbizu et al., 2021:4). The contents of the resulting papers were examined in order to identify literature studies that were beyond the scope of the sector (Calvillo-Arbizu et al., 2021:4). Out of all the resulting articles, 86 were found to be eligible for review (Calvillo-Arbizu et al., 2021:4). Although extensive research on IoT topics exists, literature studies on the design and implementation of IoT and streamed data applications are limited and the complexity of IoT systems restricts potential applications, and rarely current IoT solutions gets deployed real-time (Calvillo-Arbizu et al., 2021:8).Based on the abovementioned literature study, this study examines how to receive sensory input from an Arduino R3 Uno and read the input as a stream to the Kafka streaming platform and identify and demonstrate an area of application. Gaps in the healthcare industry might have to be researched, and the artefact should contribute to fill the gap of that sector. After the data streams are contained on the platform, the data needs to be converted into a pivot-table format and displayed in a tableau (dashboard) in order to perform data analysis. From the analysis one has to be able to make some business decisions based on the results from the tableau. Design Science will be applied to this study, which means errors will be documented as well as the progress made in order to provide a meaningful perspective of the knowledge gained by designing and building the artefact. The overall objective of this study is to design and develop an artefact that implements a streaming database/platform via sensory input, to provide meaningful information from the data received. It is rather apparent that a different approach will have to be followed and different literature studies might have to be examined to design and implement an IoT data streaming and tableau analysis-based application in order to build a knowledge base on how to successfully receive sensory input from an Arduino R3 Uno and read the input as a stream to the Kafka streaming platform. Goals and Objectives: AimsTo build a knowledge base on how to design and implement a solution to receive sensory input from an Arduino R3 Uno and read the input as a stream to the Kafka streaming platform. The knowledge base must serve as learning material for future generations on how to build a functional IoT systemObjectives of StudyTo build an Arduino Uno R3 that are able to receive sensory input (via temperature/moisture or motion scanners for example) To conduct a literature study and draft a thesis that substantiates the research (design and implementation of IoT system)To identify and demonstrate an area of application for the IoT SystemTo document findings and progress on designing and implementing the IoT SystemTo document errors and obstacles while trying to design and implement the IoT SystemResearch Paradigm: Critical Realism (CR)Ontology describes the nature of objects being studied, nature and qualities of a variety of entities that exist, as well as whether reality exists objectively or subjectively with regards to people observing the reality (Wynn & Williams, 2008:4). Critical realism is based on three essential beliefs: objective reality, stratified ontology, and an open systems perspective (Wynn & Williams, 2008:4). Epistemology focuses on acceptable truths/beliefs via specifying criteria and defining the process of how to perform assessment on assertions compiled by researchers (Wynn & Williams, 2008:7). The way knowledge claims are presented, how they are evaluated for truth or validity, and how they are weighed against existing knowledge are all determined by one's epistemological beliefs (Wynn & Williams, 2008:7). CR aims to explain reality as objective through a researcher’s observations/interpretations/examinations (Wynn & Williams, 2008:7). Knowledge assertions created by researchers are based on identifying components of reality that must exist in order for certain phenomena within a system to occur (Wynn & Williams, 2008:7). CR aims to explain rather than to predict or comprehend complex phenomena (Wynn & Williams, 2008:7). Both observability and unobservability within systems plays an important role in CR research (Wynn & Williams, 2008:7). The table below gives a summary of the methodologies typically implemented in a CR study:Table 11: Summary of Methodological Principles for Conducting CR-based Case Study Research Source: (Wynn & Williams, 2008:10)Research Methodology: Design Science Research (DSR)According to Peffers et al. (2007), Design Science is a methodology or paradigm which can be used to develop and assess IT artefacts, the artefacts themselves aimed at resolving identified organizational issues. DSR entails a thorough process of designing artefacts to solve observable problems, as well as contributing to research, evaluating the planned designs, and communicating the results from observance and experiences to relevant audiences (Peffers et al., 2007:49). Typical artefacts produced using the DSR methodology includes instantiations, methods, models and constructs, to name a few (Peffers et al., 2007:49). To sum up: any product created with an incorporated solution in mind to an extensively researched topic is included in this description (Peffers et al., 2007:49). The methodology that will be used to conduct this study is Design Science Research that falls under the Critical Realism paradigm. The goal of this research is to learn how to design and implement a real-time IoT system. Another important aspect of this study is to contribute to a knowledge base on how to design and implement an effective IoT system that can be deployed real-time. Experiences and observations should be documented, and they should serve as guidelines on how to successfully design and setup an IoT system, while also providing insights on how to tackle problems by looking at a different angle. The following figure demonstrates the DSR framework:Figure 1.1:  Information Systems Research Framework (Hevner et al., 2004:80)To conclude on this section: the study will be more empirical by nature and will contribute to a knowledge base on how to design and implement an IoT system, while documenting the experiences in designing and implementing the solution as well as to document obstacles or errors that occurred during the design/implementation of the artefact. The documentation on the obstacles/errors should be able to guide the user to quickly setup their own IoT system, and when the user does have a problem to implement the system according to guidelines, they should be able to get an alternative perspective on solving a problem based on the documentation. Project Management: ScopeTo identify and demonstrate a use case for an Internet of Things real-time system.Conduct a literature review and produce a thesis that backs up findings (design and implementation of IoT system)To develop a knowledge foundation on how to design and implement a solution that receives sensory input from an Arduino R3 Uno and reads it as a stream into the Kafka streaming platform. Future generations must be able to learn how to design a functional IoT system from the knowledge base.To build an Arduino Uno R3 that can accept sensory information (for example, via temperature/moisture sensors or motion scanners) To identify the right sensors for the identified use-caseTo create the best circuitry for the Arduino.To gain a better understanding of the Kafka streaming platform (features and implementations)To send data from sensors to the Kafka streaming platform.Using Tableau Desktop, convert data stored on Kafka into a pivot-table format and move the data to a dashboard.To explore the Tableau Desktop environment and techniques for data visualisation and analyticsTo perform analytics on the data on the dashboard via data visualisation techniques. Based on the results of the analysis, one should be able to make an informed business decisionTo document findings and progress on designing and implementing the IoT SystemTo document errors and obstacles while trying to design and implement the IoT SystemTo write a program/script that generates enough data that can be streamed and provides meaning when analytics is performedProject Management: LimitationsAccording to Watt (2014:14), limitations and constraints of projects typically include the following:CostA budget that has been approved, which covers all the necessary completion costs (Watt, 2014:14). Because many initiatives receive money or subsidies with rigorous contract terms,, project managers must strike a balance between not running out of funds and not underspending inside companies (Watt, 2014:14). Budget preparations that aren't well-executed can lead to a number 99 scramble to spend the funds available (Watt, 2014:14). Cost is ultimately a limiting constraint for almost all projects: few projects can go beyond budget without requiring a remedial step (Watt, 2014:14).ScopeWhat functionality the project should provide and the end goal of the project (Watt, 2014:14). Project outcomes are explicitly defined as well as the processes that go into creating them (Watt, 2014:14). It is the project's raison d'être and goal (Watt, 2014:14). Scope limitations can cause a project to fail, and scope creeping cause extra strain on the project’s time frame.Quality Combination of the performance criteria and stipulated standards that must be met by the project's products in order to provide the intended functionality (Watt, 2014:14). The product must deliver on its promises, solve a known problem, and give the expected benefit and value. (Watt, 2014:14). Other performance standards must be met, including maintainability, dependability and availability as well as a good conclusive result (Watt, 2014:14). The practice of evaluating overall project performance on a regular basis to verify that the project satisfies the essential quality requirements is known as quality assurance (QA). (Watt, 2014:14).RiskExternal events, in most cases, will have a negative impact on your project (Watt, 2014:14). Risk is defined as how likely an event will occur as well as the foreseen impact on the project if the risk does occur (Watt, 2014:14). If a risk happens to be a foreseen possibility, and the consequences of the risk are  extremely disastrous , one should classify the event as a risk and implement a proactive risk management plan (Watt, 2014:14).ResourcesAny assets that are needed to complete the project's tasks (Watt, 2014:14). Usually resources consist of funds, facilities, equipment, people or any other type of asset (usually labour is not included) that is necessary to view a project activity as being successfully completed (Watt, 2014:14). One can speculate how a project can be negatively affected if inadequate resources are assigned to a project.TimeThe time frame needed to complete a project (Watt, 2014:14). One of the most common project oversights is time (Watt, 2014:14). Missed deadlines and incomplete deliverables are evidence of this (Watt, 2014:14). The proper control of the schedule necessitates the precise identification of tasks to be completed, as well as accurate estimates of their durations, sequence, and allocation of personnel and other resources (Watt, 2014:14). Vacations and holidays should be considered while planning a project to include slack times (Watt, 2014:14).Project Management: Risk AnalysisAccording to (Pries-Heje et al., 2014:63), there are six classes used to identify risks that can occur within project management when using a DSR methodology, and each of the classes contains a number of risks that should be considered:Business NeedsThis class includes the identification, selection, and development understanding the business needs which includes problems and requirements, This also involves problem analysis and assessing alternatives (Pries-Heje et al., 2014:63). The following potential risks have been identified as a result of this activity's analysis:Problems that have no meaning to the stakeholder (Pries-Heje et al., 2014:64).Struggling to grasp the problem and the context it occurs within (Pries-Heje et al., 2014:64).Incompatible stakeholder interests (some of which may or may not come to light) (Pries-Heje et al., 2014:64).GroundingSearching, recognizing, and understanding relevant information (recovered from the body of human knowledge that has been recorded) (Pries-Heje et al., 2014:63). The following potential risks have been identified as a result of this activity's analysis:Incomprehension and/or failing to understand relevant existing research. Failing to comprehend the situation and excessive dependence on personal experience or imagination (Pries-Heje et al., 2014:64).Incomprehension and/or failing to understand existing design science research used to study problem-solving technology, i.e., a lack of understanding how technology works (Pries-Heje et al., 2014:64).Incomprehension and/or failing to understand important natural and behavioural science research that forms the base theories about understanding the essence of the problem and how to solve the problem (Pries-Heje et al., 2014:65).Building and designing artefactsConstruct design theories, including instantiations (hypothetical solutions to solve business needs or problems, theories about the solutions) (Pries-Heje et al., 2014:63). (Pries-Heje et al., 2014:63). The following potential risks have been identified as a result of this activity's analysis:Development of an uninstantiated (conjectural) solution that cannot be easily incorporated (being able to actually implement the solution) (Pries-Heje et al., 2014:65).Development of a hypothetical solution that cannot solve the problem, i.e., the artefact cannot be implemented in real life and has a lot of socio-technical problems (Pries-Heje et al., 2014:65).Developing a hypothetical solution that fails to solve the problem, i.e., requires excessive resource expenditures (Pries-Heje et al., 2014:65).Assess design artefacts and provide evidence for design theories or expertise.The following potential risks have been identified as a result of this activity's analysis:Tacit requirements (which cannot be disclosed by definition) are not addressed while evaluating solution technology, resulting in the solution technology failing to meet those requirements (Pries-Heje et al., 2014:66).While any or all of the relevant needs are not surfaced, they are not addressed when evaluating the solution technology, resulting in the technology failing to meet those specifications (Pries-Heje et al., 2014:66).Misalignment of specifications to Information Systems Design Theory (ISDT) meta-requirements results in the IDST being tested and an embodiment of the meta-design being evaluated in a circumstance where none of them should be used (Pries-Heje et al., 2014:66).Artefact dissemination and useDisseminate new artefacts, design theories, and knowledge to individuals and organizations for practical applications in order to meet business needs (Pries-Heje et al., 2014:64). The following potential risks have been identified as a result of this activity's analysis:A solution's implementation in practice does not work at all (Pries-Heje et al., 2014:67).Misunderstanding the solution's suitable context and constraints (Pries-Heje et al., 2014:67).Misunderstanding how to implement the solution (Pries-Heje et al., 2014:67).Knowledge additionsPublish new design artefacts and theories, and design knowledge that has practicality in order to tackle or improve business needs and problems (Pries-Heje et al., 2014:64). The following potential risks have been identified as a result of this activity's analysis:Impossibility of publishing or presenting research findings (Pries-Heje et al., 2014:67).Low-importance research is published (Pries-Heje et al., 2014:67).Publication of erroneous study findings (Pries-Heje et al., 2014:67).Project PlanFigure 1.1.2: Project PlanDescription of development platform, tools, and environments to be usedIn order to develop an IoT system that can stream real-time data, an Arduino Uno R3 microcontroller will be needed as well as various type of Arduino sensors (such as temperature, PIR motion or humidity sensors) to evaluate its data streaming capabilities. A type of streaming database is necessary in order to process and manage the real-time streaming data, in this study the Apache Kafka streaming platform will be utilized. Other tools might be needed to pipeline the streaming data from the Arduino microcontroller to Kafka. Tableau Desktop will be used as the dashboard environment to analyse and visualize data. Other tools and frameworks might be necessary to pipeline the data from Kafka to Tableau Desktop and will be researched at a later stage in this study. It is important to note that the tools and environment are subjects to change as the study progresses.Preliminary Chapter DivisionChapter 1: IntroductionOverall introduction to the literature study, which also serves as the research proposal. This chapter introduces research methodologies, research paradigms as well as a description of the artefact and project plan, background and relevant works.Chapter 2: Literature StudyThe background of the project will be examined more attentively here, a more accurate explanation of the artefact and how it will be developed.Chapter 3: Development of Project and Documentation surrounding the Artefact The chapter contains the following sub-sections:Description of ProjectThe Life Cycle and the different Phases of the ProjectDescription of the Development of the ProjectChapter 4: RecommendationsBased on the development of the project and documentation surrounding the artefact, recommendations will be made on how to finalize the project and answer the research question.Chapter 5: ReflectionKeep the study's findings in a separate document. The research question will be addressed through the goals and objectives established in this chapter. Based on the findings, recommendations will be made.Chapter 6: Conclusion and final articleThe final document will be addressed and finalised here. A comprehensive document can be created by combining all of the aforementioned chapters.Executive SummaryThe purpose of this study is to discover how to read sensory information from an Arduino R3 Uno as a stream and send it to a streaming platform (as mentioned in the Introduction, the Kafka streaming platform is preferable). The data must be translated into a pivot-table format and displayed on a dashboard after it has been stored on the platform (ideally Desktop Tableau). Based on the tableau's conclusions, one should be able to make specific business decisions based on the analysis' results. The study focuses on the development and implementation of an artefact that can receive sensory input, store data as streams, and show the results in a tableau for analytics. Another important aspect of this study is that it should contribute to a body of knowledge on how to develop and implement a real-time IoT system.Experiences and observations should be documented and used as recommendations for designing and implementing an IoT system, as well as providing insights into how to approach difficulties from a different perspective. This study falls under the Critical Realism paradigm, and the study will also focus on the events occurring within the IoT system (artefact). Documenting events, experiences and observations is an important part of this study. This study is objective by nature, even though there are events and causes which cannot easily be explained by humans. The methodology that will be implemented to conduct this study is Design Science Research, which is empirical by nature. Field studies, experiments and simulations will have to be conducted in order to design and implement a real-time IoT system.Chapter 2: LITERATURE REVIEW - RESEARCH PARADIGM AND METHODOLOGYConducting the Literature StudyIntroductionThis chapter introduces the literature review. Thorough research is being conducted on the research methodology and research paradigm applicable to this study. Each section is listed as a subheading in this chapter, and the main points of discussion of each section will be listed as further subheadings. Apart from the research methodology (DSR) and research paradigm (CR), the main points of discussion of each section will be constructed as follows: an introduction to the section, followed by the applications of each section, and finally how Design Science Research will be implemented in each section. Subheadings relevant to each section will be listed in between the main points of discussion. The objective of this chapter is to introduce standard practices, planning and design principles that should be considered when designing and developing the artefact. Section 2 introduces the conduction of the literature review, Section 2.1 provides a quick overview of this chapter and Section 2.2 focuses on the research paradigm, under which Section 2.2.1 falls: Critical Realism (CR), the essence of the research paradigm and the ontological, epistemological and methodological nature thereof. Section 2.3 introduces the research methodology applied to this study, under which Section 2.3.1 falls: the essence and nature of Design Science Research (DSR). Section 2.3.2 discusses the implementation of DSR regarding this study.Research ParadigmCritical Realism (CR) is the research paradigm this section focuses on. The focus of Section 2.2.1 emphasizes the fundamentals and characteristics of CR as a research paradigm, as well as the reasons why it is appropriate for this study. The ontological, epistemological and methodological nature of this paradigm is portrayed in this section. A thorough discussion is provided, allowing one to gain a deeper insight on the essence of Critical Realism.Critical Realism (CR)The paradigms of positivism and interpretivism are being challenged by critical realism (CR), which is a rival paradigm to the aforementioned (Wynn & Williams, 2008:2). CR-based research methodologies can be particularly useful for conducting research to examine complex organizational occurrences in a comprehensive way in order to establish deep contextual and causal explanations that account for the wide range of environmental and organizational elements that contribute to their occurrence (Wynn & Williams, 2008:2). CR-based research responds to current calls for systems-oriented MIS theories, which can be used to discover the processes that link a series of unpredictably occurring events and complicated interactions (Wynn & Williams, 2008:2). Theorists and researchers can use Critical Realism to develop more precise explanations for a group of occurrences or events. (Wynn & Williams, 2008:2). It is not necessary to use procedures that are more appropriate for the natural sciences, thus CR can be seen as the preferred paradigm for approaching complex phenomena, such as occurrences often observed in information systems (Wynn & Williams, 2008:2). CR allows for the expansion and improvement of existing research methods. (Wynn & Williams, 2008:2). Regarding social scientific research, critical realism became a powerful alternative paradigm to conduct research, opposing positivism and interpretivism (Wynn & Williams, 2008:3).       Contemporary CR is ontologically based and aims to answer what reality should be like for science to become achievable (Wynn & Williams, 2008:3).  The last-mentioned is based mostly on Roy Bhaskar's ideas (Wynn & Williams, 2008:3).  The core assumption of Critical Realism is that hypotheses developed by a scientific study must centre around the objective fact that makes up the world, even when humans are sometimes unable to completely comprehend or experience this reality, and our existing knowledge regarding objective reality is far from perfect (Wynn & Williams, 2008:3). Consequently, CR has been dubbed as "ontologically bold but epistemologically cautious” by W. Outhwaite (Wynn & Williams, 2008:3).  Hence, CR research focuses on determining what reality should be so as to explain the happening of a specific set of events (Wynn & Williams, 2008:3). Critical realism claims that the world is not falsifiable to the conditions through which humans get access to it (Wynn & Williams, 2008:4). To rephrase: humans only see a small part of the objective world, and nature of the objective world is difficult to grasp, characterize, and measure (Wynn & Williams, 2008:4). The ideas that emerge through scientific practice are part of a transitive dimension of science that includes knowledge generated by humans with the aim of explaining and understanding the objective nature of the universe, which is an intransitive dimension in and of itself (Wynn & Williams, 2008:4). These theories can't be directly compared to the real world; instead, humans can only perform comparisons to see how well the explanation of the observable occurrences under investigation happens to be (Wynn & Williams, 2008:4). The contrast between experiences, events, and structures distinguishes Critical Realism from empiricist theories like positivism (Wynn & Williams, 2008:5). Critical Realism takes into account that phenomena occurring within an environment can be quantified, such as experiences, and they are likely to only make up a small portion of the actual events that occur inside a given social system (Wynn & Williams, 2008:6). As a result, ontologically, events that occur within a given structure are unrelated to the experiences that we can objectively observe and measure (Wynn & Williams, 2008:6). Critical Realism sees reality as an open system that humans cannot directly influence  (Wynn & Williams, 2008:6).Critical realism aims to explain objective reality through participants' observations and interpretations, as well as the examination of additional objective data (Wynn & Williams, 2008:7). As a result, knowledge claims generated by Critical Realism researchers are centred on identifying components of reality, specifically mechanisms and structures, that must exist in order for the events/experiences that are being investigated to take place (Wynn & Williams, 2008:7). These knowledge claims are predicated on a number of CR-specific epistemological assumptions: explanation rather than prediction or comprehension, socially mediated knowledge and explanation by mechanisms, being unable to observe mechanisms, as well as various plausible mechanisms (Wynn & Williams, 2008:7). The belief in the presence of a mechanism that is warranted is typically hampered with in Critical Realism by the reality that these processes are rarely observable or measurable (Wynn & Williams, 2008:8). The impacts manifested in the subsequent events and experiences are sometimes the only way to identify these processes and the structural components from which they are created  (Wynn & Williams, 2008:8). Actual occurrences our experiences of them, as well as the underlying mechanisms and structures that interact to create them, are separate, which is consistent with stratified ontology principles. (Wynn & Williams, 2008:8). Consequently, the observant’s knowledge assertions may be based on both observable and unobservable elements, such as structures and mechanisms (Wynn & Williams, 2008:8). Following the discussion of the essence of Critical realism, the following figure illustrates this paradigm’s methodologies/approaches:Figure 2.1: Key Relationships between Ontological, Epistemological and Methodological Principles (Wynn & Williams, 2008:9)The table below gives a summary of the methodologies typically implemented in a CR study:Table 21: Summary of Methodological Principles for Conducting CR-based Case Study Research Source: (Wynn & Williams, 2008:10)The figure below indicates research objects along with their research modes and purpose in a Critical Realism study:Figure 2.2: A taxonomy of information systems research approaches (Dobson, 2001:203)One can speculate from the discussion of the essence of Critical Realism, as well as the figures above, Critical Realism would be the suited paradigm for this study. CR is perfect for researching IS areas where complex events occur within an open system, and only explanations and descriptions are necessary to provide for the current events occurring. Understanding how and why these events occur is insignificant within a Critical Realism paradigm. CR is also perfect for contributing to current knowledge and expanding on a subject. There is a clear distinction between experiences, events, and structures within a CR paradigm, and those elements should be objectively examined while considering that there are factors that those elements cannot influence. Critical Realism is suited for this study, since a researcher can observe and interpret events occurring within a system, while keeping objectivity in mind and considering that the truth/reality are relative. The main aim of CR is to explain why certain events in a system occur, rather than a prediction or comprehension of why these events occur in a system. Critical Realism also takes into consideration that there are events within a system that can be unobservable. Experiences and observations are an important aspect of Critical Realism, as a result, it is critical to keep track of your experiences/observations in order to get a more objective/partial picture of reality. Another important part regarding the paradigm is the methodologies that will be followed to gain knowledge and research a subject area. An empirical approach will be followed to gain knowledge and conduct the literature study via experiments and testing. By examining  the applicable ontology will most likely be an Objective Reality or Open Systems Perspective. The applicable epistemology will likely be explanations rather than prediction or explanations via mechanisms. The most suited methodology for this study will be an empirical corroboration approach. When examining , one can speculate that the appropriate modes to investigate in this literature study will be laboratory experiments, field experiments and possibly surveys. Due to the nature of the study, using surveys to collect data might not be the best possible approach, due to the fact that the research is based on designing and implementing an IoT system. Surveys provide more quantitative approach to sample data based on a hypothesis or answering a theoretical research question. To conclude:  Critical Realism is the most effective research paradigm to use to conduct the literature study, since one of the main objectives of the study is to build a knowledge base on how to design and implement an IoT system. Experiences should be documented thoroughly and events occurring should be analysed without understanding the deeper cause of all events occurring in the research environment. Research MethodologyThe study design revolves around Design Science Research (DSR), as well as experimentations and documenting experiences. When designing objectives using this methodology, the available knowledge base is usually insufficient, and designers must rely on intuition, experiences, as well as trial-and-error methods (Hevner et al., 2004:99). The artefact produced by the study is an experiment, and usually one makes use of new and upcoming technological methods/tools to design and build the artefact and finally being able to answer a research question (Hevner et al., 2004:99). Using DSR, researchers can explore the nature of the problem, the environment(s) it occurs in, and formulate solutions as a result of the artefact’s execution, which emphasizes the need of creating and implementing prototype artefacts (Hevner et al., 2004:99). Within the Information Systems context, it so happens that the design-science research paradigm is technologically proactive (Hevner et al., 2004:98). It focuses on developing and analysing cutting-edge IT artefacts that help organisations tackle critical information driven activities (Hevner et al., 2004:98). The behavioural science research paradigm is reactive to technology (Hevner et al., 2004:98). It focuses on the development and justification of theories that tries to explain and predict strange occurrences connected to the possession, deployment, management, and application of such technologies (Hevner et al., 2004:98). Problems within Design Science Research includes an exaggeration of technology artefacts and failing to maintain a sufficient conceptual framework, which could result in a artefacts that is well designed,  but happens to be ineffective in real-world organizational settings (Hevner et al., 2004:98). This study aims to provide an artefact that provides value to society and can be deployed real-time, therefore eliminating abovementioned problems while contributing to a knowledge base on how to design and implement an effective IoT system. The figure below describes the framework to conduct DSR for a given subject area:Figure 2.3:  Information Systems Research Framework (Hevner et al., 2004:80)One can conclude from this figure that experiments, field studies and simulations will have to be conducted in order to design and implement a real-time IoT system. The research methodology should be able to provide vital information and resources to the knowledge base's foundations, such as theories, frameworks, constructs and methods. The figure below describes evaluation methods in order to conduct research (and most of them will be implemented in this study):Figure 2.4: Design Evaluation Methods (Hevner et al., 2004:86)To conclude: the study will be more empirical by nature and will contribute to a knowledge base on how to design and implement an IoT system, while documenting the experiences in designing and implementing the solution as well as to document obstacles or errors that occurred during the design/implementation of the artefact. The documentation on the obstacles/errors should be able to guide the user to quickly setup their own IoT system, and when the user does have a problem to implement the system according to guidelines, they should be able to get an alternative perspective on solving a problem based on the documentation. Design Science Research (DSR)According to Peffers et al. (2007), Design Science is a methodology or paradigm which can be used to develop and assess IT artefacts, the artefacts themselves aimed at resolving identified organizational issues. DSR entails a thorough process of designing artefacts to solve observable problems, as well as contributing to research, evaluating the planned designs, and communicating the results from observance and experiences to relevant audiences (Peffers et al., 2007:49). Typical artefacts produced using the DSR methodology includes instantiations, methods, models and constructs, to name a few (Peffers et al., 2007:49). Social innovations or unique properties of technological, social, or informational resources may also be included. (Peffers et al., 2007:49). To sum up: any product created with an incorporated solution in mind to an extensively researched topic is included in this description (Peffers et al., 2007:49). The methodology that will be used to conduct this study is Design Science Research that falls under the Critical Realism paradigm. Before diving into the lifecycle of the project using DSR, a quick recap is needed on what the study is about. The goal of this research is to learn how to read sensory input as a stream from an Arduino R3 Uno and transfer it to a streaming platform (as mentioned in the Introduction, the Kafka streaming platform is preferable). After the data has been stored on the platform, it must be converted into a pivot-table format and shown on a dashboard (ideally Desktop Tableau). From the result based on the analysis, one should be able to make specific business decisions based on the tableau's conclusions. The research focuses on the design and implementation of an artefact that can collect sensory input, store data as streams, and display the findings in a tableau so that analytics may be performed. Another important aspect of this study is to contribute to a knowledge base on how to design and implement an effective IoT system that can be deployed real-time. Experiences and observations should be documented, and they should serve as guidelines on how to successfully design and setup an IoT system, while also providing insights on how to tackle problems by looking at a different angle. The following diagram illustrates the lifecycle of a project using Design Science Research (the steps that will be used to design and implement this project): Figure 2.5: Design Science Research Methodology Process Model (Peffers et al., 2007:54)A further elaboration on each step in the above process model will follow:Identifying the problem and motivationDefine the research question in detail and justify the importance of a solution (Peffers et al., 2007:54). The research question will be used to create an artefact that can provide a functional solution, breaking down the problem piece by piece can allow the solution to capture the complexity of the problem, and proves to be beneficial in the long run (Peffers et al., 2007:54). Being able to motivate why a solution can be used to solve a problem contributes to both the researcher and intended audience to pursue the answer and to accept results flowing out of the solution, while also visualising the researcher’s reasoning regarding how to solve the research question (Peffers et al., 2007:55). Emphasis is placed on understanding the problem and how to resolve it (Peffers et al., 2007:55).Defining the objectives of a solutionFrom the problem definition and knowledge of what is achievable, deduce the goals of a solution (Peffers et al., 2007:55). Objectives might be quantitative, such as the terms in which a desirable solution would be preferable to current ones, or qualitative, such as a description of how a new artefact is meant to provide answers to previously unsolved problems (Peffers et al., 2007:55). The objectives should be logically deduced from the problem description (Peffers et al., 2007:55). Observance of the current state of problems, as well as existing solutions (if they exist) and their effectiveness, are essential to define the objectives of a solution (Peffers et al., 2007:55). Design and development of the artefactDuring this step the researcher starts to build the artefact (Peffers et al., 2007:55). Artefacts include constructs, models, procedures, or instantiations (all of them defined rather broadly), and they novel qualities of social, technological, or any other informational resources (Peffers et al., 2007:55). Any constructed object that incorporates a research input into the design is referred to as a design research artefact (Peffers et al., 2007:55). This activity entails identifying the desired functionality or purpose and the architecture of the artefact (Peffers et al., 2007:55). Finally, the researcher has to build the artefact (Peffers et al., 2007:55). One of the resources required for changing from aims to design and development is knowledge of theory that may be used to design and implement a solution (Peffers et al., 2007:55).Demonstration of the artefactThe researcher has to demonstrate that an artefact works or contributes to solving a problem (Peffers et al., 2007:55). The researcher might have to simulate several instances of a problem to verify that the artefact actually does its job (Peffers et al., 2007:55). Methods to test whether an artefact’s functionality follows user requirements could be done via experimenting with it, running simulations, performing case studies, documenting experiences and observations, or any other activity that is relevant (Peffers et al., 2007:55). Effective understanding of how to use the artefact to address the problem is one of the most important principles required for the demonstration (Peffers et al., 2007:55). Evaluation of the artefactExamine and assess how efficient the artefact supports a solution that contributes to solving a problem (Peffers et al., 2007:56). This exercise entails comparing a solution's objectives to actual results observed from the demonstrations of the artefact (Peffers et al., 2007:56). It necessitates an understanding of key measurements and analysis methods (Peffers et al., 2007:56). Depending on the nature of the problem and the type of artefact, evaluation can take on many various forms. (Peffers et al., 2007:56). It may include a comparison between the functioning of the artefact or the goals of the solution from  in the process model, objective quantitative performance measurements like budgets or items produced, survey results based on customer/participant satisfaction, client feedback, or simulations, among others (Peffers et al., 2007:56). It could include measurable system performance indicators like response times or availability (Peffers et al., 2007:56). In theory, such an assessment may incorporate any relevant logical proof or empirical evidence (Peffers et al., 2007:56).Communicating the results to peers or intended audienceIf necessary, one should make researchers and other target audiences, such as practicing professionals, aware about the problem and its significance, the artefact itself, its value and distinctiveness, the thoroughness of its design, and its effectiveness (Peffers et al., 2007:56). The formal structure of an empirical research process (identifying and describing a problem, literature review, the development of a hypothesis, data sampling, analysis, observations and results, debates or discussions, and finally the conclusion) is also a required framework for empirical research papers, and researchers may use the framework of this process to structure the paper in scholarly research publications (Peffers et al., 2007:56). Communication necessitates familiarity with the presumptions, point of views, approaches, methodological analysis, and core beliefs members of an academic disciplinary community have (Peffers et al., 2007:56).DSR implementation of artefactFollowing the Design Science Research approach/processes as indicated in  on page 20, the design and development approach of the IoT system and streaming database (artefact) will be conducted as follow:Identify the problem and provide a motivationThe research problem is how to develop an IoT system by using a streaming database. How would one go about to stream data from an Arduino sensor to the Kafka streaming platform, and pipeline the converted data from Kafka to a dashboard environment (Tableau Desktop)? How would one utilize the Tableau Desktop environment to perform data analytics and present the data to make an informed business decision? The motivation for the research problem is to contribute to a new/existing knowledge base that will allow future generations of developers to design and implement IoT systems via using a streaming database. The knowledge base and documentation provided should allow individuals with the minimum IoT and streaming databases knowledge to set up and deploy smart systems at the fastest possible rates, while also being able to provide high quality and effective IoT systems. A proper use case for the IoT system will be developed at a later stage, as the research is more of a generic nature at this stage.Sensors The research question pertaining to this section is what type of Arduino sensor will be suitable to stream data to the Kafka streaming platform (given a specific use case that will be determined at a later stage). Another important aspect is to identify optimal circuitry for the Arduino microcontroller in order to efficiently stream data to the Kafka streaming platform. The motivation behind the research question is to develop an IoT system that can stream data and perform analytics in order to provide feasible results to make an informed business decision, provided a specific use case. It is also important to select a sensor that is applicable to the use case and to study the functionalities and properties of said sensor.KafkaThe research problem pertaining to this section is how to stream sensory input from the Arduino microcontroller to the Kafka streaming platform. The motivation behind the research problem is to design a IoT system by means of using an Arduino microcontroller with a selected sensor that is suitable to a specific use case, the IoT system should be able to stream data from the Arduino microcontroller to the Kafka streaming platform. It is also necessary to identify additional tools and platforms that can streamline the streaming process from a sensor to Kafka. Tableau Desktop/DashboardsThe research problem in this case is how to pipeline data from the Kafka streaming platform to Tableau Desktop. What data visualization techniques will be suitable for a given IoT sensor use case? How will the developer communicate the results from analysis and data visualization to the relevant stakeholders? What informative decision can be made based on the results from the Dashboard? Is Tableau Desktop the most efficient/effective Dashboard software to use? What are the key performance indicators or key performance risks for the given use case? The motivation behind the research problem is to create a knowledge base, with regards to creating and implementing a solution to read sensory input as a stream from an Arduino Uno R3 to the Kafka streaming platform, transferring the converted data to Tableau Desktop in order to perform analytics in the dashboard environment and make an informative business decision based on the results from the dashboards. Future generations must be able to learn how to construct a successful Internet of Things system via following guidelines from the knowledge base.Define the objectives of the solutionIn order to design and develop the artefact, efficient and effective tools and development platforms will be necessary to ensure a simplistic yet intuitive IoT system design. One of the main objectives is to find suitable tools and platforms that allows streamlined development of a smart system that utilizes streaming databases. The design and development of the IoT system must be documented as the development process moves along, and any faults/errors detected in the IoT system must be carefully documented to prevent future developers from making the same mistakes.Sensors The IoT system (specifically the Arduino microcontroller) should have a sensor that can stream real-time data to the Kafka streaming platform. Another possible objective is to include a sensor, such as light-emitting diodes, to provide noticeable output to indicate that the primary sensor is busy streaming real-time data. The Arduino microcontroller should also have efficient circuitry to prevent short-circuits in the system and to provide the sensors with optimal power.KafkaThe objective of the solution is to design/develop the Arduino microcontroller to stream data to the Kafka streaming platform. Additional tools and platforms should be identified that can assist in the design and implementation process of the IoT system.Tableau Desktop/DashboardsTransfer data from the Kafka streaming platform to Tableau Desktop or another dashboard environment.Identify the data visualization techniques that will be the most suited to present the results from analysing the real-time data to stakeholders. Identify the KPIs or KPRs for a given real-time data streaming IoT system.Set benchmarks for dashboard results (wat the developer/stakeholders want to know from a dashboard).Plan a presentation on how to communicate the final results to stakeholders.One should be able to make an informed business decision based on the results from the dashboards.Provide a knowledge base on how to design and implement a functional real-time data streaming IoT system.Design and develop the artefactDuring this stage, the artefact will be physically designed and implemented. Research must be conducted on following standard design principles of streaming databases. A plan must be developed to oversee data flows from the Arduino sensors to the Kafka streaming platform and to the Tableau Desktop environment. Pipelining of the data should be carefully planned. The artefact must be developed, connecting sensors to the Arduino, transferring real-time data from the sensors to the Kafka platform, converting data streams on the Kafka platform and transferring it to Tableau Desktop. Data visualisation techniques and data analysis strategies must be employed to conclude a valid business decision. Sensors During this stage, the design and development of the Arduino microcontroller starts. The relevant sensors, provided a specific use case, should be connected to the microcontroller. One should ensure that enough power supply terminals and digital I/O pins are available in order to connect additional sensors, should it become necessary to. Optimal circuitry should be designed to avoid short-circuits and for ideal power supply to sensors. The process of connecting the sensors and designing the circuitry should be documented. Any errors or faults detected in connecting the sensors or designing the circuitry should be documented as well to contribute to a knowledge base and preventing future developers from making the same mistakes when designing and developing a real-time streaming IoT system.KafkaDuring this stage, one should already have a functional Arduino microcontroller that can receive sensory inputs. This is the stage where one needs to setup and install the Kafka streaming platform and connect the Arduino microcontroller to the streaming platform in order to pipeline data. Data pipelining should be carefully planned via creating data flow diagrams. It would be an excellent idea to design a diagram that indicates the streaming database architecture. As the design and development of the artefact goes along, one needs to document each step of the processes. Any faults or errors that is detected in the data pipelining system or Kafka architecture must be properly documented. Experimentations, such as prototyping, with a variety of different tools and platforms to stream data from sensors to the Kafka platform might be necessary to determine which of the tools/platforms are the most efficient and simple to stream real-time data. One should create a script that generates high-velocity and high-volume data to determine how effectively the Kafka streaming architecture manages real-time data.Tableau Desktop/DashboardsDuring this stage, the design and development of the Dashboard section begins. One should pipeline the data from the Kafka streaming platform to the Tableau Desktop environment. Data analysis should be performed in the dashboard environment and data visualization techniques (such as area or bar charts, scatter plots, heat maps etc.) should be employed to display the results from the data analysis. Key performance indicators should be properly defined during this stage in order to ensure that the stakeholders receive relevant information from the dashboards. One should write a script that generates sufficient real-time data in order to make correct inferences from results on the dashboard. It might be necessary to design the dashboards with alternative dashboard software, such as Power BI or IBM Cognos, as Tableau Desktop is licensed software and only provides a two-week free trial period. The design and development process should be documented as time goes by. Any errors detected in the dashboard environment should pe properly documented as well. A presentation should be designed to communicate results to stakeholders.Demonstrate the artefactAfter designing and developing the artefact, the IoT system must be rigorously evaluated to determine whether it is functional. Sprints of several test will have to be conducted in order to gain a respectable overall perspective whether the system adheres to user requirements. It should also be taken into consideration to assess the IoT system by providing erroneous or incomplete stream data to evaluate how the IoT system manages faulty data. In cases where the IoT system is provided with erroneous data, the system should provide clear error messages as to why the system cannot manage/pipeline the data. Sensors Sprint test runs should be conducted to determine whether the microcontroller’s sensors can stream data. The results and observations should be documented. The test runs can be conducted by printing the received data on a console on the Arduino IDE platform. It would be optimal to display error messages if the Arduino IDE does not receive any streamed data, and indicators as to why the sensors failed to stream any data. In some instances, circuitry might be the cause for a sensor to fail to stream data. The circuity should either then be redesigned, or one should ensure that all pins are properly connected to the microcontroller. The demonstration serves the purpose of determining whether the system adheres to user requirements.KafkaThe demonstration is used to evaluate how well the technology complies with user needs. Once again, a series of sprint test runs should be conducted in order to determine how the Kafka streaming platform manages streamed data. All results should be properly documented and stored. It is necessary to record any errors that occurred during the demonstrations of data streaming. Keeping logs of why errors occurred, and why Kafka failed to pipeline data from the Arduino microcontroller can prove to be invaluable to design a more efficient and optimal IoT system, as well as preventing future developers from making the same mistakes when building similar systems. Tableau Desktop/DashboardsUsing the script that generates sufficient real-time data, one should observe the results from data analysis and dashboards. It might be necessary to run the script a few times in order to generate enough data for the dashboards to display relevant information. The dashboards can be screenshotted as evidence of the test runs.Perform an evaluation of the artefactAfter demonstrating the artefact, an evaluation session should occur to determine how the IoT system performed overall. The evaluation can be done by documenting the results and drawing a valid conclusion based on the data analysis from the sprint test runs. A comparison must occur between solution objectives and the actual demonstration data results. Sensors An evaluation session should follow the demonstration of the microcontroller's capability to stream data from sensors to ascertain how well the data streaming went overall. Documenting the outcomes and coming to a sound conclusion based on the data analysis from the sprint test runs are two ways to conduct the review. The solution goals and the actual demonstration data outcomes must be compared.KafkaFollowing the demonstration of the artefact, an evaluation session should be held to determine the real-time data streaming performance from the Arduino microcontroller to the Kafka streaming platform. Documented results from the demonstration will assist the developer in drawing valid conclusions (did the artefact meet user requirements/did the artefact provide the intended functionalities?). There must be a comparison between the solution objectives and the actual demonstration data results. It may be necessary to evaluate key performance indicators in order to determine the efficiency of the IoT system. It is of the utmost importance to determine how the IoT system manages high-velocity and high-volume data. Did the tools and additional platforms used to design the stream processing system contribute to an efficient IoT system, or did it lead to worse data streaming performance/cause the system to crash? Tableau Desktop/DashboardsDuring this stage it is important to evaluate the results from the dashboards. Did the dashboards provide enough information in order to make an informative business decision? Did the data visualization techniques provide enough information as to inform users what the purpose of the artefact is? A comparison must occur between the solution objectives and the actual demonstration data results, to determine the efficiency of the IoT solution and whether it adheres to user requirements. Did the Kafka-dashboard architecture efficiently pipeline data, or would it be necessary to redesign the architecture and use different dashboard tools and software? Communicate the results of demonstration/evaluation to audienceAfter conducting the evaluation session, the results should be communicated to the audience that have an interest in this artefact. The results can be presented in a written form or presentation (either visually or orally). One should communicate the research problem to the audience and why/how the solution contributes to resolving the problem. It is also a clever idea to walk one’s audience through the design and implementation process, to gain a deeper understanding of the artefact. Questions that the audience have about the artefact must be addressed professionally and being prepared for the questioning session is of utmost importance.Sensors The audience that are interested in this artefact should be informed about the evaluation session's findings. The findings may be given orally or in writing. It would be a clever idea to present the Arduino microcontroller’s data streaming capabilities live to the audience. One should explain to the audience the research problem and why or how the solution helps to solve the problem. For a deeper understanding of the artefact, it is also a good idea to walk the audience through the design and execution process. The audience's inquiries about the artefacts must be managed properly, and it is crucial to be ready for the question-and-answer period.KafkaThe findings of the evaluation session should be shared with the audience members who are interested in this artefact. The Arduino microcontroller's data streaming capabilities should be demonstrated to the audience in real time. One should explain the research problem to the audience and why or how the solution contributed to solve the problem. The streaming process system should also be demonstrated to the audience via the use of a poster or an electronic visual presentation. One should be properly prepared to answer any doubts or uncertainties the audience may have.Tableau Desktop/DashboardsDuring the final stage one should communicate the results of the dashboards to the intended audience or stakeholders. The final presentation should contain the dashboards and indicate what informative business decisions can be made from the analysed data. It might be a promising idea to walk the stakeholders through the Kafka-dashboard architecture to explain how data is pipelined from Kafka to the dashboard environment. One should be prepared for any questions the stakeholders might have during the presentation session, and it is important to focus on key metrics to discuss the relevance of information derived from the dashboards.Chapter 3:  LITERATURE REVIEW - ARCHITECTURETechnical aspects and components of literature reviewIntroductionThis chapter introduces the components of the literature review. An in-depth examination of Arduino sensors, streaming databases, the Kafka streaming platform and Tableau Desktop are provided. Each section is listed as a subheading in this chapter, and the main points of discussion of each section will be listed as further subheadings. Subheadings relevant to each section are listed in between the main components of discussion. The objective of this chapter is to introduce the crucial components of the artefact and to examine the essence of each component to the artefact. This chapter should serve as a component of the knowledge base on how to design and implement an IoT system. The pertinent question is how to stream data from sensory input to the Kafka streaming platform via an Arduino and convert the data into a pivot-table format (or any other streaming processing format for that matter). Research should be conducted on how to transfer the formatted data to a dashboard (Tableau Desktop). Data analytics in the Tableau Desktop environment is studied, such as data visualisation techniques and data transfers from Kafka to the dashboard environment. The tools and development environments necessary to design and implement the IoT system are investigated and should be utilized. This chapter should serve as the basis on which future generations of developers can rely to design and develop efficient/effective IoT systems using streaming databases to process real-time data. This research aims to fill gaps in the literature by providing a new perspective on how to implement and design an IoT system. This study should enable junior developers/individuals with little to no experience in the IoT or software development fields to quickly establish a fully functional IoT system. The study's goal is to provide overly simplistic documentation, tools, and architecture for designing and implementing an IoT system. Section 3 introduces the technical aspects and components of the literature review. Section 3.1 introduces this chapter, followed by Sections 3.2 – 3.5, which contains respectively: Streaming Databases, Arduino Sensors, Apache Kafka as a streaming platform and finally Dashboards: Tableau Desktop. Each section contains subsections that focuses on the details of each main section. This section introduces streaming databases, Section 3.2.1 describes how streaming databases process and manage real-time data, Section 3.2.2 lists the applications of streaming databases and Section 3.2.3 mentions the advantages and disadvantages of utilizing streaming databases. Section 3.3.1 introduces Arduino sensors and Section 3.3.2 mentions possible applications of Arduino sensors. Section 3.4.1 provides an overview of Apache Kafka, Section 3.4.2 lists some of the applications of Kafka, Section 3.4.3 describes the process of streaming data from a sensor to Kafka and Section 3.4.4 explains how Kafka manages and process real-time data. Section 3.5.1 introduces dashboards and Tableau Desktop, Section 3.5.2 lists the possible applications of dashboards, Section 3.5.3 explains how one pipelines data from Kafka to a dashboard environment and Section 3.5.4 focuses on data visualization techniques and types of data analysis that can be performed in a dashboard environment. The chapter concludes with Section 3.6, which contains the executive summary that provides an overall conclusion of the chapterStreaming DatabasesAn introduction is provided on streaming databases, how they manage and process real-time data, as well as the applications and benefits/drawbacks of this component. The streaming database is an essential component to the artefact, as this will be used to stream data from a sensor to a different environment.A real-time, continuous, ordered (implicitly by arrival time or explicitly by timestamp) succession of items is known as a data stream. It is not possible to save a complete stream locally, nor is it possible to regulate the sequence in which the items arrive (Hébrail, 2008:90). Data streams are processed using continuous queries by DSMSs, which are specialized systems based on stream processing engines (Surdu, 2011:69). These systems were developed to meet the needs of applications that monitor streams of data in order to deliver useful results (Surdu, 2011:69). Benchmarks demonstrated that a DSMS can perform at least a factor of four better than a DBMS when processing heavy load data streams and doing continuous and one-off queries (Surdu, 2011:69). To maximize throughput and speed up systems, Data Stream Management systems (DSMS) handle data without storing it (Zimanyi, 2017:4). Combinations of values are delivered by data sources in the form of data streams throughout time (Surdu, 2011:67). Some applications will find it easier to use the data thanks to the stream processing programming paradigm, which is comparable to data-flow programming, event stream processing, and reactive programming (Zimanyi, 2017:2). Software systems that manage data streams by providing flexible concurrent processing are known as data stream management systems (DSMS), more commonly known as streaming databases (Zimanyi, 2017:2). These systems list processing and computation of data as a higher priority than storing it (Zimanyi, 2017:2). Data streams are instantly processed, after which they are discarded (Surdu, 2011:70). An element cannot be retrieved after it has been viewed and managed by the processing engine (Surdu, 2011:70). Although some data stream history may be saved, ultimately it will be discarded (Surdu, 2011:70). The variety of data sources may significantly grow as time goes on (Surdu, 2011:70). The rate at which tuples enter the stream, or stream tuple rate, can also be adjusted (Surdu, 2011:70). When a system is overloaded, it might no longer be able to respond to queries promptly (Surdu, 2011:70). High tuple rates can be addressed using a range of methodologies (Surdu, 2011:70). Real-time analytics and continuous processors are powered by continuous queries (Zimanyi, 2017:6). A continuous query is essentially a query support that refreshes previously emitted results (Zimanyi, 2017:6). In other words, a continuous query produces results in a dynamic table that is continuously updated and may be queried in the same way as a static table (Zimanyi, 2017:6). Continuous queries run continuously (hence the name) and produce a dynamic table updated on the fly, in contrast to a conventional query that terminates and produces a static table (Zimanyi, 2017:6). This idea is comparable to the one that materialized view maintenance in SQL attempted to implement (Zimanyi, 2017:6).Management and processing of real-time data describes a stream processing system's architecture for sensor applications. The data source manager, query processing engine, catalog manager, scheduler, QoS (Quality of Service) manager, and ECA (event-condition-action) manager are the six parts that comprises the system (Jiang & Chakravarthy, 2006). Continuous data streams are accepted by the data source manager, which adds input tuples to the appropriate input queues of query plans (Jiang & Chakravarthy, 2006). Additionally, it monitors several stream input characteristics as well as data stream characteristics (such as categorization) (Jiang & Chakravarthy, 2006). These features offer valuable data for query scheduling, query optimization, and QoS control (Jiang & Chakravarthy, 2006). The task of creating query plans and dynamically improving them falls under the purview of the query processing engine (Jiang & Chakravarthy, 2006). Both continuous queries and one-off ad-hoc queries are supported (Jiang & Chakravarthy, 2006). The system's stream meta data, comprehensive query strategies, and resource data are all stored and managed by the catalog manager (Jiang & Chakravarthy, 2006). Due to the system's continuous nature, the scheduler chooses which operator or query to run during each time period (Jiang & Chakravarthy, 2006).  A few scheduling tactics exists to handle the scheduling and execution of queries (Jiang & Chakravarthy, 2006). The majority of stream-based applications have different QoS requirements, thus in order to ensure that the QoS requirements of diverse queries are met, the QoS manager uses a variety of QoS delivery mechanisms (such as load shedding, admission control, and so on) (Jiang & Chakravarthy, 2006). Monitoring changes through CQs (continuous queries) is essential and crucial in a sensor environment to record and comprehend the physical world (Jiang & Chakravarthy, 2006). It is important to note that it is equally crucial to respond to such changes right away (Jiang & Chakravarthy, 2006). Within a sensor application containing a stream processing architecture, complicated events are detected, and predefined actions are taken in response utilizing the ECA manager.Figure 3.1: Architecture of Continuous Query Processing System (Jiang & Chakravarthy, 2006)The figure below provides a simple generalized architecture of a data stream management system:Figure 3.2: Abstract reference architecture for a data stream management system (Golab & Ozsu, 2003:3)If the system can't keep up, an input monitor may reject packets as a means of controlling the input rates (Golab & Ozsu, 2003:2). Three partitions are commonly used to store data: summary storage for stream synopses, temporary working storage for window queries, and static storage for meta-data (such as the actual location of each source) (Golab & Ozsu, 2003:2). Although one-time questions over the status of the stream may also be presented, long-running queries are registered in the query repository and grouped for shared processing (Golab & Ozsu, 2003:2). In response to shifting input rates, the query processor may re-optimize the query plans in communication with the input monitor (Golab & Ozsu, 2003:2). Users either stream or temporarily buffer the results. Finally, based on the most recent results, users can refine their queries (Golab & Ozsu, 2003:2).ApplicationsSensor networks can be used to monitor geophysical conditions, traffic jams, mobility, vitals in the medical field, and manufacturing procedures (Golab & Ozsu, 2003:3). These programs use intricate filtering, and when odd patterns in the data are found, an alarm is set off (Golab & Ozsu, 2003:3). While aggregation over a single stream may be required to make up for individual sensor failures (due to physical damage or low battery power), aggregation and joins over several streams are necessary to analyse data from multiple sources (Golab & Ozsu, 2003:3). Access to some historical data may be necessary for sensor data mining (Golab & Ozsu, 2003:3).Ad-hoc technologies are already in use to monitor Internet data usage in almost real-time (Golab & Ozsu, 2003:3). Similar to sensor networks, it is necessary to combine data from many sources, monitor and filter packets, and look for unexpected circumstances (such congestion or denial of service) (Golab & Ozsu, 2003:3). It is also necessary to support historical inquiries and web mining, maybe to compare current traffic patterns with patterns that correspond to well-known occurrences like a DoS attack (Golab & Ozsu, 2003:3). Other criteria include keeping track of recent/popular URL queries or identifying the users who use the most bandwidth (Golab & Ozsu, 2003:3). These are particularly significant because it is a general conception that Internet traffic patterns follow the Power Law distribution, which results in a selected few users consuming a significant portion of the available bandwidth (Golab & Ozsu, 2003:3).Low latency requirements and real-time responses are provided by continuous queries (Surdu, 2011:69). A driver must receive a real-time notification whenever a new toll is imposed for his or her vehicle in a variable tolling system that computes highway tolls depending on dynamic criteria like accident proximity or traffic congestion, for example (Surdu, 2011:70). It would be useless to provide such a response in the future (Surdu, 2011:70).The online mining of call logs, call history, and transactions from Automated Teller Machines all adopt the data stream approach (Golab & Ozsu, 2003:3). Finding intriguing customer behavioural patterns, spotting questionable expenditure patterns that can point to fraud, and predicting future data values are main objectives of transaction log analysis systems (Golab & Ozsu, 2003:3). This necessitates merging several streams, intricate filtering, and statistical analysis, as with most other streaming applications (Golab & Ozsu, 2003:3).Regardless of the industry, current Information Systems (IS) manage an increasing amount of data to enable human activity (Hébrail, 2008:91). Analysis of ever-increasing amounts of data is necessary for the monitoring of these systems, such as the supervision of a telecommunication network (Hébrail, 2008:91). The "store then process" approach, which was formerly utilized to maintain massive data warehouses, is no longer an option (Hébrail, 2008:91). Real-time supervision applied to specific data is strongly preferred to batch supervision applied to aggregate data (Hébrail, 2008:91). This can only be accomplished using data stream processing, an approach that was devised to process data as it is being created (Hébrail, 2008:91).The primary function of many operational information systems is to process streams of data that arrive at a rapid intervals (Hébrail, 2008:91). For instance, in the financial industry, computerized systems help traders by automatically assessing the development of stock markets (Hébrail, 2008:91). In the same way that original Information Systems were created using files to store data before database technologies were available, such systems are being created without any generic tools to process streams (Hébrail, 2008:91). Data stream processing solutions will promote the approach to enable generic software to handle stream queries, mine streams for data, and publish the results (Hébrail, 2008:91).The first industry to employ commercial data stream management technologies on an industrial scale today is the Financing industry (Hébrail, 2008:91). The primary use of these type of systems in Finance is to support trading choices by tracking the evolution of stock price and sales volume as time progresses (Hébrail, 2008:91). These programs fall within the second group of operational systems, whose main objective is the control of data streams (Hébrail, 2008:91). Finding stocks whose price has climbed by 3% over the past hour and whose volume of sales has increased by 20% over the past 15 minutes are typical stream queries for these type of applications (Hébrail, 2008:91).Benefits and DrawbacksAccording to Zimanyi (2017:5), some of the benefits provided by data stream management systems are as follow:Analytics respond immediately to events (Zimanyi, 2017:5). There is no delay between the event's occurrence, how it is handled, and the actions that is executed (Zimanyi, 2017:5).Larger volumes of data can be handled by streaming databases than other processing systems  (Zimanyi, 2017:5).In the real world, data happens to be more continuous in nature. Such systems are suitable for handling real-time data containing huge volumes and rapid rates of transfer (Zimanyi, 2017:5).Infrastructure decentralization. Moving toward a microservices architecture means phasing out large, expensive data centres  (Zimanyi, 2017:5).The table below points out the main differences between a traditional database management system and a data stream management system:Table 31: DBMS and DSMS comparisons Source: (Zimanyi, 2017:7)Streaming databases are not feasible in all situations, however, and there are some issues/disadvantages, such as the following:Backtracking over a data stream is not possible due to performance and storage limitations. Online stream algorithms can only make one pass over the input data (Golab & Ozsu, 2003:2).Due to the fact that it is impossible to save an entire stream, approximate summary structures like synopses and digests are recommended (Golab & Ozsu, 2003:2). Queries about the summaries may therefore not have precise responses/results (Golab & Ozsu, 2003:2).Blocking operators, which requires the entire input stream before being able to produce any results, may not be permitted in some streaming query plans (Golab & Ozsu, 2003:2).It is rather difficult to gather, analyse, and consume dynamic, diverse, and unbounded observation streams.Figure 3.3: Big Data Challenges (Gürcan & Berigel, 2018:4)Arduino SensorsThis section introduces sensors and their applications. Sensors is a crucial component of developing the artefact, as it is the only means of providing input data to a streaming database. IntroductionA sensor is a device that changes a physical quantity's resistance or produces a voltage or current by converting it to an electrical signal (Ziemann, 2018:5). Various types of electrical signals, such as currents or voltage, are represented by the output signals (Patel et al., 2020:1). The sensor is an apparatus that receives various signals, such as physical, chemical, or biological signals, and transforms them into an electric signal (Patel et al., 2020:1). Based on the applications, input signal, conversion method, material utilized, and sensor properties like cost, accuracy, or range, the sensors are divided into many types (Patel et al., 2020:1). Active and passive sensors are the two primary categories of sensors (Patel et al., 2020:2). A passive sensor does not need an additional energy source, and it responds to external stimuli by producing an electric signal (Patel et al., 2020:2). Accordingly, the sensor transforms input energy into output signal energy (Patel et al., 2020:2). Photographic, thermal, electric field sensing, chemical, infrared, and seismic sensors are a few examples of passive sensors (Patel et al., 2020:2). The response of the active sensors, or excitation signal, requires external energy sources (Patel et al., 2020:2). Sensors make the appropriate adjustments to these input signals to produce the output signals (Patel et al., 2020:2). Due to their own features that can change in reaction to an external effect and then transform into electric signals, the active sensors are also known as parametric sensors (Patel et al., 2020:2). There are several applications for active sensors in meteorology and observation of the atmosphere and surface of the Earth (Patel et al., 2020:2). The table below indicates diverse types of sensors and their properties:Table 32: Sensors based on their detection propertiesSource: (Patel et al., 2020:3)In the past ten years, there have been significant advancements in sensor technology in terms of sensitivity, intelligence, and compactness (Patel et al., 2020:19). Traditional sensors including photosensors, optical sensors, capacitive sensors, and nearly all other types of sensors have been supplanted by integrated circuit versions such as MEMS (microelectromechanical systems) (Patel et al., 2020:19). Since the sensors are compactly integrated into all contemporary computer and navigational devices, a typical smartphone typically contains around twenty-two sensors that serve a variety of functions (Patel et al., 2020:19). The development of sensor technology has led to the creation of smart sensors that are intelligent and wearable (Patel et al., 2020:19). In major applications like self-driving cars, where hundreds of smart sensors are used for seamless and smooth driving without aid from a driver, this can be observed in smart watches, smart gadgets, or other electronic devices (Patel et al., 2020:19). This also applies to the use of AI in these fields, as well as others like robotics, medical diagnosis, and brain-computer interfaces (BCIs) (Patel et al., 2020:19). The sensors now possess the intelligence and smarts thanks to artificial intelligence for new and cutting-edge uses in the business world, the medical field, and complex automation (Patel et al., 2020:19).ApplicationsMilitaries make use of a   system called RADAR (Radio Detection and Ranging), which utilizes radio waves to detect objects and determine their range, altitude, direction, and speed (Kaswan et al., 2020:1114). Radars consists of different sizes and performance requirements (Kaswan et al., 2020:1114). Typical applications include long-range surveillance, early-warning systems in ships, and air traffic control at airports (Kaswan et al., 2020:1114). A missile guidance system's core is made up of these types of systems (Kaswan et al., 2020:1114). In times of conflict, numerous small portable radar equipment as well as systems that take up several large rooms are maintained and operated (Kaswan et al., 2020:1114). Arduinos are utilized in many industries due to the simplicity of the programming environment, signal kinds, and simple adaptability in new setups (Kaswan et al., 2020:1114). In order to add remote control and monitoring features to minor legacy industrial systems, Arduino boards provide a flexible, low-cost alternative to the typical industrial gadgets (Kaswan et al., 2020:1114). Due to the expansion of wireless technologies like Wi-Fi and cloud services in recent years, the use of wireless devices in daily life has become a standard routine (Kaswan et al., 2020:1114). Nowadays, Arduinos are used to monitor traffic lights, as well as real-time control systems with configurable timings, pedestrian illumination, as well as other applications (Kaswan et al., 2020:1114). In a traffic control system, the junction timing is automatically adjusted to accommodate smooth vehicle movement and prevent traffic jams at junctions (Kaswan et al., 2020:1114). The number of heartbeats in a minute can be counted by using an Arduino-based heartbeat monitor (Kaswan et al., 2020:1114). The Arduino possesses an integrated heartbeat sensor module that detects the heartbeat when a finger is placed on the sensor (Kaswan et al., 2020:1114). Many other medical devices can be designed using an Arduino microcontroller (Kaswan et al., 2020:1114). Arduinos can be utilized to operate a variety of bodily parts, including handSight gloves, Breathalyzer microphones, heart rate monitors, and other medical devices (Kaswan et al., 2020:1114). A heart rate monitor powered by an Arduino is more sophisticated than one that merely measures the user's heart rate (Kaswan et al., 2020:1114). Each button verbally explains what it does while also displaying the measurements on the screen. The latest four readings will be saved, displayed, averaged, and even some uplifting quotes can be provided via a monitor (Kaswan et al., 2020:1114). This sensor is used to monitor activity levels and patterns, fevers, and hypothermia (Kaswan et al., 2020:1114).Facial expressions can be detected by using this equipment (Kaswan et al., 2020:1114). We can determine breathing rate, breathing depth, activity level, and arousal levels with the aid of this Arduino contraption (Kaswan et al., 2020:1114). Some Arduino medical devices can also be used to identify both the frequency and intensity of muscular contractions, and this provides the functionality to monitor bodily movements (Kaswan et al., 2020:1114). These are but a few of the examples of fields that employs Arduino microcontrollers and their sensors. The table below indicates the applications of Arduino microcontrollers:Table 33: Summary of Arduino Application areasSource: (Kaswan et al., 2020:1115)Kafka Streaming Platform (Apache Kafka)This section introduces Apache Kafka, which is a streaming platform. The applications of Kafka are discussed, streaming data from sensors to Apache Kafka, how Kafka manages and processes real-time data, as well as the benefits and drawbacks of utilizing the streaming platform. Apache Kafka is the necessary component of the artefact to stream and pipeline data. IntroductionA distributed streaming platform called Kafka has three key features: The first step is to publish, distribute, and subscribe to records streams using a message queue or real-time messaging system (Gürcan & Berigel, 2018:3). Secondly, a reliable strategy is to store data streams with fault tolerance (Gürcan & Berigel, 2018:3). Process log and event streams are also a main capability of this streaming platform (Gürcan & Berigel, 2018:3). In order to connect streaming data systems and support real-time applications, Kafka provides dependable and low latency responses (Gürcan & Berigel, 2018:3). Kafka is designed to provide a consistent, high throughput, low latency infrastructure for processing feeds of real-time data and can be used to store and process data streams (Fernandes et al., 2020:428). Kafka can be used to handle events in real-time and integrate different software system components (Gürcan & Berigel, 2018:3). Kafka offers superior scalability and message consistency compared to other streaming platforms such as Flume (Gürcan & Berigel, 2018:3). Data flow from one application to another is managed by a messaging system, which focuses on data. Reliable message queuing serves as the foundation for distributed messaging (Fernandes et al., 2020:428). Two different messaging patterns are available, namely a Point to Point Messaging System, during which messages (streaming data) is transferred as a queue, the messages in the queue can be consumed by several consumers (Fernandes et al., 2020:428). The Publish-Subscribe Messaging System utilizes topics to contain messages. Each message in a given topic can be consumed by consumers from many topics (Fernandes et al., 2020:428). Kafka provides streaming applications with benefits such as load balancing and data replication, as well as high-velocity management of data (Fernandes et al., 2020:428). The limitations of Kafka include a decrease in performance due to brokers and consumers compressing and decompressing data flows, this can also affect the throughput, which can cause problems for the Kafka broker (Fernandes et al., 2020:428). When a message requires some tuning, the Kafka broker may occasionally have issues since Kafka's performance is decreased (Fernandes et al., 2020:428).ApplicationsActivity trackingKafka was originally designed to monitor transactions and activities on web pages (Franklin, 2022). LinkedIn had to redesign its user activity tracking pipeline as a collection of publish-subscribe feeds that are updated in real-time (Franklin, 2022). Due to the enormous volume of activity messages, also known as events, generated by each user page view, activity tracking is frequently quite intensive (Franklin, 2022). Clicks on web pages, user registrations, likes on social media, amount of time spent on web pages, orders, and environmental changes are some examples of events that can be tracked by Kafka (Franklin, 2022). Events can be published or produced under specific Kafka topics (Franklin, 2022). Each feed can be loaded into a data lake or warehouse for offline processing and reporting, among a variety of other use cases (Franklin, 2022). Additional applications can subscribe to topics, obtain the data, and use it as necessary for monitoring, analysis, reports, newsfeeds or personalization purposes (Franklin, 2022).Real-time data processingData must be processed as soon as it becomes available for many systems (Franklin, 2022). Kafka transports data with an extremely low latency from producers to consumers (measured in milliseconds) (Franklin, 2022). Financial institutions can collect and process payments/transactions as soon as they are received, this provides to be very useful when institutions try to counter fraudulent transactions and stop the transactions immediately (Franklin, 2022). Financial institutions are also provided with the ability to update dashboards with current market values (Franklin, 2022). Predictive maintenance (used in IoT systems) makes use of models that continuously examine streams of measurements from out-of-the-box equipment and provides sound alerts as soon as they spot deviations that might be signs of impending breakdowns (Franklin, 2022). Mobile devices must be able to process real-time data for the use of certain applications, such as navigation (a user can use Google Maps to travel from a starting location to any specified location) (Franklin, 2022). Logistics and the supply chain industries can monitor and update their tracking software in order to continuously track shipments/cargos and estimate the time of arrival in real-time to clients (Franklin, 2022).Messaging systemsSince Kafka has superior throughput, built-in segmentation, replication, and fault-tolerance, as well as improved scaling characteristics, it works well as a replacement for conventional message brokers (Franklin, 2022).Monitoring of operational metrics or key performance indicators (KPIs)Kafka provides the frequent monitoring of operational data. Statistical data from remote applications are combined in this process to create consolidated feeds of operational data (Franklin, 2022).Log AggregationsKafka is a popular log aggregation tool for companies (Franklin, 2022). In order to process log files, log aggregation often entails gathering actual log files from servers and storing them in a centralized location, such as a file server or data mart (Franklin, 2022). Kafka encapsulates the data as a stream of messages and filters file specifics (Franklin, 2022). This makes it possible to process data with lower latency and to accommodate distributed data consumption and numerous data sources more easily (Franklin, 2022). Kafka provides comparable speed to log-centric systems like Scribe or Flume while also providing substantially reduced end-to-end latency and higher durability assurances thanks to replication (Franklin, 2022).Streaming data from sensors to KafkaSources that are generated continuously streams data to the Kafka platform (Wu et al., 2020:207). Examples of these sources includes IoT device sensors, user activities stored from websites via the use of cookies or local storage, and payment requests received from mobile apps (Wu et al., 2020:207). In most of these use cases, streaming data needs to be managed real-time due to the fact that the value of these data becomes useless over time (Wu et al., 2020:207). To provide an example: a fraud detection system should be able to detect a fraudulent transaction before it is completed, thus ensuring that financial losses are circumvented in the process (Wu et al., 2020:207). Given that Apache Kafka serves as a modern distributed messaging system, offering elevated durability and scalability functionalities, it makes sense that corporate companies such as Twitter, Spotify, Uber and Netflix employ this platform to handle real-time data streaming (Wu et al., 2020:207). Kafka can process streamed data as an uninterrupted series of batch jobs on small batches of streaming data (Wu et al., 2020:207). Throughput as well as end-to-end latency in the stream processing system can be severely impacted by larger batch sizes (Wu et al., 2020:207). It does however provide the utilization of network bandwidth. The main purpose of Kafka is to transfer data from one platform to another in order to process data for multiple reasons (Wu et al., 2020:208).Components of Kafka in a stream processing system can be described as follow:Messaging schema (Publish/Subscribe, also known as pub/sub): built on top of the producer, topic, broker and consume components (Wu et al., 2020:208).Producer: application that sends streaming data, also known as messages, to their respective topics (different logical groups of messages) (Wu et al., 2020:208).Consumer: application that subscribes to a specific topic and reads messages from said topic (Wu et al., 2020:208).Kafka cluster: serves as the core part of the messaging system. This is essentially a distributed storage system that consists of various brokers (Wu et al., 2020:208).Brokers: servers that receives messages from a Kafka producer, provides disk storage for messages and duplicate the messages in order to provide fault tolerance (Wu et al., 2020:208). Brokers distribute all received messages in a specific topic among each other and they are stored in sequential write-ahead logs, also known as partitions (Wu et al., 2020:208). Enterprise applications usually have a predetermined number of partitions per topic, each broker can host multiple partitions under a specific topic (Wu et al., 2020:208).The figure below describes the stream processing pipeline using Kafka as the messaging system:Figure 3.4: Kafka in a streaming systemA Kafka cluster consists of three brokers, and the Filter, Map, reduce and Aggregate stream processors executes specific operations on the received streaming data (Wu et al., 2020:208). The messages can be delivered from multiple type of external sources, such as IoT sensors, payment terminals as well as mobile devices (Wu et al., 2020:208). The messages are then transferred via a stream processor (Wu et al., 2020:208). From the above figure, filter processing is performed, and messages are published to an explicit topic, such as Topic A from the example (Wu et al., 2020:208). Other processors from the streaming system transfer required messages from the Kafka cluster by means of subscription to respective topics (Wu et al., 2020:208). To provide an example: the Map processor subscribes to topic A, ingests messages, mapping operations are then performed on the messages and send the results to Topic B for other processors to subscribe (Wu et al., 2020:208). Furthermore, downstream applications may make use of user-defined functions to retrieve results for different services in order to make decisions (Wu et al., 2020:208). That is, every processor uses the producer and consumer application programmable interface to communicate with the Kafka cluster, in order to transfer data flows between different processors in the streaming system (Wu et al., 2020:208). Processing real-time data and queriesThis section is a follow-up from the previous section. As mentioned previously, Kafka is a distributed streaming platform that allows the streaming of high-velocity and high-volume data. Refer to for the standard Kafka architecture. Every topic category has several partitions, and each partition has an ordered, immutable sequence of messages that are continuously added (Liu et al., 2014:359). Kafka maintains these message feeds (Liu et al., 2014:359). Distinct sequential ids are provided to each message in order to identify it within a partition (Liu et al., 2014:359). A partition in a Kafka cluster is distributed over several nodes for fault tolerance (Liu et al., 2014:359). The published messages are stored in a Kafka cluster for a specified amount of time (Liu et al., 2014:359). Regardless of whether they have been read or not, messages are discarded when the time comes. Each subscriber to a message is identified by the name of the group they are a part of (Liu et al., 2014:359). To guarantee the proper message order, a partition of a topic can only be sent to one consumer in a consumer group (Liu et al., 2014:359).User activity tracking pipelines was originally rebuilt using Kafka as a collection of real-time publish/subscribe feeds (Liu et al., 2014:359). Site activities, such as page browsing, searching, and other user actions, are published to a Kafka cluster as topics, one topic is provided for each type of activity (Liu et al., 2014:359). For a variety of use cases, including real-time processing, real-time monitoring, data loading into Hadoop or a data warehouse, those topics are available via subscription (Liu et al., 2014:359). The Kafka data pipelines are observed for monitoring activities, such as keeping track of the statistics of aggregations from distributed applications that produce centralized feeds of operational data (Liu et al., 2014:359). Kafka is thus well adapted for circumstances where users must manage and analyse real-time data (Liu et al., 2014:359). LinkedIn currently supports dozens of subscribing systems and sends more than fifty-five billion messages to users each day. This is all possible through the use of streaming data with Kafka (Liu et al., 2014:359). Dashboards: Tableau DesktopThis section introduces dashboards, specifically Tableau Desktop as a tool to create dashboards. The applications of dashboards are discussed, how one can transfer data from Apache Kafka to a dashboard environment, as well as data visualization techniques and data analysis in dashboard environments. The dashboard is the crucial output component of the artefact, as it is used to display and analyse results in order to make informed business decisions.IntroductionA dashboard is a program or user interface that aids in understanding the organizational structures and business procedures of a company as well as being a useful analytical tool that measures the performance of an organisation (Ioana et al., 2014:852). Dashboards address the intensifying complex nature of market data that senior management faces in the age of information (Pauwels et al., 2009:176). Key performance indicators (KPIs) and key risk indicators (KRIs) are displayed in connected charts, maps, and scorecards in modern dashboards so that a company can concentrate on the most crucial performance operations (Ioana et al., 2014:852).The dashboard's goal is to clearly present information on a single screen so that it can be understood by all stakeholders  (Ioana et al., 2014:852). Four factors are mentioned by managers as driving the need for dashboards: the need for cross-departmental integration in performance reporting practices and for resource allocation, poorly organized pieces of potentially decision-relevant data, managerial biases in information processing and decision making, the increasing demands for marketing accountability, given the dual objective of companies to grow the top-line while keeping costs down for a healthy bottom-line (Pauwels et al., 2009:176). The purpose of a dashboard is to summarize important performance metrics with underlying factors, and they should be integrated in order to communicate performance levels throughout a company (Pauwels et al., 2009:177). A dashboard can be described as a reasonably compact group of linked KPIs and underlying performance factors that represents both immediate and long-term objectives and can be accessed by all members of the company (Pauwels et al., 2009:177). Tableau Desktop is software that provides analytics and data visualization using drag and drops (Sleeper, 2021:9). One can connect to a range of data sources, analyse data of all sizes, perform ad hoc studies instantly, integrate various components into coherent dashboards for simpler consumption, share views with others, and far more using Tableau Desktop (Sleeper, 2021:10). Tableau Desktop and Tableau Public are comparable, except Tableau Desktop allows one to load a worksheet onto the Tableau platform (Akhtar et al., 2020:31). It has a two-week trial period and is a licensed version (Akhtar et al., 2020:31). By directly linking to data from a data warehouse, one can take full advantage of real-time data analytics (Akhtar et al., 2020:31). One can quickly import data from various sources into Tableau's data engine and combine it by merging different views into an interactive display (Akhtar et al., 2020:31). Files created by Tableau desktop have twb and twbx extensions (Akhtar et al., 2020:31). Server as well as online sources can be used by Tableau Desktop users as a high-performance data store (Akhtar et al., 2020:31). Excel, Tableau Public, Reporting Services, SharePoint, Microsoft SQL Server, and Performance Point are all programs that can be used to construct dashboards (Ioana et al., 2014:852). All of these tools allow for the breakdown of business objectives, and the development of a plan to incorporate the required data (Ioana et al., 2014:852). Nevertheless, the majority of dashboards are generated in Excel because it is inexpensive to implement and can be used to track business performance on timely intervals (daily, weekly, monthly) (Ioana et al., 2014:852). Dashboards can be used as a tool to alter the corporate culture at all levels of the organization (Ioana et al., 2014:855). One of the most significant advantages of adopting dashboards is that managers are able to analyse data on a single screen where KPIs and KRIs are tracked, the decision-making process are streamlined, and actions can be taken to reduce risks and enhance corporate performance (Ioana et al., 2014:855).ApplicationsDashboards are frequently associated with corporate organizations, where they are used to optimize decision-making, improve operational efficiency, increase data visibility, drive strategy and transparency, lower costs, and improve communication (Sarikaya et al., 2018:686). A dashboard ensures that measurements and measurement practices are uniform across all departments and business units (Pauwels et al., 2009:179). To provide an example: Avaya employs a variety of marketing strategies and conducts business in over 50 different countries and markets (Pauwels et al., 2009:179). Prior to the dashboard project, the company lacked a global system of records (which restricted data collection), had disparate definitions of qualified leads (a crucial performance metric in the transition from marketing to sales), and had a lack of regional interest in metrics collection (Pauwels et al., 2009:179).Performance levels can be tracked via the use of a dashboard (Pauwels et al., 2009:179). The monitoring process itself can be both developmental (what the company have learned from the performance measures) and/or evaluative (which employees or departments within the company performed well?) (Pauwels et al., 2009:179). Google employs dashboard measures which serve as early performance indicators, and the organization will take corrective action if, for instance, the "trust and privacy" statistic indicates a deterioration (Pauwels et al., 2009:179).A dashboard can be used to plan the goals and strategies of a company (Pauwels et al., 2009:179). To illustrate this point, Ameritrade developed a dashboard that links into the planning cycle and is connected to quarterly bonuses using corporate scorecards from the strategic planning department (Pauwels et al., 2009:179).A dashboard could be used to inform stakeholders about essential company information and statistics (Pauwels et al., 2009:179). By selecting the metrics on the dashboard, it specifically communicates not only what the performance is but also what a business regards as performance measures (Pauwels et al., 2009:179). Vanguard provides to be a fantastic example of a company that effectively communicates dashboard metrics to their Board and translates their business focus on client loyalty, feedback, and word-of-mouth into their dashboard measurements (Pauwels et al., 2009:179). The dashboard is extensively utilized for many different reasons in sectors including education, health, and logistics (Rahman et al., 2017:1). For instance, dashboards are used in the educational environment to monitor performance and ensure consistency in order to increase student awareness of their own and their peers' learning activities (Rahman et al., 2017:1). It also aids in reflection, clarity, and the impact of the learner's traces on behaviour (Rahman et al., 2017:1). Even more diverse dashboard use cases and requirements can be found outside of the Business Intelligence area (Sarikaya et al., 2018:686). As an illustration, health organizations have adapted dashboards at both the large-scale (hospital management) and patient-care levels, with the main goal to promote collaboration and awareness across various roles, timespans, and skills (Sarikaya et al., 2018:686). Urban informatics and community organizations/non-profits, and large/diverse group of stakeholders all encounter obstacles with integrating various data sources, adapting to multiple platforms, including mobile devices, and developing metrics and representations to represent abstract results like community awareness, stakeholder engagements, as well as trust (Sarikaya et al., 2018:686). Table 34: Dashboard purposes and featuresSource: (Rahman et al., 2017:3)Transferring data from Kafka to dashboard environmentThis section suggests a hybrid architecture that strives to make the most of available tools in order to create an environment that can support the processing and real-time visualization of information. The data sources can be of various forms presented in the first section of this architecture (Sousa et al., 2021:15). In accordance with the suggested architecture, these data sources communicate with Solace directly (Sousa et al., 2021:15). The company's own APIs as well as open protocols should be used, according to the guideline. This approach has no technological limitations placed on the architecture (Sousa et al., 2021:15). Since Solace PubSub+ excels in both event broadcasting and event management, Solace viewed other streaming platforms as obsolete (Sousa et al., 2021:15). This is only accurate in instances of transactional and operational use, though (Sousa et al., 2021:15). For the storage and administration of analytical use cases in event flows, Kafka continues to shine (Sousa et al., 2021:15). Kafka is a necessary component in this architecture, due to the fact that the objective is to build an architecture that is as resilient and covers as many use cases as possible (Sousa et al., 2021:15). Although still in development, this new Solace PubSub+ version is not yet as stable as hoped. Additionally, a number of businesses that have invested in using these technologies in production use Kafka (Sousa et al., 2021:15). This technology may be utilised with the architecture suggested here, and it also fosters synergy between all of its parts (Sousa et al., 2021:15). Furthermore, this proposal makes Kafka utilization optional for those who like simpler architectures and are only beginning to invest in these areas (Sousa et al., 2021:15). Furthermore, it is suggested that in this instance of incorporating Kafka, connectors created by Solace should be used to enable data connectivity between Solace and Kafka (Sousa et al., 2021:15). MQTT, AMQP, REST, WebSocket, and JMS are just a few of the basic protocols that Solace supports for connecting directly to Kafka (Sousa et al., 2021:15). Pipelining data to Apache Spark is the next step after storing it in Solace and/or Kafka (Sousa et al., 2021:15). By doing this, we are allowing consumers to select the technology they want to employ to access the data (Sousa et al., 2021:15). The data analysis will subsequently be carried out in a parallel and distributed fashion using the data present in Apache Spark (Sousa et al., 2021:16). The two data access zones on Tableau (suitable tool for data analysts) and Power BI (more suitable for less skilled employees) forms the final component of the architecture (Sousa et al., 2021:16). By employing this approach, expenses can be decreased due to the fact that Tableau is the most expensive software in the suggested design, and staff can use Power BI more efficiently because it is highly flexible and simple to use (Sousa et al., 2021:16). The provided architecture is unique since it incorporates hybridization on top of Kafka in addition to offering a well-researched pipeline option for information processing (Sousa et al., 2021:16). Users have the choice to remain invested because Solace no longer supports the connection between Kafka and the suggested architecture (Sousa et al., 2021:16). Additionally, as depicted, the use of Kafka is optional since Solace supports all tasks designed for Kafka (Sousa et al., 2021:16). Figure 3.5 below demonstrates the architecture described in this section:Figure 3.5: A hybrid architecture supporting Kafka and Tableau integration (Sousa et al., 2021:15)Data visualization techniques/Data analysisData visualization is the ability to increase the effectiveness of data organization and processing (Akhtar et al., 2020:29). The depiction of data or information in a graph, chart, or other visual format is known as data visualization (Akhtar et al., 2020:29). People can process more complicated information and improve memory through visualization. It conveys how data and images relate to one another (Akhtar et al., 2020:29). This is crucial since it makes it easier to spot trends and patterns (Akhtar et al., 2020:29). We need to be able to comprehend progressively larger quantities of data as Big Data becomes more relevant in the modern era (Akhtar et al., 2020:29). Data analysis is the study of examining raw data to draw inferences from it (Patel, 2022:7). Numerous methods and strategies for data analysis are automatically transformed into machine operations and algorithms that process raw data for human utilization (Patel, 2022:7). A corporation can enhance its performance through the use of data analysis (Patel, 2022:7). In a typical business analytics dashboard, important components include:Data aggregation: the process of gathering, organizing, and sorting data in preparation for analysis (Patel, 2022:9).Data mining: the process of analysing large amounts of data for business statistics utilizing statistics, machine learning, and big data sets to find patterns and establish connections (Patel, 2022:9).Association and Sequence Identification: recognizing unexpected behaviours carried out in tandem with other behaviours or patterns (Patel, 2022:9).Text Mining: large-scale, unstructured data sets are scanned and organized for optimal quality/quantity analysis (Patel, 2022:9).Forecasting: the process of periodically analysing previous data to provide accurate predictions of future actions or events (Patel, 2022:9).Optimization: after patterns have been discovered and forecasts have been made, companies can use simulation techniques to determine the ideal circumstances (Patel, 2022:9).Data Visualization: the visual presentation of data in the form of graphs and charts allows for quick and simple data interpretations (Patel, 2022:10).To efficiently and simply portray the key points of the data, graphs and charts covering enormous amounts of information can be edited into simple formats (Patel, 2022:10). Consider the goal of the graph or chart as well as the information one wants to provide before deciding how to present the data in the best possible way (Patel, 2022:10). The following type of charts are available using Tableau software:Line chartsA series of data points connected by a straight line are included this type of chart (Patel, 2022:11). Each point plotted on the chart explains how the graph's horizontal and vertical axes relate to one another (Patel, 2022:11). The X-axis displays the important measurements or time dimensions, such as month, quarter, or year, while the Y-axis displays the numerical value (Patel, 2022:11). Typically, temporal patterns and correlation, as well as data over a specific time period, are represented using line and area charts (Patel, 2022:11).Area chartsBy colouring the region between the line segment and the x-axis, area charts are used to jointly measure data patterns across timely periods (Patel, 2022:12). A line chart is simply a continuation of an area chart (Patel, 2022:12).Column line chartsA combination of a line chart and a column chart is commonly known as a column line chart (Patel, 2022:13). One measure is shown as a column and another as a line in this sort of chart (Patel, 2022:13). These two metrics are displayed under several time units, including months, quarters, and years (Patel, 2022:13). This graph works far better to indicate the relationship between two measurements over time (Patel, 2022:13).  Sales revenue, net income, and gross margins are examples of common measurement on a column line chart (Patel, 2022:13). Bar chartsAmong all chart kinds, the bar chart is the one that is used the most (Patel, 2022:15). Bar charts are frequently used to organize data, filter out information that is unimportant given a particular use case, and position data by value in descending or ascending order (Patel, 2022:15). A bar chart is typically used to compare several categorical data values, which includes grouping the data by fusing numbers together in a chart (Patel, 2022:15). The less important categorical data should be placed into another group if there are multiple categories available (Patel, 2022:16).Stacked area chartsThe base area chart is expanded upon the stacked area chart (Patel, 2022:16). Since the values of each group are placed on top of one another, it is possible to see both the sum of a numerical variable and the importance of each group from the same image (Patel, 2022:16).For example, the lines can be made to show how the population of different states has changed through time (Patel, 2022:16). A graph representing population trends and showing statistics of each state in order from least to most populous state can be created by colouring the region beneath each line to symbolize the state it represents (Patel, 2022:16).Horizontal ChartsThe most effective graphical tool for showing comparisons between data categories frequently is a horizontal bar chart (Patel, 2022:17). With the use of a horizontal bar chart, the presenter can easily exhibit huge data labels because the horizontal rectangles offer space for text presentations  (Patel, 2022:17). Waterfall ChartsA waterfall chart is a visual representation of data that makes it easier to comprehend the impact of adding up positive or negative values (Patel, 2022:17). These intermediary values may be based on phase or on time (Patel, 2022:17). Due to what appears to be a formation of columns (bricks) in the midst of the air, the waterfall chart is also known as the flying brick chart or Mario chart (Patel, 2022:17). It is typically referred to as a bridge in the Financing industry (Patel, 2022:18). Using this kind of chart, one can observe how values fluctuate with regards to positive and negative values (Patel, 2022:18). Typically, waterfall diagrams are used to show how a series of intermediate values rise and decrease a starting value to produce a final outcome (Patel, 2022:18).Abovementioned points are but a few of the visualization techniques used within Tableau software. It is important to identify key performance indicators in order to determine which type of visualization technique to employ to present statistics/results to stakeholders. Several dashboards allow numerous coordinated views (Sarikaya et al., 2018:685). Data faceting using slicers and filters, cross-highlighting by choosing data items within views, and drilling up and down the layers of a data hierarchy can all be used to interact between views (Sarikaya et al., 2018:685). These dashboards enable users to narrow their analysis to the data points that matter to them (Sarikaya et al., 2018:685). Cross-highlighting (such as darkened visual elements) or the presence of standard interactive elements (such as drop-down menus or slicers), are deemed as interactive capabilities of a dashboard (Sarikaya et al., 2018:685). The figure below is how a typical Tableau Desktop dashboard looks:Figure 3.6: A typical Tableau Dashboard (Akhtar et al., 2020:37)Chapter SummaryA streaming database provides the means to store and process real-time data. Streaming databases can manage multiple data streams from disparate data sources. These type of data repositories allows one to analyse data real-time. Streaming databases are perfectly suited for IoT applications, automated business processes and real-time analytics since they can process and analyse queries immediately as they are received. A streaming database provides several advantages when developing real-time applications, such as reduced development costs and faster application development times. Managers can market their products a lot faster and improve the decision-making process via using the real-time analytics provided by streaming databases. IoT sensors allows streaming databases to receive data in real time. Sensors are used in modern day applications to detect changes in an environment and respond accordingly. Sensors communicate with other devices in a network in order to transfer data between devices and systems. Sensors are well suited for modern real-time applications as they can warn one of events occurring, and businesses can make an informed decision on how to respond to the events based on the warnings and data provided by sensors. Data provided by sensors are often used for real-time analysis purposes and allows business managers to effectively manage business processes based on the real-time data. Sensors have a variety of purposes, and are often used in temperature measuring applications, detecting health vitals of patients in a hospital, humidity detection in an environment and detecting movements in a room.Apache Kafka is a distributed streaming platform that publishes/subscribes to real-time data streams and makes use of message queueing to pipeline data. A main feature of Kafka is to provide fault tolerance for data streams and to process data streams as soon as they arrive on the platform. Kafka allows developers to build real-time streaming data pipelines to transfer data between sensors and applications, as well as assisting in the development of real time applications that transforms or reacts to streams of data. Kafka also provides a level of scalability that most other streaming platforms fail to offer. A dashboard is a tool that allows one to visualize data and present the most valuable information so managers can make informed business decisions and to plan company objectives and strategies. Dashboards should be arranged on a single screen so stakeholders can gain a comprehensive overview of key performance metrics. Dashboards provides the means to display vast amounts of data on a single screen and streamlines the decision-making process. A dashboard can also indicate what type of data is essential to business process, and managers can re-evaluate business strategies and determine new key performance indicators to monitor the performance of business processes. A variety of data visualization techniques are provided by dashboards, such as charts and graphs. It is important for managers to decide which type of visualization technique to use, as they support various kinds of performance measurements. Tableau Desktop is a dashboard software solution that allows the rapid creation of dashboards. It can also manage enormous amounts of data sufficiently and is thus well suited for real-time data analytics. The main objective of this study is to determine how stream sensory input from an Arduino microcontroller and pipeline real-time data to a streaming database/platform, such as Apache Kafka. The data must be processed on the streaming platform and finally get pipelined and displayed on a dashboard. Tableau Desktop is the preferred dashboard software at the moment. Based on the data analysis from the provided dashboards, one should be able to make a strategic business decision. This study's potential to contribute to a body of information on how to design and deploy a real-time IoT system is another crucial feature. It is important to keep track of experiences and observations since they may be used to provide suggestions for creating and trying to implement an IoT system as well as to shed light on alternative ways to tackle challenges observed during the design and implementation process. An essential component of this study is the documentation of incidents, interactions, and observations. Chapter 4:  DEVELOPMENT OF ARTEFACTArtefact developmentIntroductionThis chapter introduces the development of the artefact. A quick overview is provided on the research question, what the artefact is about and how the artefact attempts to solve the research question. A further overview of the artefact is provided afterwards, such as the development environment and technologies that was identified to develop the artefact. An extensive overview is provided of the artefact life cycle, the phases that occurred in the lifecycle and the processes/steps of each phase in the life cycle. Furthermore, a data mining technique will be discussed, which will be utilized to perform data analysis on the results from the dashboard. Finally, a discussion occurs on the development of the artefact. The last section focuses on the incremental steps followed to develop the IoT system, how tools and environments were setup and implemented to solve the research question. The final section can be regarded as a user manual for future developers who would like to quickly and efficiently design and develop IoT systems.The purpose of this artefact is to design and develop an architecture to stream data from a sensor connected to an Arduino Uno R3 to Apache Kafka, which pipelines the data to a Dashboard environment, such as Power BI, in order to perform data analytics and visualize the data. From the results and data visualization, one should be able to make an informed business decision. The focus of the artefact is architecture, therefore a use case for the IoT system is not of utmost importance. A few tools and development environments have been identified to design and develop the IoT architecture:An Arduino Uno R3 and a KY-039 5V pulse rate sensor to stream input data. A heart rate detection use case is employed to demonstrate the architecture of the artefact.Apache Kafka, which is utilized as a streaming platform to pipeline real-time data from the Arduino to the dashboard environment.MongoDB, a NoSQL database which is employed to persist the data read from the sensor and pipelined via Kafka. This allows one to keep historical records of sensor data.The dashboard environment: Power BI was selected due to its powerful analytical tools and efficient data visualizations, as well as being the most suitable free dashboard product.The purpose of this artefact is to contribute to an existing knowledge base on how to design and implement IoT systems. The research should contribute to future developers or individuals with little to know experience in programming and the IoT field to quickly setup and develop simple IoT systems. Possible issues detected in the artefact must be documented, and possible use cases must be listed, as well as possible improvements that could be made to improve the artefact.The structure of the chapter is as follow: Section 4.1 introduces the artefact officially, and how it pertains to the research question. Section 4.2 provides a more comprehensive overview of the artefact, which tools and develop environments have been selected to develop the artefact. Section 4.3 introduces the life cycle of the artefact, a DSR project life cycle was selected to design and develop the artefact. Section 4.3.1 highlights the research question (problem identification and motivation), Section 4.3.2 highlights the objectives of the artefact, and Section 4.3.3 covers the design and development of the artefact. Section 4.3.4 discusses the demonstration of the artefact, which involves an iteration of testing to identify issues and to optimize the artefact. Section 4.3.5 provides an evaluation of the artefact, which involves the assessment of the artefact to determine whether the artefact solved a real-life problem and provided a solution to the research question. The final phase of the project life cycle involves communicating the results to interested parties. From the results provided by the dashboard and after analysis of the data has occurred, one should be able to make an informed business decision based on a specific use case. Once again, the focus of this study is on architecture, not applications. Section 4.4 introduces a data mining technique which will be used to analyse the results on the dashboard. Finally, Section 4.5 provides an extensive overview of the development of the artefact. The structure of the development of the artefact will use the format of a typical user manual.Description of artefactThe artefact demonstrates an architecture that is utilized to stream real-time data from an Arduino sensor to a dashboard environment, the dashboard results should allow one to make informed business decisions. The artefact consists of several components:Even though the focus of the research is on architecture and not applications, a heart rate detection application will be demonstrated to visualize the architecture of the artefact (IoT system).An Arduino Uno R3 microcontroller is used to receive input data and connected to the microcontroller is a KY-039 5V pulse rate sensor. This sensor can be used to detect pulse rates via the use of infra-red light. When something is shading the light sensor, the sensor outputs an analogue number between 0 and 1023 that indicates how much infrared light it receives. The less infrared light, the higher the value becomes.A finger needs to be inserted between the sensor's IR LED and light transistor. The finger's blood vessels expand in response to heartbeat, which filters infrared radiation. As a result, a pulsing signal is produced, which is transformed into an antilog numeric reading that can be output in a terminal or graphical user interface (GUI).Arduino IDE is the development environment which the developer uses to write code to display analogue values received in a numeric format from the Arduino sensor. C++ is primarily used as the language to develop Arduino sketches (code). One can display the streamed values in a terminal within the IDE or in a serial plotter, which displays the values as a graph, with a time series on the x-axis.VS Code and Python: VS Code is a text-editor developed by Microsoft. It is a particularly useful tool, as it provides an integrated development environment with numerous extensions that allows one to develop applications in multiple different programming languages. This tool was selected due to the fact that it is an overly simplistic tool and one can setup and develop applications/scripts and websites quite easily. Python was selected as the main programming language to pipeline rea-time data from the Arduino to the dashboard environment, due to its simplicity in comparison to other programming languages like Java and C++. Python is one of the programming languages that is the easiest to develop scripts with, as well as containing various modules (libraries) that makes it a powerhouse in the mathematical, statistical analysis and data science fields. Regarding this artefact, it also contains useful libraries like serial and Kafka that allows one to stream data from an Arduino to Apache Kafka. Java is the default programming language to develop Kafka streaming applications but was not selected due to its complexity and tedious dependency management. Apache Kafka: the streaming platform. Apache Kafka allows developers to stream data from IoT devices in order to pipeline data for processing or retain it. Typically, a Kafka application consists of a topic, producer and consumer. Apache Kafka was developed to keep track of events (a real-time occurrence or action). These events are stored in topics, which is basically a folder or file system Kafka uses to group data. A producer is a component of Kafka which publish (writes) events to a topic, and a consumer is a Kafka component that reads and processes messages from a producer. For further information on Apache Kafka architecture, read the official Apache Kafka docs or refer to  of the literature study.MongoDB: as mentioned in the introduction of this chapter, MongoDB is a NoSQL database that is used to store and process unstructured data. That is, MongoDB does not contain concepts of relational data and tables, SQL, foreign and primary keys etc. Instead, MongoDB relies on storing data in a document format, typically JSON (JavaScript Object Notation). Data is stored by key and value pairs, and one can query the documents by key or values. MongoDB was chosen as the database to persist the real-time data, due to the fact that the Arduino streams the data in an unstructured format, as well as providing better performance when receiving high volumes of unstructured real-time data in comparison to typical RDBMSs such as MySQL or Postgres.Finally, the dashboard environment. Power BI was selected as the tool to visualize the data pipelined from Kafka and persisted in MongoDB. Power BI is a group of software services, applications, and connectors that are combined to transform disparate data sources into cohesive, engaging visuals, and interactive insights. Data could be stored in a hybrid data warehouse that is both cloud-based and on-premises, or it could be an Excel spreadsheet. What makes Power BI particular useful is that one can employ the services to combine different data sources into one dataset and perform powerful analytics on the data and visualize the data via different graphs and tools such as reports or dashboards, each providing their own unique insights. The Web version forms a component of the artefact, since the desktop version cannot update visuals that contains real-time data. An Application Programmable Interface (API) is utilized to allow communication between the Kafka streaming application and the Power BI Web services.The design and development of the artefact is thoroughly discussed in the following sections. The artefact should contribute to a knowledge base on how to design and implement simple IoT systems. The design and development of the artefact will be documented, as well as issues detected with the artefact and possible improvements to optimize the artefact. *Note: it is preferable to deploy the IoT system on a machine with the Windows 10 operating system.Life cycle of artefact and phases involvedDesign Science is a methodology that can be utilized to create and evaluate IT artefacts that are themselves intended to address identified business challenges. DSR comprises a full process of creating artefacts to address observable issues, as well as contributing to research (adding on to existing knowledge bases), assessing the proposed designs, and informing the appropriate audiences of the findings via observation and experiences. The DSR life cycle consists of very distinctive phases that makes it suitable for implementing Agile project management on artefacts since the phases are iterative by nature. DSR involves testing components of an artefact and changing user requirements until it satisfies a project scope. This section provides a detailed overview of the DSR life cycle that is implemented in phases to design and develop this artefact.Problem identification and motivationDuring the initial phase, one needs to state the research question and explain why finding a solution to the research question is important. By dissecting the problem into chunks, the solution can encapsulate the intricacy of the issue and ultimately prove useful. The research question serves as the basis of designing and developing the artefact, and all other processes and planning flows from this stage. It also helps the audience to understand the researcher's thinking as it relates to how to answer the research question. The focus is on comprehending the issue and finding a solution.The research question can be defined as follow: how would one go about to design an architecture that allows an Arduino Uno R3 microcontroller to stream data to a dashboard environment via using a streaming database/streaming platform to pipeline the data? From the results visualized on a dashboard, one should be able to make a business decision that is apparent on the data insights. The artefact must contribute to a knowledge base on how to design and implement simple IoT systems. It is important to research and identify the tools and development environments that will be necessary to design an IoT streaming architecture. The knowledge base and documentation should make it feasible for anyone with the minimal possible IoT and streaming databases experience to quickly set up and deploy smart systems while also being able to deliver high-quality and efficient IoT systems.The research question can be solved by examining a proper architecture on implementing an IoT streaming system. Firstly, one needs to identify a sensor that will be applicable given a certain use case. A proper Arduino microcontroller circuitry must also be researched in order to prevent a sensor from short-circuiting or decreasing the performance of the microcontroller. One must research how to receive analogue values from the Arduino sensor and display the values in and integrated development area, in this case Arduino IDE proves to be a suitable environment. Research must be conducted on Apache Kafka, how the Kafka architecture works and how to stream the sensor data from the microcontroller to a Kafka cluster/topic. Apache Kafka is suitable for streaming and pipelining data in order to process or retain it. In order to persist the data, one must research a suitable DBMS that can manage unstructured data well at high velocities and huge volumes. MongoDB was identified as a database that manages unstructured data quite well. Finally, one must conduct research on how to pipeline the data from Kafka to a dashboard environment such as Power BI. The dashboard tool must be utilized to provide data insights and visualization. Based on the data analysis, one must be able to make an informed business decision, based on a practical use case. The architecture of the IoT streaming system will be demonstrated by providing a heart rate monitoring application as a use case.Define objectives of the solutionDetermine the objectives of a solution based on the problem statement and your understanding of what a possible feasible solution is. A description of how a new artefact is intended to provide solutions to previously unsolved problems is an example of a qualitative objective. Objectives can also be quantitative, such as the terms in which a desirable solution would be preferred to existing ones. The goals should be inferred logically from the problem identification. The goals of a solution must be defined in light of the current state of problems, as well as any existing solutions, if there is any, and their viability.The following objectives have been identified based on the research problem:Design an Arduino Uno R3 that can receive sensory input.Document discoveries and progress on creating and implementing the IoT System.Identify and illustrate an application area for the IoT System to demonstrate the architecture.Identify a suitable sensor for the use case to demonstrate the architecture of the IoT system.Include a sensor that can create a visible signal - for example, light-emitting diodes - to show that the primary sensor is actively streaming real-time data.Additionally, the Arduino microcontroller needs effective circuitry to avoid system short circuits and supply the sensors with the right amount of power.Ensure that the Arduino can stream accurate analogue values and test the output in a specified development environment such as Arduino IDE.Design and develop a streaming application that can receive the real-time data from the Arduino microcontroller, pipeline it to a streaming platform and deliver the formatted data to a dashboard environment. Python was chosen as the programming language to develop the script. Design a Kafka architecture to pipeline and process the real-time dataDesign a MongoDB database that can store the real-time values in a document format in order to enable the historic processing and analysis of data.Make use of a dashboard environment (Power BI) to perform data analytics and visualize the data. Based on the insights gained from analysing the data, one should be able to make an informed business decision. The business decision must be related to the identified use case to demonstrate the IoT system architecture.The abovementioned objectives provide the planning and design of a feasible and efficient architecture to develop the architecture of an IoT streaming system. The results from the dashboard must be quantifiable and thresholds must be indicated in the application as well.Design and develop the artefactThe researcher begins to construct the artefact during this phase. Constructs, models, methods, or instantiations - all terms with quite broad definitions - are considered artefacts. They are innovative qualities of social, technological, or other informational resources. A design research artefact is considered a product that uses research as an inspiration for its design. This task comprises understanding the architecture of the artefact as well as its desired functionality or purpose. The researcher must then start developing the artefact. Understanding the theory that may be used to design and implement a solution is one of the resources needed to move from objectives to designing and developing the artefact.The IoT system architecture consists of four main components:The Arduino Uno R3 microcontroller and a KY-039 5V pulse rate sensor.Apache Kafka and the streaming application developed in the Python programming languageMongoDB, a NoSQL database used to save the sensor data that has been pipelined through Kafka. This enables the preservation of past sensor data recordings in order to perform historic data analysis.The dashboard environment: Power BI Web services.The architecture design of the artefact is structured as follow:The KY-039 5V pulse rate sensor must be connected to the Arduino Uno R3 microcontroller via using jumper cables. The sensor has three pins: an analogue output signal, a 5V power supply and a ground pin. The sensor must be connected to the respective Arduino module pins in order for the sensor to stream analogue values. A green LED must be connected to the digital pins on the Arduino module in order to provide light output via flashes each time an analogue pulse is detected. Additionally, the LED must flash for a longer period when threshold values is detected from the sensory input. In order to display the analogue values streamed by the microcontroller, one must install Arduino IDE and ensure that the Board setting is set to “Arduino Uno” and the port setting must be set to “COM3”. One must upload an Arduino sketch (code) to the microcontroller via Arduino IDE, the code is written in C++. For the specific use case, a sketch is uploaded to the Arduino that detects a pulse by placing one’s finger between the sensor's infra-red LED and light transistor. The average of last twenty output values is displayed in the Arduino IDE environment. After uploading the sketch to the microcontroller, one can click on the “Verify” button in the IDE to ensure that the sketch compiles without any errors. Once the sketch is verified, one can start to stream the pulse values by running the sketch and the values will be displayed in a terminal. Additionally, one can make use of the serial plotter to visualize the pulse rate values with a time series on the x-axis.After setting up the Arduino microcontroller and connecting the sensor to it, one must install and setup Apache Kafka on a machine. Ensure that Java JDK 8 or 13 is installed on the machine. Download Apache Kafka directories from their official documentation: . After extracting the directories from the zip file, ensure that the Zookeeper and Kafka log directories are correct. Kafka makes use of Apache Zookeeper for configuration purposes, such as managing Kafka clusters, groups and topics. Access the server.properties and zookeeper.properties files in the config directory to ensure that the directories are correct. Ensure that the Zookeeper client port is set to 5181 or 2181. The Kafka server port should be set to 9092 or 5181. When demonstrating the artefact, one can start up the Zookeeper and Kafka servers via CLI commands. One can then create a topic to group and save the Kafka data.The next step is to develop the streaming application with VS Code and the Python programming language. The essence of this application is to stream real-time data from a Kafka producer to a Kafka consumer, in order for the consumer to process and pipeline the data. Firstly, one must develop two Python Scripts: a producer.py and consumer.py. In the producer script, one must import the serial and Kafka libraries. One instantiates a serial object that allows one to connect to the Arduino microcontroller using the COM3 port, specifies the baud rate and timeout value. Afterwards one must specify the bootstrap server, which is usually localhost:9092. Specify the topic name of the Kafka instance. Instantiate a Kafka producer object that receives the bootstrap server as an argument. One creates an infinite loop to keep reading values from the Arduino microcontroller. The producer object sends the topic name and serial values to the consumer, and one flushes the producer object to ensure that the consumer actually receives the output. The serial values read from the Arduino is transferred in a binary format. Additionally, one can insert a Keyboard Interrupt exception class to ensure the script exits safely when a keyboard interrupt is detected.Secondly, the consumer.py file must contain the following code:Import the Kafka library, pymongo library to write data to a MongoDB database, json and time libraries to format data into a document structure and allow time delays in the script. Import the os and sys libraries to gain access to operating system functionalities. Import the datetime library to read timestamps from numeric values received from the Arduino microcontroller. Import the dotenv library to read variables from a .env file as well as the bson.json_util library to enable the binary-encoded serialization of JSON documents. Connect to the MongoDB server by initializing a MongoClient object, connecting to localhost port 27017. Set the database name to test and the document collection to pulse_rate. Set the Power BI Web Service API key to the value received from the .env file and insert a try-catch statement in the code to determine whether the client connected successfully to MongoDB. The bootstrap server must be specified to localhost:9092, and the name of the topic for the Kafka instance. A KafkaConsumer object must be instantiated, which receives the following as arguments: the topic name, group id of the topic, which clusters data according to certain groups specified, as well as the bootstrap server. One must loop through all the values received from the producer. One must then convert the values received from the producer from binary into JSON. The pulse rate is displayed in a VS Code terminal environment to ensure that the producer streamed accurate pulse rate values. A try-catch statement is inserted to ensure that the serial values can be transferred to the dashboard environment, as well as another nested try-catch to ensure that the JSON formatted data can be written to a MongoDB database. The current datetime stamp associated with a pulse rate value is saved to a variable. If the pulse rate value is detected as empty, the script will convert the empty value to a float value of 0.0. The timestamp and pulse rate value are saved in a dictionary and inserted as a record into the MongoDB database. Finally, a new variable must be created that saves the timestamp and pulse rate value into a new dictionary, since the previous dictionary contains an Object ID after being inserted into MongoDB, one does not want to stream the record ID to the dashboard environment. One sends a response to the Power BI Web Service by binary-encoding the JSON record and using the API key to ensure that the data is transferred to the correct dashboard environment. If a threshold pulse rate is detected by the application, a message will be displayed in the console indicating the pulse rate value and the timestamp of that value. To ensure that the Power BI services do not become overloaded, the consumer only reads one value per second. A Keyboard Interrupt class is also inserted to ensure that the script exits safely after a keyboard interrupt have been detected.MongoDB must also be installed on the machine. Install the MongoDB Community Edition from . One can follow the installation instructions from their official documentation. In the MongoDB Community Server section, choose Version 6.0.2, platform Windows and package msi. Download the msi file and follow the installer instructions. Choose the complete setup type. At Service Configurations, select “Install MongoD as a service”. Select “Run Service as a network user”. Keep the service name “MongoDB”. Specify the data and log directories on the machine. Optionally, one can install MongoDB Compass, which serves as a GUI/DBMS for MongoDB. Click install after selecting all the options. Additionally, one will have to install mongosh, which serves as the CLI (command line interface) for MongoDB. Download the shell from . Select version 1.6.0, platform Windows 64-bit (8.1+) and package zip. Download the zip file and extract the directories on the machine. Add the mongosh binary to the PATH environment variable in the system environment variables for the Windows operating system. One can use the shell to connect to a MongoDB deployment, create and query documents via a command line interface. However, using MongoDB Compass proves to be much easier to create and query documents.Installing MongoDB Compass: download the Compass installation file from . Follow the prompts to install Compass from the installer file. Once installed, Compass launches and prompts one to configure privacy settings and specify update preferences. Launch the MongoDB Compass DBMS after completing the installation. One will need to connect to a MongoDB instance. Provide a connection string, which will usually be mongodb://localhost:27017. Click on the connect button, the user will be redirected to the Compass home page. Create a database named “test”, when accessing the database, click on “Create collection” and name the collection “pulse_rate”. When the streaming application is up and running, the JSON records from the streaming applications will be written to the MongoDB collection and updated real-time. Since the data is unstructured, there is no need to specify a schema for the document collection.Finally, the dashboard environment. Power BI Web Services is utilized as the dashboard environment. Log into Power BI using a Microsoft account. Go to My WorkSpace, click on “New”. Name the workspace “HeartRate_Demo” and select “Streaming dataset”. A new menu will appear that allows the user to choose between an API, Azure stream and PUBNUB as data sources. Select API. Click on next, a configuration menu will appear that requires one to enter the dataset name, and values that will be streamed to the dataset. Name the dataset “pulse_rate”. The values that should be received from the stream must be date, which should be specified as type DateTime and pulseRate which must be specified as type Number. Enable Historic Data Analysis in the dataset. When navigating back to the Heartrate_Demo workspace, click on the three dots next to the dataset and select “API info”. Copy the URL provided in the textbox, this is the variable one will have to specify in the .env of the streaming application. After doing this, click again on the three dots and click on “Create New Report”. Save the report and name it “HeartRate_Report”. A new page will be displayed where one can generate and filter the reports. Select Visualizations and insert a line chart. Select Fields and select all the data. In the Visualizations tab, set Zoom Slider to “On”. This will allow one to zoom into the line chart, and acts as a date range slider of some sort. In the Visualizations tab, set Page Refresh to on, and set Refresh type to “Auto Page Refresh” and refresh the page every one second. This allows one to refresh dashboards real-time. In the Build Visuals tab, set the y-axis to “Sum of pulseRate”. In order to create a dashboard based on the report data, click on the pin icon to pin the report to a dashboard. A full-page line chart will be displayed, and when the streaming application runs, the results will be updated live on the dashboard. Figure 4.1: IoT streaming system architectureThis section described the design and development of a simple IoT system. The application of this architecture is a heart rate monitoring application, which can be used to indicate the health level of an individual based on their pulse rate, and threshold values can be detected in the pulse rate, which can indicate that the individual should make a medical appointment to have their health checked, or it can also be an indication that a sensor is faulty and should be replaced if continuous threshold values are being streamed. The overall architecture of the IoT streaming application allows one to make an informed business decision based on the use case and insights provided from the dashboard. The official development of the artefact will be discussed and documented in Section 4.5. Demonstrate the artefactThe researcher must establish that an artefact is functional or helps to solve a problem. To confirm that the artefact genuinely fulfils its purpose, the researcher may need to mimic numerous instances of a problem. It is possible to experiment with an artefact, run simulations, conduct case studies, record experiences and observations, or engage in any other relevant activity to determine whether the functionality of the artefact complies with user needs. One of the most crucial elements needed for the demonstration is an effective grasp of how to use the artefact to address the research question. Regarding this research, the artefact demonstration will occur via testing and running simulations. Each component of the IoT system will be tested for its intended functionalities, and whether it adheres to user requirements. Observations and issues must be documented during this phase. The simulations and testing occur in iterative sprints to ensure that the artefact meets the user requirements, as well as ensuring threshold pulse values are being accurately detected. The simulation of each component of the architecture follows:Simulating the streaming of values from the KY-039 5V pulse rate sensor:After designing the circuitry of the Arduino Uno R3 microcontroller and connecting the pulse rate sensor to the microcontroller, it is time to assess whether the sensor can stream accurate values. Ensure that the jumper cables are properly connected between the microcontroller and pulse rate sensor. Ensure that the cables are connected to their respective pins, and that the LEDs are connected to the correct digital pins as well. The simulations occur in the Arduino IDE environment. Connect the Arduino microcontroller to a machine via a USB port. Open the Arduino IDE, save and upload the heart rate monitoring sketch to the microcontroller and verify the sketch for compilation. After the sketch have been compiled, open the serial monitor and observe the numeric pulse rate values that is being streamed by the microcontroller. Typically, a pulse rate should have a value between a hundred to two hundred. Monitor whether the green LED starts flashing when a pulse rate have been detected by the sensor. If any threshold pulse rate values occur, the LED must flash for a longer period. Additionally, one can use the serial plotter to plot the pulse rate values on a real-time graph.Simulating Zookeeper and Kafka server start-ups:Once the Arduino microcontroller and KY-039 5V pulse rate sensor has been tested, one needs to test that the Zookeeper and Kafka servers can be started and keep running. Firstly, open cmd or Windows PowerShell and navigate to the bin/windows directory of the Kafka folder. Start the Zookeeper server with a CLI command. Monitor whether the Zookeeper server fails and ensure that the server does not stop. If the Zookeeper server appears to be running successfully, start the Kafka server with a CLI command. Monitor whether the Kafka server fails and ensure that the server does not stop. Once the Kafka server runs successfully, one can create a Kafka topic to store the data that will be pipelined from the microcontroller. One also need to create the topic with a CLI command. Additionally, instead of creating a Kafka topic via using commands, one can simply change the topic name in the Python producer and consumer scripts, which will also generate a new Kafka topic once the scripts are being executed. To assess whether a topic have been created successfully, run a CLI command to list all Kafka topics in the server.Simulating the Python streaming application:After the Zookeeper and Kafka servers have been started up and running successfully, it is time to test the streaming application for accurate pulse rate values. Open two terminals in the VS Code environment, one to execute the producer script, another to execute the consumer script. Run the consumer script first, in order to ensure that the consumer receives messages from the producer in a timely manner. In the producer terminal, ensure that the producer keeps running and printing the value “Reading…”. In the consumer terminal, one has to monitor a few outputs: Ensure that the program displays that a connection was successfully established to MongoDB. Ensure that pulse rate values are inserted into MongoDB via checking for exceptions in the terminal. Ensure that the application can stream the timestamps and pulse values to Power BI via ensuring that the Response Code displayed in the terminal is a 200, which indicates that the request has succeeded. Most importantly, check that a numeric pulse rate value is displayed in the consumer terminal. Any other response code indicated in the terminal can be an indication of an invalid API key or the data that is being send to Power BI is not in the correct format. It is also important to keep in mind that MongoDB Compass should be running in the background to ensure that records are being inserted into the database real-time. Monitor the application for valid threshold timestamps and values.Testing for values in the MongoDB collection:The streaming application should be running in order for this simulation to work successfully. Once connected to a MongoDB instance, access the database and pulse_rate document collection. While the streaming application is running, refresh the page in order to ensure that records are delivered to the database real-time from the application. It is also important to observe the format of the incoming records. Three values must be received from each document (record): an Object ID, timestamp and pulse rate value. The timestamp value must be a formatted date type and the pulse rate value must be numeric (float) in order to perform calculations on the data in the dashboard environment.Simulating dashboard insights and visualizations:Finally, one must perform simulations in the dashboard environment. Open the Power BI dashboard and monitor the values on the line chart in real time. Observe the fluctuations in values and check for any thresholds that might occur. It is important to monitor whether the dashboard is updated every second a pulse rate value is being streamed. Monitor the insights provided by the dashboard. From the insights provided by the dashboard, and the output in the consumer terminal, one should be able to make an informed business decision. This concludes evaluating the complete IoT streaming system architecture, and the user will be aware that the architecture has been successfully implemented if the dashboard provides valuable insights. The architecture can also be proven to be successful if the provided use case delivers promising results i.e., the heart rate monitoring application provides indications of an individual's health levels and the dashboard provides an accurate heart rate monitoring graph.Evaluate the artefactEvaluate the artefact's ability to effectively support a solution that aids in solving the research question. It involves contrasting a solution's goals with the actual outcomes seen in the artefact's demonstrations. It requires knowledge of important measurements and analytical techniques. Evaluation can take on a variety of different shapes depending on the problem's nature and the type of artefact. It could involve a comparison of the artefact's functionality or the objectives of the solution, objective quantitative performance data such as a budget or items produced, client feedback, simulations, et cetera. It could consist of quantifiable system performance measures like availability or response times. Theoretically, such an evaluation may include any significant logical argument or empirical demonstration.Based on the objectives of the solution, the following must be evaluated:Can the Arduino Uno R3 microcontroller stream data by means of connecting a KY-039 5V pulse rate sensor to the Arduino module? Can the sensor stream accurate pulse rates? Do the LED lights indicate that heart rate monitoring occurs as well as the detection of threshold pulse rates?Have the development of the artefact been documented. Have observations and issues of the architecture been documented? It is important to note that the documentation must allow future developers and individuals with little to no experience in programming and the IoT field to set up and deploy simple IoT streaming systems.Has a suitable use case been identified to demonstrate the architecture of the IoT system? A heart rate monitoring application is utilized to demonstrate the architecture in this case.Has a relevant sensor been identified that is applicable to the use case in order to demonstrate the architecture? A KY-039 5V pulse rate sensor is used to detect pulse rates for the heart monitoring application.Can the LED lights connected to the digital pins of the Arduino module indicate the streaming of real-time values via flashes? Can the microcontroller detect threshold pulse rate values?Has efficient circuitry been implemented in the microcontroller? Have short-circuits been avoided? Do the sensors receive the correct amount of power to function?Can numeric pulse rate values be detected in the Arduino IDE via means of serial monitoring and serial plotting?Can the streaming application display pulse rate values? Can the application detect threshold values and timestamps? Can the application pipeline and process data from the microcontroller to Kafka producers and consumers? Can the application insert records into MongoDB? Can the application deliver formatted data to Power BI?Can the Zookeeper and Kafka servers keep running and not fail while the streaming application is running, and real-time data is being processed? Is the MongoDB document collection updated in real-time while receiving records from the streaming application? Are the data inserted into the collection using a specified JSON format?Does the Power BI dashboard receive real-time data from the streaming application? Is the dashboard being updated in real-time? Can valuable insight be gained from the data on the dashboard by means of analysis? Can an informed business decision be made based on the use case and insights provided by the dashboard? Can threshold pulse rate values be detected on the dashboard?If one can answer all the questions above, it is evident that the user requirements have been met and that the IoT streaming system architecture have been successfully designed and implemented. If the heart rate monitoring application works and valuable insights can be provided by the Power BI dashboard, indicating an individual’s health levels, the IoT system was successful, and the research question/problem has been solved.Communicate results to stakeholdersThe problem and its importance, the artefact itself, its value and distinctiveness, the thoroughness of its design, and its usefulness should all be made known to academics and other target audiences, such as practicing professionals. Researchers may use the framework of the life cycle to structure the paper in scholarly research publications. Understanding the assumptions, viewpoints, strategies, methodological analyses, and fundamental ideas held by members of an academic disciplinary community is necessary for effective communication. Provide a PowerPoint presentation on the results and findings of the artefact to an intended audience, such as lecturers and industry individuals. The PowerPoint presentation should include the problem statement and a description of the artefact. Explain the procedures and methods used to design and develop the artefact. Explain the architecture and functionalities of the streaming application. Present the results and findings of implementing the architecture via screenshots of dashboard insights and terminal outputs. To demonstrate the functionalities of the IoT streaming system, it can be particularly useful to simulate the heart rate monitoring application live to the lecturers and industry personnel. If the heart rate monitoring application meets user requirements, it proves that the architecture solved the problem, and that the research conducted contributed towards a solution to answer the research question.It is important to focus on how the architecture solves the problem. It is also important to make it abundantly clear that the focus area of the artefact is on architecture, not applications. An application area/use case can however be utilized to demonstrate the architecture of the artefact. If the heart rate monitoring application can provide valuable insights to the audience, it is a clear indicator that the artefact met user requirements and that the results have been clearly communicated to the intended audience. This concludes the life cycle of the artefact.Data Mining TechniqueData is considered a huge asset to companies for obvious reasons (Lee & Siau, 2001:41). Companies employ data mining techniques to extract meaningful information from huge volumes of data (Lee & Siau, 2001:41). Data mining assists companies in strategizing and organizing business objectives (Lee & Siau, 2001:41). Data mining techniques involves the non-trivial mining of invaluable, previously unknown and embedded information such as regularities, constraints and knowledge rules from data stored in warehouses and other repositories utilizing pattern recognition technologies such as mathematical and statistical techniques (Lee & Siau, 2001:41). These techniques have proven to be invaluable to companies, as it can point out KPIs and other areas businesses can focus on to improve the performance of their business (Lee & Siau, 2001:41). Data mining tools are especially useful to recognize patterns in data as well as the relationships between data (Lee & Siau, 2001:41). Data mining can value to the information extracted from different departments within an organization via the utilization of humongous corporate data warehouses into a client-server (Lee & Siau, 2001:41). The most common data mining techniques include statistics, mining transactional/relational database techniques, Artificial Intelligence techniques, decision tree approaches, genetic algorithms and of course, data visualizations (Lee & Siau, 2001). Data visualizations are especially useful for exploratory data analysis as well as mining for data in enormous databases (Lee & Siau, 2001:44). Visualizations requires integrating human knowledge and perspectives into the data mining process (Lee & Siau, 2001:44). Visualizations can provide insightful information on large data sets, and they can provide interactive displays that can convey the results to stakeholders (Lee & Siau, 2001:44). Well-known techniques of data visualizations include scatterplot matrices, graphs and parallel coordinates, to mention a few (Lee & Siau, 2001:44). A line chart was specifically chosen to display and analyse the results of the artefact, as it can convey valuable information in an overly simplistic manner. Utilizing a heart rate monitoring application to demonstrate the architecture of an IoT streaming system, one can use a line chart to easily visualize the average heart rate of an individual over a certain period of time.Development of artefactThis section covers the development of the artefact. A comprehensive overview is provided on how to set up and run the streaming application. This section also visualizes the architecture implemented to develop the IoT streaming application. The architecture is demonstrated via utilizing a heart rate monitoring application as a use case. The average heart rate of an individual is being monitored by the streaming application, and maximum heart rates is indicated by using a max line on the Power BI dashboard, a trend line is also added to the dashboard to indicate the fluctuations of heart rates over a period of time. This section focuses on five components: The Arduino Uno R3 microcontroller and the KY-039 5V pulse rate sensor, Apache Kafka, the Python streaming application, MongoDB and finally Power BI. Each component’s installation and setup are discussed, as well as how to deploy each component to utilize the heart rate monitoring application and the architecture of the IoT streaming application.Arduino and SensorsThis is the input component of the IoT streaming system. An Arduino Uno R3 microcontroller is employed and can stream data via connecting a sensor to it. The KY-039 5V pulse rate sensor is connected to the Arduino microcontroller to provide pulse rate values to the IoT streaming application. The image below displays the microcontroller connected to a device, with the KY-039 5V pulse rate sensor connected to the Arduino module. The final result of the streaming application is the dashboard that monitors the user’s average heart rate over a period of time. Figure 4.2: IoT SystemInstallation and SetupIn order to build the input component of the IoT system architecture, one needs the following, displayed in the image below:An Arduino Uno R3 microcontrollerKY-039 5V pulse rate sensorJumper cablesLEDsFigure 4.3: Arduino Microcontroller Materials (Maleki, 2021)The Arduino microcontroller serves as the component that can compute and process the analogue signals received from the sensor and output the analogue signals as numeric output in a GUI/terminal environment. Fingertip heartbeat pulse detections is possible using the KY-039 heartbeat sensor module. The output of this sensor is analogue. One’s finger can be placed on the module to view the heartbeat signal coming from the analogue output pin. The sensor has three pins, which are visualized in the figure below:S – analogue signal output5V – power supplyGND - groundFigure 4.4: KY-039 5V pulse rate sensor pins (Maleki, 2021)The figure below demonstrates the connections between the KY-039 5V pulse rate sensor and the Arduino microcontroller. Connect the ground pin from the sensor to the ground pin on the microcontroller’s ground pin on the power module using a black jumper cable. Connect the power supply pin from the sensor to the power supply (5V) pin on the microcontroller’s power module using a red jumper cable. Connect the analogue output pin from the sensor to the analogue output pin on the microcontroller’s analogue module using a green jumper cable (the colour of the cable does not matter for the analogue output).  A purple jumper cable was used in this architecture.Figure 4.5: Arduino Microcontroller Module (Maleki, 2021)Figure 4.6: Arduino MicrocontrollerThe next step is to connect the green LED to the Arduino microcontroller. Refer to  for the layout of the microcontroller. The light emitting diode is referred to as an LED. Typically, gallium arsenide and gallium phosphide are utilized to produce semiconductor components. A positive electrode and a negative electrode comprise the LED. Only when a forward current flows through the LED, does it turn on, and it can flash a variety of colours.  The material from which the LED is made determines its colour. The majority of LEDs have polarity; thus, they must be connected to a power source in a specific manner. Ground pins are typically connected to the LED's shortest lead or "leg". Connect the shortest lead to the digital ground pin on the Arduino microcontroller, and the longer lead to pin 13 on the digital module of the Arduino. The LED should be properly connected now to start flashing once an Arduino sketch have been uploaded. The figure below demonstrates how the LED should be connected to the microcontroller:Figure 4.7: Arduino Microcontroller LEDThe next step is to install the Arduino IDE in order to upload sketches (scripts) to the microcontroller to process analogue signals and output values in a GUI or terminal environment. Download the Arduino IDE installer file from their official site (release 1.8.19 was employed to upload sketches to the Arduino microcontroller). IDE 2.0 should also work fine for the IoT streaming Architecture:Figure 4.8: Arduino IDE Installer (Söderby, 2022)Once the installer file has been downloaded, follow the installer guidelines as demonstrated below:Figure 4.9: Arduino IDE Installer Guidelines (Söderby, 2022)The next step is to open the Arduino IDE, create a new sketch and upload it to the microcontroller. First, click on “Tools” in the menu bar, select Board as “Arduino Uno”. Click on Port and select “COM4”, which is the USB port that should be utilized to connect the Arduino microcontroller to a device. The code below should be inserted into the sketch. The Arduino switches electrical currents on and off to communicate with sensors. This resembles ones and zeros in binary. A "HIGH signal" is what is produced when a current is turned on. That is analogous to binary code's "one." When the current is cut off, a "LOW signal" is produced, which is comparable to the binary value zero. Set the LED pin to 13, since this is the output pin on the digital module that the sensor is connected to. Set the initial state of the LED to LOW, this indicates that the LED does not receive an analogue signal, and therefore will not flash. The setup method executes when the script starts to execute on the microcontroller. Within the method, one needs to specify that the LED’s pin 13 is connected to the digital output of the microcontroller. The code that instructs the Arduino as to which pins will receive the HIGH and LOW output signals needs to be changed. This can be done using the digitalWrite() function. The Arduino board and another device can communicate serially thanks to the function Serial.begin(). The most frequent serial communication one can establish is by using a USB cable to connect an Arduino to a computer. Two devices can communicate via a serial protocol once serial communication has been established between them. The speed at which data is sent through a communication channel is known as the baud rate. When describing electronics that employ serial transmission, baud rate is a term that is frequently used. "9600 baud" in relation to a serial port indicates that the port can transfer a maximum of 9600 bits per second and is usually the standard rate at which data is transferred in a serial fashion. The loop method allows one to execute a piece of code repeatedly once the setup void have been executed. The loop method must contain the code that allows the Arduino to stream continuous values. For an iteration of twenty pulse beats, the sum is calculated of the pulse values streamed by the analogue output of the sensor (connected to pin A0 on the microcontroller’s analogue module). The average pulse rate for the iterations is calculated afterwards. An average pulse rate of 100 and above is considered a threshold pulse rate value. The program checks for each second the average pulse rate is measured whether a threshold value is detected. If a threshold has been detected, the Arduino will flash constantly for 250 milliseconds. If no pulse is detected (a pulse rate value of three or less), the LED on the microcontroller will stop flashing. The Arduino will flash each second a pulse is being detected by the KY-039 5V pulse rate sensor.Figure 4.10: Arduino SketchDeployment of the Arduino and SensorAfter the code have been added to the sketch, one can upload the sketch to the Arduino microcontroller by clicking on the “Upload” button (arrow) right below the menu bar. One can click on the “Verify” button (checkmark) below the menu bar to ensure that the sketch compiled successfully, and no syntax errors have been detected in the sketch. Once the sketch has been compiled successfully, click on the Serial Monitoring button (search glass) at the top right corner of the screen to start outputting pulse rate values in a terminal within the Arduino IDE:Figure 4.11: Serial MonitoringAdditionally, one can also monitor the pulse rate values streamed by the microcontroller using a serial plotter. Go to “Tools” and click on serial plotter to display a graph with a time series that indicates how the pulse rate values are being streamed real-time.Figure 4.12: Serial PlottingThe installation and setups from this section was derived from (Maleki, 2021), (James, 2021), (Campbell, 2015), (Söderby, 2022), (Phongchit, 2016) and (Roy, 2020). Apache KafkaDevelopers can pipeline data for processing or retain it using Apache Kafka by streaming data from IoT devices. A Kafka application typically has three components: a topic, a producer, and a consumer. In order to keep track of occurrences (a real-time event or action), Apache Kafka was created. These events are kept in topics, which can be thought of as Kafka's version of a file system or folder. A Kafka consumer is a component that reads and processes messages from a producer, whereas a producer is a component that publishes (writes) events to a topic. Read the official Apache Kafka documentation or consult  of the literature review for more details on the architecture of Apache Kafka.Apache ZooKeeper is a centralized service that offers group services, distributed synchronization, configuration information management, and naming. Distributed applications make use of all of these services in some capacity. There is a lot of work involved in resolving the inevitably occurring defects and race situations each time these services are being implemented. Applications initially tend to skimp on these services because they are complex to create, which makes them unstable in the face of change and challenging to administer. Different ways of implementing these services result in management complexity when the applications are deployed, even when executed properly. In order to create a very straightforward interface for a centralized coordination service, ZooKeeper seeks to condense the core of these various services. The service itself is widely available and quite dependable. The service will implement consensus, group management, and presence protocols, eliminating the requirement for distributed applications to implement these protocols on their own. Application-specific deployments of these will combine particular ZooKeeper features with application-specific standards.Installation and SetupIn order to setup and run Kafka on a machine, one will need to install Java onto a machine. Search for the JDK 13 in a browser or go to the link to download from the Oracle website: .The JDK download page will appear as displayed in the figure below:Figure 4.13: Java SDK Download (Anon, 2019)The next step is to accept the terms of the license agreement window that will pop up when clicking on the link to download the SDK:Figure 4.14: Java License Agreement (Anon, 2019)After downloading the JDK, double click on it, the following window will pop up on the machine:Figure 4.15: Java Installation Guide (Anon, 2019)Click on next to continue the installation. The figure below allows one to change the installation directory of the Java SDK files. Select an installation directory according to personal preferences, and click next:Figure 4.16: Java Destination Folder (Anon, 2019)After selecting the installation directory, the installer will extract the SDK files to the selected location. After the files have been successfully extracted, the window below will pop up on the machine. Click on Close to exit the installer.Figure 4.17: Finish the Java Installation (Anon, 2019)The environment variable for Java must be configured before one can use the JDK that were installed. Navigate to the search glass at the bottom of the screen, next to the windows icon. Enter “environment” in the search bar and click on “Edit the system environment variables” option that pops up. The figure below will then pop up on the machine:Figure 4.18: Java Environment Variable (Anon, 2019)After choosing Path in the System Variables section, click Edit under Environment Variables. The path of the installed JDK needs to be added to the system path. Delete the installation of the JDK's prior route. Additionally, if a JAVA HOME has already been established, change it. Now select the New Button and enter the location of the JDK bin, which could be a directory such as C:\java\java-13\jdk-13\bin. To close every window, press the OK button three times. This configures the JDK 13 system environment variables for console access. Open a console, such as indicated in the figure below. Enter java --version into the command line and press enter. If Java is not recognized as an internal or external command, it means that one of the steps have been configured incorrectly. One needs to redo the Java installation steps if that happens to be the case.Figure 4.19: Ensure Java is installedThe next step is to install Apache Kafka. Go to  to download the Apache Kafka setup files.Figure 4.20: Apache Kafka Mirror Download (Anon, 2020)Extract the .tgz file to the machine’s C drive.Figure 4.21: Apache Kafka ExtractionFigure 4.22: Apache Kafka DestinationThe figure above displays the directories within the Apache Kafka folder. The next step is to navigate to the config directory and open the zookeeper.properties file in a text editor. Ensure that the dataDir property is kept at the default location, in this case the tmp directory on the machine’s C drive. Ensure that the clientPort property is set to 2181, which is the default port the Zookeeper server will run on. In this case, configuring the Zookeeper port to run on port 5181 also worked. Save the file and close it. Figure 4.23: Zookeeper PropertiesNext, open the server.properties file in the same directory. Ensure that the log.dirs property is kept at the default directory: the tmp folder on the C drive. Figure 4.24: Kafka Server PropertiesEnsure that within the same file, that the zookeeper.connect property is set to localhost:9092, which is the default port the Kafka server will run on. In this case, setting the port to localhost:5181 also worked.Figure 4.25: Kafka Server PortSave the changes made in the server.properties file and close it. The figure below displays the location of the Kafka and Zookeeper log files. It is also important to note that there is a chance that a Zookeeper or Kafka server instance might fail. If that happens to be the case, it is always feasible to clear the log directories and restart the Kafka and Zookeeper servers.Figure 4.26: Kafka Logs DirectoryAfter the steps for installing and setting up Kafka and Zookeeper have been completed, one can start to deploy the Zookeeper and Kafka servers. The following sources have been used to compile this section: (Anon, 2019), (Anon, 2020), (Anon, 2022d), (Tiwari, 2020), (Anon, 2022c) and (McDonald, 2021).Deployment of Apache KafkaBefore one deploys the Zookeeper and Kafka servers, it is a good idea to create a Windows batch file in order to execute CLI scripts quickly and efficiently, without the need to enter commands manually into a terminal. Create three batch files named server.bat, zookeeper.bat and kafka.bat. Use a text editor to write some code to the batch files. Enter the following code into the zookeeper,bat file:cd C:\kafka_2.13-3.2.0\bin\windows\.\zookeeper-server-start.bat ..\..\config\zookeeper.propertiesThe code above will navigate the terminal to the zookeeper-server-start.bat file in the windows directory of the Kafka directories and execute the batch file using the zookeeper.properties file’s properties as arguments. This will allow the Zookeeper server to start and manage Kafka configurations.Using a text editor to edit the file, enter the following code into the kafka.bat file:cd C:\kafka_2.13-3.2.0\bin\windows\.\kafka-server-start.bat ..\..\config\server.propertiesThe code above will navigate the terminal to the kafka-server-start.bat file in the windows directory of the Kafka directories and execute the batch file using the server.properties file’s properties as arguments. This will allow the Kafka server to start running, thus allowing one to stream and pipeline data to a Kafka topic.Next, edit the server.bat file in a text editor, and add the code as indicated in the figure below:Figure 4.27: Kafka Server Batch FileThis will allow one to run the Zookeeper and Kafka batch files simultaneously, with a five second interval between the Zookeeper and Kafka script, since the Zookeeper server must run first in order to manage the Kafka server configurations. The figures below demonstrate how the Zookeeper and Kafka terminals look once the server.bat file have been executed: Figure 4.28: Zookeeper ServerFigure 4.29: Kafka Server DeploymentOnce the servers are up and running, one can create a Kafka topic which will be used to retain and pipeline the data that will be streamed from the Arduino microcontroller. Open a new terminal, navigate to the windows directory of the Kafka folder and enter the following command:.\kafka-topics.bat --create --topic test --bootstrap-server localhost:9092This will create a new topic named test in one’s Kafka clusters. In order to list all topics in a Kafka cluster, run the following command:.\kafka-topics.bat --list --bootstrap-server localhost:9092The commands to create the topics and listing topics, as well as configuring the Kafka and Zookeeper setups can be attributed to TutorialsPedia (2009).The following section introduces the Python streaming application, which allows a Kafka producer and consumer to pipeline data. Python Streaming ApplicationThe next step is to use Python and a text editor, such as Visual Studio Code, to create the streaming application. This application's main function is to stream real-time data from a Kafka producer to a Kafka consumer so that the consumer may pipeline and process the data. Two Python scripts must be created: a producer.py and a consumer.py. In order to execute the scripts, one must have Python installed on a machine. Python was selected as the programming language of choice to develop the streaming application, since it has tons of libraries with a variety of functionalities, as well as being one of the simplest programming languages. This allows one to design and develop a remarkably simple IoT system architecture. Installation and SetupThe first step is to install Python on the machine, in order to be able to deploy the streaming application on a device. Open a web browser and navigate to the official Python website: . Select the Download tab for Windows. Choose a Python 3 release, in this case, select a Python version such as 3.10.2. If one happens to be using a using a 32-bit installer, click the link to get the Windows x86 executable installer. Download the Windows x86-64 executable installer the Windows installation is a 64-bit version. Launch the Python installer after downloading it. Select the box next to “Install launcher for all users”. Additionally, one can include the interpreter in the execution path by selecting the “Add Python 3.7 to path” check box. The figure below displays the window that will pop up once the installer file is being executed:Figure 4.30: Python Installation (Gangwar, 2022)Choose the "Customize installation" option. By selecting the following check boxes, you can select the optional features for installation:Documentationpiptcl/tk and IDLE (to install tkinter and IDLE)Python test suite (to install the standard library test suite of Python)Install the global launcher for “.py” files. This makes it easier to launch Python on a device.Install for all users.Figure 4.31: Python Optional Features (Gangwar, 2022)Click on Next. This will navigate the user to the Advanced Options page for the Python installation. Tick the boxes next to "Add Python to environment variables" and "Install for all users". One can alternatively choose the "Associate files with Python", "Make shortcuts for installed programs" and other sophisticated features. The Python installation directory shown in this step should be noted. It is required for the next action. Click Install to begin installation after making your selections for the Advanced settings. After the installation have been completed successfully, the figure below will pop up on the device. Click on Close to exit the installer.Figure 4.32: Python Installation Success (Gangwar, 2022)If one configured the Advanced settings during the installation process and added Python to the system environment variables, this step can be skipped. In order to ensure that the Python system variable have been added to the system environment variables for Windows, Navigate to the search glass at the bottom of the screen, next to the windows icon. Enter “environment” in the search bar and click on “Edit the system environment variables” option that pops up. Click on the “Environment Variables” button. Find the system's Python installation directory. Python will be installed in the following locations if you followed the instructions above exactly:C:\Program Files (x86)\Python37-32: for a 32-bit installationC:\Program Files\Python37-32: for a 64-bit installationIf one installed a different version of Python, the folder name might be different. Look for a folder with the word "Python" in the name. As indicated below, add the following entries to the PATH variable. Click on “New” and add the Python path variable:Figure 4.33: System Environment Variables (Gangwar, 2022)Figure 4.34: Python Environment Variable (Gangwar, 2022)After the Python environment variable have been added to the system, close all the windows by clicking on the OK button on each window. The next step is to ensure that Python have been successfully installed on the machine and that the operating system detects the Python environment variable. Open a terminal, and enter the following command into the terminal: Python -VAs indicated in the figure below, Python have been successfully installed on the machine. If Python is not recognized as an internal or external command, it means that one of the steps have been configured incorrectly. One needs to redo the Python installation steps if that happens to be the case.Figure 4.35: Python Installation ConfirmationThe next step is to create a project folder with a similar structure as indicated in the figure below. The .env file will be used to store an environment variable that stores an API key which allows one to stream data to Power BI. One will also need to create a producer and consumer Python script. The producer script will be used to receive data from the microcontroller and pipeline messages to a Kafka consumer. The consumer script will be used to receive messages from the Kafka producer and process the data, which will then be transferred to MongoDB in a JSON format. The data will be transferred to Power BI in real-time as well for analysis and monitoring. In order to edit the scripting files, one can make use of any text editor, such as VS Code. When one uses VS Code to edit the scripting files, it might be useful to install a Python extension that enables the editor to provide IntelliSense for the Python language, which includes features such as code completion, parameter info, quick info, and member lists. Download VS Code from . Select an installer that is suitable for the machine’s current operating system and modify the editor according to personal preferences.Figure 4.36: Streaming Application FolderThe figure below demonstrates the code that should be added to utilize the Kafka producer. The sys library allows one to utilize operating system functionalities. The serial library allows one to access an Arduino microcontroller port using Python. The time library allows one to pause the execution of scripts. Finally, the KafkaProducer class allows one to access and instantiate a producer from the Kafka server, which will be utilized to stream real-time messages from the microcontroller. Instantiate a serial object, providing the port number of the Arduino microcontroller and the baud rate as arguments. Specify the bootstrap server, which is the server that Kafka runs on by default: localhost:9092. Specify a topic name of the Kafka cluster, such as test or kafka20. Instantiate a Kafka producer object, providing the object with the bootstrap server as an argument. Insert the following code into an infinite loop, which will allow the Arduino microcontroller to stream pulse rate values constantly until a keyboard interrupt is detected by the script (pressing Ctrl + C). The Arduino microcontroller will keep streaming a pulse rate value in a binary format to the Kafka consumer. The Kafka producer will send the pulse rates to the relevant Kafka topic. Flushing the producer will block a message (pulse rate value) until the previously sent messages have been delivered (or errored), effectively making the producer synchronous.Figure 4.37: Python Producer ScriptThe additional code needed to use the Kafka consumer is shown in the next figure.Use the KafkaConsumer class, which enables Python to accept messages from a Kafka producer and process them. To write data to a MongoDB database, import the pymongo package (collection). To format data into a JSON structure and allow time delays within the script, import the json and time libraries. To access operating system features, import the os and sys libraries. To read timestamps from pulse rates the Arduino microcontroller has sent to the consumer, import the datetime library. Import the bson.json_util library to enable the binary-encoded serialization of JSON documents as well as the dotenv library to read variables from an .env file.Initialize a MongoClient object and connect to localhost port 27017 in order to connect to the MongoDB server. Set the document collection to pulse_rates and the database name to test. To validate whether the client connected successfully to MongoDB, add a try-catch statement to the code. Set the Power BI Web Service API key to the value obtained from the.env file. The topic name for the Kafka instance must be set to test or kafka20, and the bootstrap server must be given as localhost:9092. It is crucial to remember that for data to be pipelined using Kafka services, the topic names of the consumer and producer must match. It is necessary to create a KafkaConsumer object, which requires the topic name, the topic's group id (which clusters data according to certain groups specified), and the bootstrap server as arguments.All of the messages (pulse values) that were received from the producer must be iterated through. The values obtained from the producer must then be converted from binary to JSON. To confirm that the pulse rate values streamed by the producer were accurate, the pulse rate will be displayed in a terminal environment. In order to ensure that the serial values can be transported to the Power BI dashboard environment and that the JSON-formatted data can be written to a MongoDB database, nested try-catch statements are inserted into the code. The terminal will output the API response code and the pulse rate value. A status of 200 means the data have been successfully transferred to Power BI. A status of 400 or 404 indicates that there are some issues with data formatting or an invalid API key. Error 429 means that the Power BI services cannot handle the incoming requests to huge data loads. A pulse rate value is assigned to a variable with the current datetime stamp of the pulse value attached. The script will change the empty value to a float value of 0.0 if the pulse rate value is found to be empty. In order to store the timestamp and pulse rate value, a dictionary is created, and a record is added to the MongoDB database. In order to avoid streaming the record ID to the dashboard environment, a new variable needs to be constructed that saves the date and pulse rate value into a new dictionary. This is due to the fact that the old dictionary included an Object ID after being loaded into MongoDB. To ensure that the data is sent to the proper dashboard environment, one sends a response to the Power BI Web Service by binary-encoding the JSON record and using the API key. If the program detects a pulse rate above the specified threshold, a message with the pulse rate value and timestamp will be displayed in the console. The consumer only reads one value per second in order to prevent the overloading of the Power BI services. To ensure that the script terminates without any incidents following the detection of a keyboard interrupt, a Keyboard Interrupt class is also included.Note: The Power BI API key that will be retrieved later from the dashboard environment must be stored inside the .env file. Create a variable named POWERBI_API and set the variable’s value to the API key (Power BI URL). The dotenv library allows one to read variables from a .env file, and thus you can store keys in a more secure manner, specifically when using source control and gitignoring the .env file to prevent others from using one’s API keys. One does not want anyone to have access to your Power BI API key, since they can use the key to gain access to one’s dashboard environment and waste resources. If one is less concerned about security, one can just paste the Power BI URL in the requests.post() function on line 51 in the figure below:Figure 4.38: Python Consumer ScriptThe next section explains how to execute and deploy the Python streaming application via utilizing the Kafka services. Credit must be given to Gangwar (2022) for providing the instructions on how to install Python on a Windows 10 machine.Deployment of the streaming applicationIn order to execute and deploy the streaming application on a machine, create the following batch files in the bats folder in the project directory, as indicated in the figure below:Figure 4.39: Batch FolderAdd the following code to the consumer.bat file:cd C:\Users\Llewellyn\Desktop\Python-Kafka-Demopython producer.pyThis allows the batch file to navigate to the producer script’s location and execute the file.Add the following code to the consumer.bat file:cd C:\Users\Llewellyn\Desktop\Python-Kafka-Demopython consumer.pyThis allows the batch file to navigate to the consumer script’s location and execute the file.Add the following code to the streaming.bat batch file:Figure 4.40: Running the Streaming ApplicationThis will execute the consumer and producer Python scripting files simultaneously after navigating to the batch file directories in the project folder. Note that it is important to execute the consumer script before the producer script, since the consumer must receive messages from the producer in a timely manner. Alternatively, one can create a run.bat file which starts the Zookeeper and Kafka servers simultaneously, times out for ten seconds to allow the servers to start up properly, and execute the Python scripting files directly afterwards to automate the IoT streaming system for efficiently:Figure 4.41: Running the Batch FilesOnce the producer script is being executed, the output in the terminal environment should look like this:Figure 4.42: Producer OutputThe following output should appear in the terminal environment after the consumer script has been executed, and received the pulse rate values in real-time from the producer:Figure 4.43: Consumer OutputThe next section explains how to store the real-time data into MongoDB, in order to allow historic analysis of the heart rate monitoring application.MongoDB (NoSQL)Unstructured data is stored and processed in MongoDB, a NoSQL database. Therefore, concepts like relational data and tables, SQL, foreign and primary keys, etc. are not present in MongoDB. Instead, MongoDB relies on document-based data storage, often in a JSON format. Key and value pairs are used to store the data, and either one can be used to query the documents. Since the Arduino microcontroller streams its data in an unstructured fashion, and because MongoDB provides better performance than more traditional RDBMSs like MySQL or Postgres when receiving substantial amounts of unstructured real-time data, it was chosen as the database to store the real-time data.Installation and SetupInstall MongoDB 6.0 Community Edition on Windows using the default MSI installation wizard. Windows Server 2019 and Windows 10/Windows Server 2016 are two examples of 64-bit Windows versions on x86 64 architecture that are supported by the MongoDB 6.0 Community Edition. Only the 64-bit versions of these platforms are supported by MongoDB. Visit this page to download the MongoDB Community.msi installer: .In the Version dropdown, select the version of MongoDB to download.In the Platform dropdown, select Windows.In the Package dropdown, select msi.Click Download.Run the MongoDB installer as indicated in The MongoDB Community Edition installation wizard should be followed.One follows the wizard's instructions to install MongoDB and MongoDB Compass.Choose the Complete setup option (recommended for the majority of customers) or Custom setup type. MongoDB and the MongoDB tools are installed to the default location using the Complete setup option. One can choose which executables are installed and where via choosing the Custom setup.Service Configuration:  During installation, one has the option to either install the binaries or configure MongoDB as a Windows service. Take note of the figure below.Figure 4.44: MongoDB Service Configuration (Anon, 2022b)Select the option Install MongoDB as a Service.Run the service as the Network Service user (default). This is a Windows user account that comes with Windows, or you can use a local or domain user to run the service.Specify a period, i.e., for the Account Domain, the Account Name, and the Account Password for the user for an existing local user account.Indicate the Account Domain, Account Name, and Account Password for an existing domain user.Provider Name: Provide the name of the service. MongoDB is the default name. One must select a different name if the requested service already exists.The data directory, which is the same as the —dbpath, should be specified. The installer will create the directory and grant the service user access to it if it does not already exist.Log Directory: Enter the Log directory, which is the same as the —logpath, in this field. The installer will create the directory and grant the service user access to it if it does not already exist.Figure 4.45: Mongosh InstallerUse the Services console to start/restart the MongoDB service:Find the MongoDB service in the Services console.Start the MongoDB service by performing a right-click on it (click on Start).Install Compass:Compass serves as a GUI for creating and managing MongoDB document collections. Compass requires:64-bit version of Microsoft Windows 7 or later.MongoDB 3.6 or later.Microsoft .NET Framework version 4.5 or later.The Compass installer prompts one to install the minimum required version of the .NET framework if it is not already installed on the system.Starting the installation as an administrator when running a silent installation using Microsoft PowerShell or installing on Azure Virtual Desktop Infrastructure (VDI).One can use a web browser of personal choice to download Compass.Navigate to the downloads page of Compass: .Choose the preferred installer. One can download the MongoDB Compass installation as an.exe,.msi, or.zip package.Download the most recent version of MongoDB Compass for Windows.Double-click on the downloaded installer file.To install Compass, adhere to the instructions of the installer. The location of the Compass installation is up to the user.Compass will launch directly after installation and promptly ask to select privacy and update preferences.After the Compass installation, it is a good idea to install Mongosh as well. Mongosh is a shell for MongoDB that allows one to connect to a MongoDB instance using a shell, as well as querying documents using commands in a terminal environment. In order to install Mongosh, follow the next procedures step-by-step:Open the MongoDB Shell download page: .Open the MongoDB Download Centre.Download the Mongosh installation archive for the operating system.Download the proper version of Mongosh for the operating system. Extract the files from the downloaded archive.Add the Mongosh binary to the PATH environment variable.Ensure that the extracted MongoDB Shell binary is in the desired location on the filesystem of the machine, then add that location to the PATH environment variable.To add the MongoDB Shell binary's location to the PATH environment variable:Open the Control Panel.In the System and Security category, click System.Click Advanced system settings. The System Properties modal will pop up.Click Environment Variables.In the System variables section, select Path and click Edit. The Edit environment variable modal displays.Click New and add the file path to your Mongosh binary.Click OK to confirm your changes. On each other modal, click OK to confirm your changes.To confirm that your PATH environment variable is correctly configured to find Mongosh, open a command prompt and enter the mongosh --help command. If your PATH is configured correctly, a list of valid commands will be displayed in the terminal. If that does not happen, retry the Mongosh installation procedure.MongoDB should now be configured properly to run on the operating system. The following references were used to compile the installation and setup of MongoDB: (Anon, 2022b), (Anon, 2022a) and (Anon, 2022e). The next section indicates how to connect to an instance of MongoDB and store data to a document collection.Deployment of the MongoDB servicesIn order to connect to an instance of MongoDB, one will need to make use of the Compass GUI software. Open the application and refer to . Provide a connection string in order to connect to a MongoDB instance, in this case, mongodb://localhost:27017 is the server on which MongoDB runs locally on the machine. Save and test the connection in order to ensure that the connection string is valid. If the connection string is valid, one should be redirected to the Compass Home page, which will contain a list of databases. Create a new database named test, and within that database a new collection named pulse_rates.Figure 4.46: MongoDB Instance ConnectionIn order to stream real-time data to the MongoDB collection, one has to make sure that the Zookeeper and Kafka servers are running in the background. The Python consumer and producer scripts also need to run in order to pipeline the data from Kafka to MongoDB. Refresh the MongoDB pulse_rates collection to view the updates in the data. One will notice that each record streamed from the application inserts a record into the collection that contains an Object ID automatically generated by MongoDB, as well as a timestamp and a numeric pulse rate value. Figure 4.47: MongoDB CollectionMongoDB can be utilized as a data store. If it happens that a Power BI dashboard does not update with live results or becomes corrupted, one can import the document collection as a JSON file to Power BI, restore the results and keep performing historical data analysis. The next step is to get the data from these records onto the dashboard environment.Dashboard (Power BI)A business intelligence platform called Microsoft Power BI offers non-technical business users the means to gather, analyse, visualize, and share data (Scardina, 2018). With its strong interaction with other Microsoft products, Power BI is an extremely adaptable self-service tool that requires little initial training (Scardina, 2018). Its user interface is very intuitive for Excel users (Scardina, 2018). Data from an organization's sources can be analysed using Power BI (Scardina, 2018). Power BI may assist in merging separate data sets together, transforming and cleaning the data into a data model, and producing charts or graphs to display the information visually (Scardina, 2018). Other Power BI users within the company can access and use all of this information (Scardina, 2018).Organizations can utilize the data models produced by Power BI in a variety of ways, such as telling stories through charts and data visualizations and to explore "what if" scenarios in the data (Scardina, 2018). In order to ensure that department within an organization achieve business metrics, Power BI reports can also provide real-time answers to queries and assist with forecasting (Scardina, 2018). Administrators or managers can also access executive dashboards from Power BI to gain further insight into how departments are performing (Scardina, 2018). Power BI was selected as the dashboard environment for this architecture, due to its simplistic nature and variety of data visualisation tools that suits the heart rate monitoring application perfectly.Installation and SetupLogging into Power BI is the first thing that must be done. To do that, one will need a work or student Microsoft account, it is not necessary to have a pro or premium account. This is how a typical Power BI workspace looks:Figure 4.48: Power BI WorkspaceAfter logging in, select the "My Workspace" from the menu on the left of the screen, and provide your workspace with a name:Figure 4.49: Create a new Workspace (Grando, 2022)After that, one can build the streaming dataset by selecting "New" from the drop-down menu next to "My workspace":Figure 4.50: Streaming dataset window (Grando, 2022)After clicking on “Streaming dataset”, a new popup should appear:Figure 4.51: Create a new streaming dataset (Grando, 2022)The free Power BI API for streaming datasets allows one to stream real-time data to Power BI via using an API key. This is the approach that one needs to follow to implement the streaming architecture. Select the API method, then click on next. The following configuration menu ought to appear now:Figure 4.52: Streaming dataset values (Grando, 2022)One can specify the data's structure on that menu for transmission to Power BI. One must be careful when editing the data fields because the fields are case-sensitive. The field names must match exactly how one named them in the Python streaming application within the dictionary. Enable Historic data analysis as well, in order to monitor pulse rate values over a period of time. Figure 4.53: Edit the streaming data valuesAfter one is satisfied with the fields that will be provided to the dashboard, one can click on Done. Navigate back to the Workspace, click on the three dots next to the Workspace’s data set, and select “API Info”: Figure 4.54: API Info After selecting API Info, a menu will appear as indicated in the figure below. Copy the Push URL. This is the API key that you’ll need to insert into the .env file in your project folder or the requests.post() method in order to stream the data in real-time from the application to Power BI. If one executes the streaming application, a terminal that outputs a 200 status every second should appear. If a different status appears in the terminal, there was a mistake in one of the earlier steps. In that scenario, it is advised to check the field names on the API and in the streaming application’s code to see if there are any mistakes. This can also happen if the data is not transferred in the correct format to Power BI.Figure 4.55: API KeyClick on Done after adding the changes to the Python streaming application. One can now construct a report in Power BI using the data that are being passed to the API. Go back to the Workspace, click on the three dots next to the data set a click on “Create Report”:Figure 4.56: Create a new reportThe figure below demonstrates the report that was created to monitor pulse rates:Figure 4.57: Power BI ReportPerform the following steps to create the current report:Drag the line chart component from Visualizations into the whitespace area. Select the date and pulseRate fields from the Field tabEnsure that date is on the x-axis and pulseRate is on the y-axisClick on Filters and set “Filters on this visual” to Average for the pulse rate. This will allow the dashboard to monitor the average heart rate as long as pulse values are being streamed from the application.Select the “Format your Visuals” icon in the Visualizations tab and select the Zoom slider option. This will allow one to filter the period on the dashboard, and thus ease the process of historical data analysis.Click on the search glass icon in the Visualizations tab and add a trend line to the report. Add a Max line as well that will be used to indicate the maximum pulse rate detected when performing historical data analysis.   Add a gauge component from the Visualizations tab, this will monitor the heart rate by updating a meter real-time. Set the value of the gauge to average of the pulse rate. 	Click on the white space area of the report, then go to “Format your report page” on the Visualizations tab and enable page refresh. This will allow the dashboard to update real-time. Click on the dropdown next to page refresh and set the page to refresh every second.One can add custom descriptions for the report’s x and y-axes, as well as a title for the line chartThe next step is to pin the report to a dashboard, as indicated in the figure below:Figure 4.58: Pin dashboard to a reportPin the report to a new dashboard, and provide a name for the dashboard: Figure 4.59: Create a new dashboardThe figure below displays the dashboard after pinning it live:Figure 4.60: Power BI DashboardOne can notice the trend line and max line on the dashboard, as well as the average heart rate gauge from sample data being streamed to the dashboard. The next step is to deploy the complete streaming application and monitor the average heart rate of a person via utilizing the Power BI dashboard. Credit should be given to Grando (2022) for providing the guidelines to create a Power BI dashboard and utilizing data visualizations from a streaming Python application.Deployment of the Power BI dashboardsAfter one have connected the Arduino microcontroller to a device, set up and run the Zookeeper and Kafka servers as well as running the streaming application, one can start to monitor the average heart rate of a person in real-time. The figures below display real-time heart rate monitoring being performed:Figure 4.61: Power BI Dashboard ResultsFigure 4.62: Power BI Dashboard Live UpdatesThe trend lines and max line will update as the pulse rate values are being received from the streaming application. A heart rate monitoring application was developed to demonstrate the architecture of an IoT streaming system. This concludes the development of the artefact, and demonstrated how one can design and develop an IoT system architecture utilizing an Arduino microcontroller and a KY-039 5V pulse rate sensor as means of providing input streaming data, pipelining the data through Apache Kafka, using a Python application to format and stream the data directly to Power BI to gain insights on the data received from the application, with MongoDB being utilized as a data retainer which allows the analysis of historical data.Chapter 5:  RESULTSCollection of resultsIntroductionThis chapter introduces the results provided by designing and developing the artefact. Section 5.1 introduces the measurements of the results: how the results contributed to meeting the objectives of the research and in essence, solving the research question. Section 5.2 describes the actual results, which includes screenshots from the Power BI dashboards and a description of how each component of the artefact contributed to answering the research question: implement an architecture that can stream data utilizing an Arduino Uno R3, pipelining the data through Kafka and transferring it to a dashboard environment in order to make an informed business decision based on data analysis and the visualization of the results. A heart rate monitoring application was utilized to demonstrate the architecture of an IoT streaming application.The following measurements will be used to indicate whether the results contributed to designing and developing a successful artefact (IoT streaming system):Create an Arduino Uno R3 that can take input from sensory devices.Keep track of new developments and the IoT System's development and implementation.Select and illustrate an IoT System application area to show off the architecture.Choose a relevant sensor for the use case to illustrate the IoT system's design.Incorporate a sensor that can emit a visible signal, such as light-emitting LEDs, to demonstrate that the main sensor is actively streaming real-time data.The Arduino microcontroller needs efficient circuitry to prevent system short circuits and deliver the proper quantity of power to the sensors.Check the Arduino's ability to stream precise analogue values and verify the results in a predetermined development environment like the Arduino IDE.Create a streaming application that is capable of pipelining real-time data from the Arduino microcontroller to a streaming platform and deliver the formatted data to a dashboard environment.Create a Kafka architecture for the real-time data flows and processing.Create a MongoDB database that can store current values in a document format to make it possible to process and analyse historical data.Use Power BI to perform data analysis and visualize the data in the form of a dashboard. An informed business decision should be made, based on the information learned from data analysis. To illustrate the IoT system architecture, the business decision must be connected to the defined use case.ResultsTo demonstrate that the design and development of the artefact was successful, this section contains a result of each component of the artefact:Arduino Uno R3 and KY-039 5V pulse rate sensorApache KafkaPython streaming applicationMongoDBPower BIArduino microcontroller and sensorThe figure below demonstrates how an individual’s pulse rate is being measured by the KY-039 5V pulse rate sensor. One can measure the success of the Arduino streaming pulse rate values by checking the average pulse rate values in the Arduino IDE serial monitor or serial plotter. Another metric of success is the fact that the LED flashes when a pulse rate is being detected:Figure 5.1: Arduino microcontroller sensing a pulse rateApache Kafka and ZookeeperOne needs Apache Zookeeper to configure and maintain Kafka clusters. Apache Kafka is the tool that allows one to pipeline and process real-time data in a streaming architecture. The figure below demonstrates that the Zookeeper and Kafka servers are running and are ready for the processing and management of real-time data:Figure 5.2: Running the Kafka and Zookeeper serversPython Streaming ApplicationThe python streaming application is utilized to send real-time data from a Kafka producer to a Kafka consumer, sending the JSON formatted data to MongoDB and to the Power BI dashboards using an API. One can monitor values in the producer and consumer consoles, as shown below. As long as the consumer console prints an API response value of 200 and a pulse rate, it means the streaming application is working:Figure 5.3: Streaming pulse rate values real-timeMongoDBMongoDB is utilized as a data store in order to perform historic data analysis. If a Power BI dashboard stops updating with real-time data or becomes corrupted, it is possible to restore the results by importing the document collection as a JSON file into Power BI and continuing to analyse past data. One can be sure that the streaming architecture is successful if one refreshes the MongoDB document collection and new records are read into the collection and the record count is updated each time:Figure 5.4: MongoDB receiving real-time recordsPower BI DashboardsPower BI serves as a powerful tool to monitor and analyse real-time data. The dashboard below demonstrates how the streaming architecture pipelines data all the way from the sensor connected to an Arduino microcontroller to the dashboards on Power BI: Figure 5.5: Monitoring heart rate in real-timeSimilarly, one can conduct real-time analysis on data displayed on the dashboards. One can detect the maximum heart rate over a period of time. A trend line can also be added to the dashboard to indicate how the average heart rates fluctuated over time. The gauge component on the dashboard also provides a meter that indicates the average heart rate.Figure 5.6: Data analysis on dashboardOne can gain a few insights by analysing the results from the heart rate monitoring dashboard. If the average heart rate is very low over a period of time, it can indicate some serious health issues that the individual has whose heart rate is being monitored currently, or it can also be an indication of a faulty sensor that needs to be replaced. The same applies to extremely high average pulse rates. Higher average pulse rates can be an indication of stress, high blood pressure, lower fitness levels or simply an individual’s average heart rate being higher due to recent exercising sessions.Chapter 6:  REFLECTION AND CONCLUSIONOverall ConclusionLearning and development of artefactThis chapter finally concludes the study. This section provides the conclusion of the study, it is important to reflect on the discoveries made while designing and developing the artefact. The observations and issues detected while conducting the study should be highlighted and assessed. A description is provided of the procedures followed to design and develop the artefact. Recommendations about the improvement of the artefact and design/development processes must be distinctly acknowledged in this section. A reflection is given on the management of the project and how deadlines were met. An extensive overview of the objectives of the study is provided and whether they were satisfied by conducting the study. A reflection with a summary that offers a critical assessment of the steps taken to complete the study will be provided as well. Section 6.1 covers the learning that occurred during the development of the artefact. Section 6.2 discuss the achievement of the study objectives. Section 6.3 covers the project management aspect of the study: how scope, time and constraints were managed during the development of the artefact. Section 6.4 provides the conclusion of the study: what the researcher found the essence of the study to be, and why its content can contribute to future studies. A number of observation and issues have been detected while conducting the study. It is extremely important to have a well-defined research question, as an invalid research question can render the research useless if a researcher does not understand the research question. Having clearly defined research objectives is of utmost importance: it provides guidelines for conducting research, processes for planning and developing an artefact and they highlight the essence of the research. Having a clear goal in mind towards completing research or an artefact can keep a researcher focused on what aspects of a study is important and where key focus areas of the research are. Having a suitable research paradigm and research methodology helps researchers to identify the objectives of their research. Paradigms and methodologies can also assist researchers in identifying processes to conduct research and how to design and develop artefacts. Even though development tools and environments may change as research progresses, it is important to have a set of guidelines and project planning in order to identify what topics are important to complete a study, as well as identifying and adhering to user requirements and study objectives. The Critical Research (CR) paradigm helps researchers to identify the ontological, epistemological and methodological nature of the research that needs to be conducted. CR helps scientists to understand the reality of science and that certain phenomena in the world does not influence or determine the outcome of another event that occurs within a system. The Design Science Research methodology happened to be extremely effective during the design and development of this artefact. It provided the guidelines and planning to complete the artefact, indicating clearly what the objectives are for each phase of the development of the artefact. The methodology also allowed development iterations in order to adhere to changing user requirements, as well as improving/optimizing the artefact. During the design and development of the artefact, additional observations and issues have been detected:Understanding how an architecture works is much more important than focusing on the latest technologies to design and develop a solution.Breaking a problem into modular pieces assists in identifying issues much quicker and also leads to solving a problem more efficiently.Documenting approaches and processes can essentially help researchers to become more structured and focused.The research assists developers in designing a simple and efficient IoT streaming system that is fully functionalThe nature of this research is based on the study of architecture; thus, any given use case should be able to be implemented within the provided IoT streaming architectureProject management and planning is an especially important skill that researchers must learn. Identifying the scope, constraints and schedule of any given project in the real world is an essential human skill.Researchers must be adaptive and flexible, as a project’s scope and time constraints can change at any given moment.The study teaches developers how a streaming application works, specifically the architecture thereof. An essential component of the study was installing and setting up tools and development environment.Deploying a system or architecture is an important deliverable in the real world.Finding alternative to a solution is sometimes less tedious and provides to be more efficient than standard approaches.Achievement of objectivesThe study can overall be viewed as a success since it met all of the objectives and aims of the study. Chapter 5 provided the evidence that each of the objectives of the study have been met:An Arduino Uno R3 have been designed that can receive sensory input.The discoveries and progress on creating and implementing the IoT System have been documented.An application area for the IoT System to demonstrate the architecture have been identified, namely a heart rate monitoring application.A suitable sensor has been identified for the use case to demonstrate the architecture of the IoT system: a KY-039 5V pulse rate sensor.A LED have been connected to the Arduino and can indicate the detection of an individual’s pulse rate by the KY-039 5V via flashing.The circuitry of the Arduino provided optimal power to the LED and KY-039 5V pulse rate sensor. No short-circuits have ever occurred while streaming data.The pulse rate values detected by the sensor have been exactly accurate, when comparing it to other smart devices with heart rate sensors such as Garmin watches and smartphones.A streaming application have been designed and developed that can receive real-time data from the Arduino microcontroller, pipeline it to a streaming platform (Apache Kafka) and deliver the formatted data to a dashboard environment (Power BI).Apache Kafka was able to pipeline data in real-time to the Power BI environment.The MongoDB database could store the real-time values in a document format. Historic processing and analysis of data could be performed in the dashboard environment.Power BI was utilized to perform data analytics and visualize the data. Based on the insights gained from analysing the data, the researcher was able to make an informed business decision by looking at the heart rate monitoring dashboard, as discussed in Chapter 5.Project managementManaging the study in terms of scope, constraints and time was an important aspect of the study. The following processes was followed to design and develop the artefact:Identify the research problem and motivate how the problem will be solved.Define the objectives of the study/solution.Design and develop the artefact. This involved iterations of testing and optimizations to adhere to the objectives of the study and user requirements.Demonstration of the artefact. This also involved “sprints” of testing the artefact. Each component of the artefact should have been properly tested to determine its efficiency as part of the IoT streaming architecture.Evaluation occurs after the design and development of the artefact. Evaluation occurred Section 6.2, which determined whether the artefact met the objectives of the study.Communicating the results to stakeholders. The heart rate monitoring application was demonstrated to lecturers and individuals from the industry. The overall responses to the study and architecture of the IoT streaming system was positive. The scope of the artefact was feasible, and the design and development of the artefact could be completed in time. Each phase of the artefact was properly planned, and deadlines for each of the deliverables of the study/artefact was properly indicated on a personal schedule and the project plan in . Limitations of the artefact was addressed by finding alternative solutions to address an issue or solve a problem.ConclusionThis study provided the means to design an architecture that allows one to stream data from an Arduino Uno R3 to a streaming platform such as Apache Kafka and pipeline the data from the platform to a dashboard environment in order to perform data visualisations and analysis. Based on the results of the dashboard, one was able to make an informed business decision based on the demonstrated use case. This study met all the objectives created at the start of the study and has contributed to a knowledge base on how to design and develop an architecture for a simple IoT streaming system. The architecture of the current IoT streaming system can be improved by dockerizing the application or running it on a virtual machine. This will make the architecture more complex however. Virtualizing the IoT system will prevent developers from having to install and setup the development tools and environments on their own computers. Another important objective of the study that have not been mentioned prior have been achieved: being able to learn a new topic and implementing the knowledge gained via practical implementation. Self-studying is the key to empowering oneself. By learning new skills and “connecting the dots” between various kinds of information, researchers and developers alike can help the human race to advance into a new era of prosperity and excellence.Bibliography Akhtar, N., Tabassum, N., Perwej, A. & Perwej, Y. 2020. Data analytics and visualization using tableau utilitarian for covid-19 (coronavirus). Global Journal of Engineering and Technology Advances, 3(2):28-50.  Date of access: 3 July 2022. Anon. 2019. How to install java 13 on windows. (How to install).  Date of access: 23 Oct 2022.Anon. 2020. Kafka_2.13-3.2.0.Tgz. (kafka_3.2.0).  Date of access: 15 Aug 2022.Anon. 2022a. Download and install compass.  Date of access: 26 Oct 2022.Anon. 2022b. Install mongodb community edition on windows.  Date of access: 16 Sept 2022.Anon. 2022c. Kafka server.  Date of access: 25 Oct 2022.Anon. 2022d. Install apache kafka on windows.  Date of access: 25 Oct 2022.Anon. 2022e. Install mongosh.  Date of access: 26 Oct 2022.Calvillo-Arbizu, J., Román-Martínez, I. & Reina-Tosina, J. 2021. Internet of things in health: Requirements, issues, and gaps. Computer Methods and Programs in Biomedicine, 208:10. Date of access: 30 March 2022. 0.1016/j.cmpb.2021.106231Campbell, S. 2015. Getting started with the arduino – controlling the led (part 1).  Date of access: 24 Oct 2022.Fernandes, E., Salgado, A.C. & Bernardino, J. 2020. Big data streaming platforms to support real-time analytics. In. ICSOFT. pp. 426-433.Franklin, J. 2022. Apache kafka use cases: When to use it & when not to.  Date of access: 2 July 2022.Gangwar, M. 2022. How to install python on windows 10.  Date of access: 23 Oct 2022.Golab, L. & Ozsu, M.T. 2003. Data stream management issues–a survey.  Date of access: 27 June 2022.Grando, U. 2022. How to stream your data to power bi with python.  Date of access: 5 Oct 2022.Gürcan, F. & Berigel, M. 2018. Real-time processing of big data streams: Lifecycle, tools, tasks, and challenges. In. 2018 2nd International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT). IEEE. pp. 1-6.Hébrail, G. 2008. Data stream management and mining. Mining Massive Data Sets for Security:89-102.  Date of access: 27 June 2022. 10.3233/978-1-58603-898-4-89Gürcan, F. & Berigel, M. 2018. Real-time processing of big data streams: Lifecycle, tools, tasks, and challenges. In. 2018 2nd International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT). IEEE. pp. 1-6.Hébrail, G. 2008. Data stream management and mining. Mining Massive Data Sets for Security:89-102.  Date of access: 27 June 2022. 10.3233/978-1-58603-898-4-89Hevner, A.R., March, S.T., Park, J. & Ram, S. 2004. Design science in informationHevner, A.R., March, S.T., Park, J. & Ram, S. 2004. Design science in information systems research. MIS Quarterly, 28(1):75-105.  Date of access: 27 Apr. 2022. Ioana, B., Claudia, S.-P.D. & Ioan, B.M. 2014. Using dashboards in business analysis. THE ANNALS OF THE UNIVERSITY OF ORADEA:851-856.  Date of access: 3 July 2022. James, M. 2021. What is serial.Begin(9600)? . Date of access: 24 Oct 2022.Jiang, Q. & Chakravarthy, S. 2006. Anatomy of a data stream management system. ADBIS Research Communications, 215,  Date of access: 28 June 2022. Kaswan, K.S., Singh, S.P. & Sagar, S. 2020. Role of arduino in real world applications. INTERNATIONAL JOURNAL OF SCIENTIFIC & TECHNOLOGY RESEARCH, 9(1):1113-1116.  Date of access: 30 June 2022. Lee, S.J. & Siau, K. 2001. A review of data mining techniques. Industrial Management & Data Systems, 101(1):41-46.  Date of access: 24 Oct 2022. Liu, X., Iftikhar, N. & Xie, X. 2014. Survey of real-time processing systems for big data. In. Proceedings of the 18th International Database Engineering & Applications Symposium. pp. 356-361.Maleki, M. 2021. Interfacing ky-039 heartbeat sensor module with arduino. (Learn - Sensors).  Date of access: 3 Oct 2022.McDonald, G. 2021. Zookeeper: Index.  Date of access: 25 Oct 2022.Patel, A. 2022. Data visualization using tableau. Metropolia University of Applied Sciences.  Date of access: 3 July 2022.Patel, B.C., Sinha, G. & Goel, N. 2020. Introduction to sensors. In. Advances in modern sensors: IOP. p 367. Pauwels, K., Ambler, T., Clark, B.H., LaPointe, P., Reibstein, D., Skiera, B., ... Wiesel, T. 2009. Dashboards as a service: Why, what, how, and what research is needed? Journal of service research, 12(2):175-189.  Date of access: 3 July 2022. Peffers, K., Tuunanen, T., Rothenberge, M.A. & Chatterjee, S. 2007. A design science research methodology for information systems research. Journal of Management Information Systems, 24(3):45-77.  Date of access: 27 Apr. 2022. 10.2753/MIS0742-1222240302Phongchit, N. 2016. What is baud rate & why is it important? . Date of access: 24 Oct 2022.Pries-Heje, J., Venable, J. & Baskerville, R. 2014. Rmf4dsr: A risk management framework for design science research. Scandinavian Journal of Information Systems, 26(1):57-82.  Date of access: 27 Apr. 2022. Rahman, A.A., Adamu, Y.B. & Harun, P. 2017. Review on dashboard application from managerial perspective. In. 2017 International Conference on Research and Innovation in Information Systems (ICRIIS). IEEE. pp. 1-5.Roy, A. 2020. Blinking led © gpl3+. (ProjectHub).  Date of access: 24 Oct 2022.Sarikaya, A., Correll, M., Bartram, L., Tory, M. & Fisher, D. 2018. What do we talk about when we talk about dashboards? IEEE transactions on visualization and computer graphics, 25(1):682-692.  Date of access: 3 July 2022. Scardina, J. 2018. Microsoft power bi.  Date of access: 27 Oct 2022.Sleeper, R. 2021. Tableau desktop pocket reference. 1005 Gravenstein Highway North, Sebastopol, CA 95472: O'Reilly Media, Inc.Söderby, K. 2022. Downloading and installing the arduino ide 2.0.  Date of access: 23 Oct 2022.Sousa, R., Miranda, R., Moreira, A., Alves, C., Lori, N. & Machado, J. 2021. Software tools for conducting real-time information processing and visualization in industry: An up-to-date review. Applied Sciences, 11(11):4800.  Date of access: 3 July 2022. Surdu, S. 2011. Data stream management systems: A response to large scale scientific data requirements. Annals of the University of Craiova-Mathematics and Computer Science Series, 38(3):66-75.  Date of access: 27 June 2022. Tiwari, G. 2020. Setting up and running apache kafka on windows os. (Big Data Zone).  Date of access: 25 Oct 2022.TutorialsPedia 2021, Install Kafka on Windows | Setup Kafka on Windows | Run Kafka As Windows Service | Kafka Tutorial, online video, YouTube, viewed 15 Aug 2022, https://www.youtube.com/watch?v=-fWznY5nBSg.Watt, A. 2014. Project management. 2.  British Columbia: BCcampus Open Education. Available from:  Date of access: 28 Apr. 2022. Wu, H., Shang, Z., Peng, G. & Wolter, K. 2020. A reactive batching strategy of apache kafka for reliable stream processing in real-time. In. 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE). IEEE. pp. 207-217.Wynn, J., Donald E. & Williams, C.K. 2008. Critical realism-based explanatory case study research in information systems. Paper presented at the Association for Information Systems AIS Electronic Library (AISeL), Paris.  Date of access: 26 Apr. 2022.Ziemann, V. 2018. A hands-on course in sensors using the arduino and raspberry pi. 6000 Broken Sound Parkway NW, Suite 300: CRC Press. (Series in sensors).Zimanyi, E. 2017. Streaming databases & pipelinedb.  Date of access: 27 June 2022.Annexures: ETHICAL CONSIDERATIONS FORM
